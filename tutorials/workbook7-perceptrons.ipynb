{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "TODO change to the  code from lectures and change  progression:\n",
    "    <ol> \n",
    "    <li>experiment with widget- MCQWs to guide them and test what we want them to get out of it</li>\n",
    "    <li>  Iris data - making the point that we could see one class was separable so setting up a chain of two-way classifiers </li>\n",
    "    <li> MNIST - introduction to dataset and then make a classsifier for one digit.<br> adapt code from could show code from https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-mnist-py to visualise the weights in the model- e.g. for 8s</li>\n",
    "    <li> link to blog https://www.zswarth.com/blog/creating-a-perceptron-classifier-for-the-mnist-dataset/ to read and answer one mcq </li>\n",
    "    <li> discuss chaining vs parallel and when you might use either: <b>mentimeter to mediate?</b> </li>\n",
    "    </ol></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity1:\n",
    "Work through the first few cells ‘Logical Operators’, ‘Implementation’ and ‘Linear Decision Boundary’ sections and try to familiarise yourself with the algorithm and how the process builds a model by learning values for the weights.\n",
    "How well does it perform on different logical functions?\n",
    "- You do not need to remember the equation for the calculating the line.\n",
    "- You should try and understand how this decision boundary relates to the Perceptrons output (and why it can only be straight).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons - The basis of Artificial Neural Networks\n",
    "\n",
    "Perceptrons, invented by Frank Rosenblatt in the late 1950's,\n",
    "are a form of supervised machine learning algorithm inspired by neuron cells.\n",
    "In neurons, signals come in along the dendrites and out along the axon. \n",
    "A synapse is the connection between the axon of one cell and the dendrites of another.\n",
    "Crudely, input signals are 'summed' and if they reach a certain threshold the neuron 'fires'\n",
    "and sends a signal down the synapse to the connected cells.\n",
    "\n",
    "![Perceptron](resources/Perceptron.png \"Perceptron Image\")\n",
    "\n",
    "Perceptrons are an algorithmic approximation of this process and can learn to solve simple classification problems.\n",
    "Input values are multiplied by a learnable parameter called a *weight*.\n",
    "If the sum of the inputs $\\times$ weights is over a certain threshold the Perceptron 'fires' and generates an output.\n",
    "We use the *error* in the output to change the value of the *weights* by a small amount - the *learning rate*.\n",
    "The process is repeated until the error is 0, or as small as we can get it.\n",
    "\n",
    "**Note:** The threshold which determines if the Perceptron produces an output is determined by its *activation function*.\n",
    "For Perceptrons this is often a step function which outputs a 1 or 0 i.e. 'fires' or not. However, it can also be a\n",
    "non-linear function such as sigmoid, which will always produce a real numbered output in the range 0 to 1.\n",
    "\n",
    "### Perceptron - Algorithm\n",
    "\n",
    "1. Set weights to random values in range [-0.5, 0.5]\n",
    "\n",
    "2. Set learning rate to a small value, usually less than 0.5\n",
    "\n",
    "3. For each training example in the dataset i.e one 'epoch'\n",
    "\n",
    "    A. Calculate output (activation)\n",
    "    \n",
    "    $sum = \\sum\\limits_{i=0}^{n} w_i \\times x_i$\n",
    "      \n",
    "    $if\\ sum >\\ 0 \\\\ \\;\\;\\;activation = 1 \\\\ \\\\else \\\\ \\;\\;\\;activation = 0$\n",
    "       \n",
    "    B. Calculate error\n",
    "    \n",
    "    $error = target \\, output - activation$\n",
    "\n",
    "    C. Update each of the weights values\n",
    "    \n",
    "    $change \\, in \\, weight = error \\times input \\times learning \\, rate$\n",
    "\n",
    "\n",
    "4. Repeat from step 3 until error is 0 (or as close as possible), or for the number of training epochs.\n",
    "\n",
    "### Perceptrons - Logical Operators\n",
    "\n",
    "We are going to use binary data to show that Perceptrons can learn to represent logical functions,\n",
    "though you could also think about it as a prediction/classification problem\n",
    "i.e. for a given set of inputs what is the correct output.\n",
    "A truth table can be used as the Perceptrons *training* data, with each row representing an input example.\n",
    "Each training example has two inputs (*features*) and one output (*label*).\n",
    "\n",
    "| Input 1| Input 2| AND | OR  | XOR |\n",
    "|:------:|:------:|:---:|:---:|:---:|\n",
    "| 0      | 0      | 0   | 0   | 0   | \n",
    "| 0      | 1      | 0   | 1   | 1   |\n",
    "| 1      | 0      | 0   | 1   | 1   |\n",
    "| 1      | 1      | 1   | 1   | 0   |\n",
    "\n",
    "First we will import some python modules and then create the training data.\n",
    "\n",
    "**Note:** Input data is often denoted as X and labels/target outputs with Y.\n",
    "Here we are going to use **inputs**, but the target outputs have been labeled **AND**, **OR** and **XOR**.\n",
    "This is so we can be clear about what the outputs should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import some needed modules\n",
    "import plotly\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from perceptron_utils import BinaryPerceptronGraph\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "# Create input and target output data\n",
    "inputs = [[0, 0],\n",
    "          [1, 0],\n",
    "          [0, 1],\n",
    "          [1, 1]] \n",
    "print(\"Input data: \" + str(inputs))\n",
    "\n",
    "target_outputs_AND = [0, 0, 0, 1]\n",
    "print(\"AND: \" + str(target_outputs_AND))\n",
    "\n",
    "target_outputs_OR = [0, 1, 1, 1]\n",
    "print(\"OR: \" + str(target_outputs_OR))\n",
    "\n",
    "target_outputs_XOR = [0, 1, 1, 0]\n",
    "print(\"XOR: \" + str(target_outputs_XOR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Perceptron - Implementation\n",
    "\n",
    "Now lets write a function to build and train a Perceptron.\n",
    "This is just an implementation of the algorithm above, except we are going to train one **step** or one **epoch** at a time.\n",
    "This allows us to see what the algorithm is doing more clearly.\n",
    "\n",
    "- A training **step** applies the algorithm to just one input example (A, B and C above).\n",
    "- An **epoch** repeats the training step for all input examples in the data (so in this case 4).\n",
    "\n",
    "First we define the learning rate and model.\n",
    "\n",
    "- The **learning_rate** variable determines how large a change we will make to the weights each time they are updated.\n",
    "\n",
    "- The **model** is the collection of weight variables that are adjusted ('learned') as the model is trained to produce the \n",
    "target outputs. The model will be stored as a python dictionary to keep everything in one data structure.\n",
    "\n",
    "As it trains you should see output for the current inputs and target outputs,\n",
    "training step, epoch and total error for that epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set the learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialise the model weights to small random values in the range -0.5 to 0.5\n",
    "weight_1 = np.random.uniform(-0.5, 0.5)\n",
    "weight_2 = np.random.uniform(-0.5, 0.5)\n",
    "bias_w = np.random.uniform(-0.5, 0.5)\n",
    "bias = 1\n",
    "# Store the model in a dictionary\n",
    "model = {'weight_1':weight_1, 'weight_2':weight_2, 'bias_weight': bias_w}\n",
    "print(\"Initialised model weights:\")\n",
    "print(model)\n",
    "\n",
    "# Trains the perceptron for one step or epoch\n",
    "def train(inputs, target_outputs, step, num_steps):\n",
    "    # Unpack the model values into local variables\n",
    "    w1, w2, bw = model['weight_1'],  model['weight_2'], model['bias_weight']\n",
    "\n",
    "    # Define some other variables we need\n",
    "    weight_sum, activation, error = 0, 0, 0\n",
    "\n",
    "    # Loop for the desired number of steps (1 step or 1 epoch)\n",
    "    for i in range(step, num_steps):\n",
    "        \n",
    "        # Calculate sum of inputs * weights\n",
    "        weight_sum = (inputs[i][0] * w1) + (inputs[i][1] * w2) + (bias * bw)\n",
    "\n",
    "        # Activation (step) function\n",
    "        if weight_sum > 0:\n",
    "            activation = 1\n",
    "        else:\n",
    "            activation = 0\n",
    "\n",
    "        # Calculate error (target output - actual output)\n",
    "        error = target_outputs[i] - activation\n",
    "\n",
    "        # Update weights (error * input * learning rate)\n",
    "        w1 += error * inputs[i][0] * learning_rate\n",
    "        w2 += error * inputs[i][1] * learning_rate\n",
    "        bw += error * bias * learning_rate\n",
    "        \n",
    "        # Update the graph\n",
    "        perceptron_graph.update_step(model, w1, w2, bw, error)\n",
    "        # Repack the model values from local variables\n",
    "        model['weight_1'], model['weight_2'], model['bias_weight'] = w1, w2, bw\n",
    "\n",
    "# Create the perceptron graph widget\n",
    "perceptron_graph = BinaryPerceptronGraph()\n",
    "train_widget = perceptron_graph.create_train_graph(train, learning_rate)\n",
    "display(train_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Perceptrons - Linear Decision Boundary\n",
    "\n",
    "To give you an intuition for what the Perceptron is doing, consider the equation for a straight line:\n",
    "\n",
    "$y = ax + c$\n",
    "\n",
    "a and c are coefficients just like the learned weights and bias in the Perceptron.\n",
    "So with a bit of rearranging:\n",
    "   \n",
    "$y = (input1 \\times weight1) + (input2 \\times weight2) + bias \\, weight$\n",
    "\n",
    "Becomes:\n",
    "\n",
    "$input2 = (-weight1 \\div weight2) \\times input1) + (-bias \\, weight\\div weight2)$\n",
    "\n",
    "- weights 1 and 2 = slope\n",
    "\n",
    "- bias = intercept\n",
    "\n",
    "- step function = which side of the line!\n",
    "\n",
    "So, the Perceptron is essentially learning a function for a straight line which is called the decision boundary.\n",
    "In this case, which 'class' the set of inputs belongs to i.e. True or False.\n",
    "\n",
    "You can run the below code to create an interactive widget that allows you to manually adjust the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create the perceptron decision boundary widget\n",
    "perceptron_graph = BinaryPerceptronGraph()\n",
    "decision_boundary_widget = perceptron_graph.create_decision_boundary_graph()\n",
    "display(decision_boundary_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO add mcq “if a I have a perceptron that does or, what do I change to make to do amd? Increase decrease w1,2,3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity2: \n",
    "The aim of this task is to illustrate how Perceptrons can be applied to a larger, more realistic, dataset with real numbered features. We will apply the algorithm in using the training step code from before, but this time we will train the perceptron for a fixed number of epochs and then test the model on a held back test set.\n",
    "In the ‘Training and Testing on Real-Fake Data’ section the first cell generates some random training and test data.\n",
    "In the next cell you need to complete the train_perceptron function by copying the relevant code (only the training step!) from previous cells. Once it is complete you should see output of error and accuracy from training.\n",
    "Next you can complete the test_perceptron function in a similar manner and you should see output of how well the trained model performed on the test data.\n",
    "What effect does the learning rate and number of examples per epoch have?\n",
    "Try changing some of the parameters of the randomly created data and see what effect it has.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Perceptrons - Training and Testing on Real-Fake Data\n",
    "\n",
    "Truth table data and logical functions are a good way to learn the Perceptron algorithm but the data isn't very realistic.\n",
    "\n",
    "Most problems are much more complex and cannot be represented with binary data or solved with only 4 training examples.\n",
    "We were also only training for one **step** (one input example) or one **epoch** (all input examples) at a time, so that we\n",
    "could see what the algorithm was doing.\n",
    "\n",
    "In supervised learning, generally we want to train for a fixed number of epochs, or until there is no improvement in\n",
    "the error on the training data. Once training is finished we apply the model (trained weights) to some test data and\n",
    "measure its performance. This gives us an indication of how well it would perform on new data it has not 'seen' before.\n",
    "\n",
    "Next we will train and then test a Perceptron on a larger, real numbered dataset so that we can see the process of \n",
    "applying machine learning in practice.\n",
    "\n",
    "As before, we will first import some python modules and then randomly generate some training and test data.\n",
    "This time the features of the data will be real numbers but there are still only 2 classes/labels, 0 and 1.\n",
    "\n",
    "It's also helpful to plot the data so that we can see how it is distributed.\n",
    "\n",
    "**Note:** The make_blobs function generates a random dataset, the centers variable determines how many classes/labels\n",
    "are in the data, n_features is the number of features each example has and cluster_std is the standard deviation of\n",
    "each class i.e. how randomly scattered they are from eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import some needed modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "# Generate random dataset\n",
    "num_samples = 150\n",
    "features, labels = make_blobs(n_samples=num_samples, centers=2, n_features=2, cluster_std=1.0, random_state=0)\n",
    "\n",
    "# Split data to training and test data, 2/3 for training and 1/3 for testing\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.33)\n",
    "\n",
    "# Print some information about the data\n",
    "print(\"Shape of training data: \" + str(train_x.shape))\n",
    "print(\"Shape of test data: \" + str(test_x.shape))\n",
    "print(\"First 5 features of training data:\")\n",
    "print(train_x[: 5, :])\n",
    "print(\"First 5 labels of training data:\")\n",
    "print(train_y[:5])\n",
    "\n",
    "# Plot the training data\n",
    "figure, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "train_plot = sns.scatterplot(x=train_x[:,0], y=train_x[:,1], hue=train_y, ax=ax[0])\n",
    "ax[0].title.set_text(\"Train\")\n",
    "test_plot = sns.scatterplot(x=test_x[:,0], y=test_x[:,1], hue=test_y, ax=ax[1])\n",
    "ax[1].title.set_text(\"Test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Perceptron - Train Function\n",
    "\n",
    "Now we can define the training function for the Perceptron. It is very similar to the previous, except it will train\n",
    "for the num_epochs that we define.\n",
    "\n",
    "The function **inputs** are the training examples, training labels, number of epochs and learning rate.\n",
    "\n",
    "The function **returns** the trained model i.e. the 3 trained weights.\n",
    "\n",
    "Every epoch will calculate and print the error and accuracy for that epoch.\n",
    "\n",
    "Once training is complete we can use the equation from the *Linear Decision Boundary* section above to draw the\n",
    "decision boundary for the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define the number of training epochs and learning rate\n",
    "num_epochs = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "def train_perceptron(inputs, target_outputs, num_epochs, learning_rate):\n",
    "\n",
    "    # Set the weights to small random values in the range -0.5 to 0.5  \n",
    "    w1 = np.random.uniform(-0.5, 0.5)\n",
    "    w2 = np.random.uniform(-0.5, 0.5)\n",
    "    bw = np.random.uniform(-0.5, 0.5)\n",
    "    bias = 1\n",
    "    print(\"Initialised model weights:\")\n",
    "    print(\"weight_1: \" + str(w1) + \" weight_2: \" + str(w2) + \" bias_weight: \" + str(bw))\n",
    "    \n",
    "    # Each epoch will loop over the training data once\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_error = 0\n",
    "\n",
    "        # Loop over all of the input examples\n",
    "        for i in range(len(inputs)):\n",
    "            error = -1\n",
    "            # Calculate sum of inputs * weights\n",
    "\n",
    "            \n",
    "            # Activation (step) function\n",
    "\n",
    "\n",
    "            # Calculate error (desired output - actual output)\n",
    "\n",
    "            epoch_error += np.absolute(error) # Also keep track of total error for this epoch\n",
    "\n",
    "            # Update weights (error * input * learning rate)\n",
    "\n",
    "        \n",
    "        # Calculate epoch accuracy and print metrics    \n",
    "        epoch_accuracy = (100/len(inputs)) * (len(inputs) - epoch_error)\n",
    "        print(\"Epoch: \" + str(epoch + 1) + \" Error: \" + str(epoch_error) + \" Accuracy: \" + str(round(epoch_accuracy, 2)))\n",
    "    return w1, w2, bw\n",
    "\n",
    "# Call the function to train the Perceptron and return the trained weights\n",
    "input_w1, input_w2, bias_w = train_perceptron(train_x, train_y, num_epochs, learning_rate)\n",
    "\n",
    "print(\"Trained model weights:\")\n",
    "print(\"w1: \" + str(input_w1) + \" w2: \" + str(input_w2) + \" bw: \" + str(bias_w))\n",
    "\n",
    "# Create the range of values for decision boundary\n",
    "x_range = np.linspace(train_x[:,0].min(), train_x[:,0].max(), 10)\n",
    "y_range = [((-input_w1/input_w2) * x) + (-bias_w/input_w2) for x in x_range]\n",
    "\n",
    "# Plot the training data and the decision boundary\n",
    "figure, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(x=train_x[:,0], y=train_x[:,1], hue=train_y, ax=ax)\n",
    "sns.lineplot(x=x_range, y=y_range, color='r', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Perceptron - Test Function\n",
    "\n",
    "Now we can test the trained model by making predictions on the test data.\n",
    "\n",
    "The main difference is that now we **don't** update the weights. So, we just pass in the trained weights and iterate\n",
    "for one epoch over the training data, recording the error and accuracy.\n",
    "\n",
    "Finally we can plot the test data and decision boundary as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_perceptron(inputs, target_outputs, w1, w2, bw):\n",
    "    \n",
    "    # Keep track of the error on the test data\n",
    "    test_error = 0\n",
    "    \n",
    "    # Loop over all of the test examples\n",
    "    for i in range(len(inputs)):\n",
    "        error = -1\n",
    "\n",
    "\n",
    "\n",
    "        test_error += np.absolute(error) # Also keep track of total error testing\n",
    "      \n",
    "    # Calculate epoch accuracy and print metrics    \n",
    "    test_accuracy = (100/len(inputs)) * (len(inputs) - test_error)\n",
    "    print(\"Test Error: \" + str(test_error) + \" Accuracy: \" + str(round(test_accuracy, 2)))\n",
    "\n",
    "# Call the test perceptron function with the test data and trained model    \n",
    "test_perceptron(test_x, test_y, input_w1, input_w2, bias_w)\n",
    "\n",
    "# Create the range of values for decision boundary\n",
    "x_range = np.linspace(test_x[:,0].min(), test_x[:,0].max(), 10)\n",
    "y_range = [((-input_w1/input_w2) * x) + (-bias_w/input_w2) for x in x_range]\n",
    "\n",
    "# Plot the training data and the decision boundary\n",
    "figure, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(x=test_x[:,0], y=test_x[:,1], hue=test_y, ax=ax)\n",
    "sns.lineplot(x=x_range, y=y_range, color='r', ax=ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
