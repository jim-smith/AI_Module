{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbook 6: Supervised Machine Learning\n",
    "\n",
    "## Description and aims\n",
    "\n",
    "This tutorial is designed to give you your first experience of machine learning in practice by implementing a simple nearest-neighbour classifier.\n",
    "\n",
    "The learning outcomes are:\n",
    "- experience of implementing the K Nearest Neighbours classification algorithm\n",
    "- experience of using the sklearn DecisionTree classification algorithm\n",
    "-  experience of working through different preprocessing steps to try and improve the performance of your classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1: Getting to know your data: \n",
    "\n",
    "We will start by importing and visualising the two datasets used as examples in the lecture: students marks,  and Iris\n",
    "### You should already have uploaded the data and figures from the lecture materials folder - if not, do so now.\n",
    "### Then run the 5 code cells below to load and display the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_scatterplot_matrix(X,y,featureNames,title=None):\n",
    "    f = X.shape[1]\n",
    "    if(len(y) != X.shape[0]):\n",
    "        print(\"Error,   the y array  must have the same length as there are rows in X\")\n",
    "        return\n",
    "    fig, ax = plt.subplots(f,f,figsize=(12,12))\n",
    "    plt.set_cmap('jet')\n",
    "    for feature1 in range(f):\n",
    "        ax[feature1,0].set_ylabel( featureNames[feature1])\n",
    "        ax[0,feature1].set_xlabel( featureNames[feature1])\n",
    "        ax[0,feature1].xaxis.set_label_position('top') \n",
    "        for feature2 in range(f):\n",
    "            xdata = X[:,feature1]\n",
    "            ydata = X[:,feature2]\n",
    "            ax[feature1, feature2].scatter(xdata,ydata,c=y)\n",
    "    if title != None:\n",
    "        fig.suptitle(title,fontsize=16,y=0.925)\n",
    "        \n",
    "        \n",
    "# simple function - currently only works for 2D data - but could easily be extended\n",
    "def PlotDecisionSurface(trainX,trainy,theClassifier,theTitle,featureNames,xvar=0,yvar=1,stepSize=2.0,minZero=False):\n",
    "    #create and prettify the plot\n",
    "    cmap=\"Set3\"\n",
    "    fig,ax= plt.subplots(figsize=(8, 8))\n",
    "    ax.set_title(theTitle)\n",
    "    ax.set_xlabel(featureNames[xvar])\n",
    "    ax.set_ylabel(featureNames[yvar])\n",
    "\n",
    "    #define a grid we use to plot the decision boundaries\n",
    "      #get max/min values for gri edges\n",
    "    columnMax,columnMin = np.max(trainX,axis=0), np.min(trainX,axis=0)\n",
    "    if(minZero==True):\n",
    "        x_min , y_min= 0,0\n",
    "    else:\n",
    "        x_min, y_min = columnMin[ xvar]*0.95, columnMin[yvar]*0.95\n",
    "    x_max, y_max = columnMax[xvar]*1.05, columnMax[yvar]*1.05 \n",
    "    #make the grid\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, stepSize),np.arange(y_min, y_max, stepSize))\n",
    "\n",
    "    #predict and plotfor evey point on the grid\n",
    "    Z = theClassifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z,cmap=cmap)\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(x=trainX[:,xvar ],y= trainX[:, yvar], c=trainy.astype(float), alpha=1.0, cmap=cmap, edgecolor=\"black\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Student marks dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grades= np.genfromtxt(\"../lectures/data/assessment-grades-2features.csv\", delimiter= ',',skip_header=1)\n",
    "\n",
    "featureNames=(\"exam\", \"CW_mean\")\n",
    "nStudents = grades.shape[0]\n",
    "\n",
    "outcomes= (\"Pass\",\"Resit Exam\", \"Resit Coursework\",\"Resit Both\")\n",
    "simpleoutcomes= (\"pass\",\"resit\")\n",
    "\n",
    "# make target labels\n",
    "result = np.empty(nStudents, dtype=np.int8)\n",
    "\n",
    "for row in range (nStudents):\n",
    "    exam = grades[row][0]\n",
    "    cw   = grades[row][1]\n",
    "    if (exam>=35 and cw>=35 and (exam +cw >=80) ):\n",
    "        result[row] = 0 # PASS \n",
    "\n",
    "    elif ( cw>=40 and exam < 40):\n",
    "        result[row] = 1 #resit just exam \n",
    "    elif ( cw<40 and exam>=40):\n",
    "        result[row]= 2 # resit just coursework\n",
    "    else:\n",
    "        result[row] = 3  # resit both\n",
    "        \n",
    "simpleResult = np.where(result<1,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easiest to split the data into 4/2 subgroups ot plot the outomes /simplified outcomes\n",
    "\n",
    "passStudents = np.empty((0,2))\n",
    "resitCWStudents = np.empty((0,2))\n",
    "resitExamStudents = np.empty((0,2))\n",
    "resitBothStudents = np.empty((0,2))\n",
    "\n",
    "for student in range (nStudents):\n",
    "    if (result[student]==0):\n",
    "        passStudents = np.vstack( (passStudents,grades[student]) )\n",
    "    elif (result[student]==1):\n",
    "        resitExamStudents = np.vstack( (resitExamStudents,grades[student]) )\n",
    "    elif (result[student]==2):\n",
    "        resitCWStudents = np.vstack( (resitCWStudents,grades[student]) )\n",
    "    else:\n",
    "        resitBothStudents = np.vstack( (resitBothStudents,grades[student]) )\n",
    "simpleResitStudents = np.vstack( (resitExamStudents,resitCWStudents,resitBothStudents))\n",
    "\n",
    "print(passStudents.shape)\n",
    "print(resitExamStudents.shape)\n",
    "print(resitCWStudents.shape)\n",
    "print(resitBothStudents.shape)\n",
    "print(simpleResitStudents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(14,5))\n",
    "plt.xlabel(\"Exam\")\n",
    "plt.ylabel(\"Coursework\")\n",
    "ax[0].set_title(\"Outcomes\")\n",
    "ax[1].set_title(\"Simplified Outcomes\")\n",
    "\n",
    "ax[0].scatter(passStudents[:,0],passStudents[:,1],label = \"Pass\" )\n",
    "ax[0].scatter(resitExamStudents[:,0],resitExamStudents[:,1],label = \"Resit Exam\" )\n",
    "ax[0].scatter(resitCWStudents[:,0],resitCWStudents[:,1],label = \"Resit CW\" )\n",
    "ax[0].scatter(resitBothStudents[:,0],resitBothStudents[:,1],label = \"Resit Both\" )\n",
    "ax[1].scatter(passStudents[:,0],passStudents[:,1],label = \"Resit\" )\n",
    "ax[1].scatter(simpleResitStudents[:,0],simpleResitStudents[:,1],label = \"Pass\" )\n",
    "\n",
    "ax[0].legend(loc='lower right')\n",
    "ax[1].legend(loc='lower right') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:  Iris flowers <img src=\"../lectures/figures/ML/Iris-image.png\" style=\"float:right\">\n",
    "- classic Machine Learning Data set\n",
    "- 4 measurements: sepal and petal width and length\n",
    "- 50 examples  from each 3 sub-species for iris flowers\n",
    "- three class problem:\n",
    " - so for some types of algorithm have to decide whether to make  \n",
    "   a 3-way classifier or nested 1-vs-rest classifers\n",
    "- most ML classifiers can get over 90%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "irisX,irisy = sklearn.datasets.load_iris(return_X_y=True)\n",
    "columnLabels= (\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\")\n",
    "title=\"Scatterplots of 2D slices through the 4D Iris data\"\n",
    "show_scatterplot_matrix(irisX,irisy,columnLabels,title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2: Implementing K-Nearest Neighbours\n",
    "Below is the pseudocode for the K-nearest Neighbours algorithm.\n",
    "- Make sure you understand this,   \n",
    "- Then read the cell below which is my implentation for K=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode for KNearest Neighbours\n",
    "**init()**  :  \n",
    "SPECIFY function to calculate distance metric d(i,j) for any two items *i* and *j*     \n",
    "  e.g. Euclidean (continuous variables) or Hamming (categorical)  \n",
    "SET value of K\n",
    "\n",
    "**fit(trainingData)** :  \n",
    "\n",
    "SET numExemplars = READ(number of rows in training data)  \n",
    "SET numFeatures = READ(number of columns in training data)  \n",
    "*#Just store a local copy of the training data as two arrays:*   \n",
    "CREATE_AND_FILL(X_train of shape (numExemplars , numFeatures)).     \n",
    "CREATE_AND_FILL(y_train of shape( numExemplars))\n",
    "  \n",
    "**predict(newItems)** :  \n",
    "\n",
    "\n",
    "*Step 1:   Make 2D array distances of shape (num_newItems , numExemplars)*   \n",
    "SET numToPredict = READ(number of rows in newItems)  \n",
    "FOREACH newItem in (0...,numToPredict-1)    \n",
    "...FOREACH exemplar in (0,...,numExemplars -1)    \n",
    ".....SET distances [newItem] [exemplar] = d (newItem , X_train[exemplar] )   \n",
    "\n",
    "*Step 2: Get indexes of the k nearest neighbours for each new item*    \n",
    "SET closestK = CREATE(2DArray with numToPredict rows and K columns)  \n",
    "FOREACH newItem in (0...,numToPredict-1)        \n",
    "...SET distFromNewItem = CREATE(2D array with  2 columns, and numExemplars rows)  \n",
    "...FOREACH exemplar in (0,...,numExemplars -1)    \n",
    ".......SET distFromNewItem[exemplar][0] = distances[newItem][exemplar] *#column 0 holds distance from new item*  \n",
    ".......SET distFromNewItem[exemplar][1] = exemplar          *#column 1 holds the index in the training set*  \n",
    "...SET  sortedByDist = SORT (rows of distFromNewItem by increasing distances (column 0) )  \n",
    "...FOREACH k in ( 0,...,K-1)  \n",
    ".....SET closestK[newItem][k] = sortedByDist[k][1] *#column 1 of each row holds the index*   \n",
    " \n",
    "  \n",
    "*Step 3: Store majority vote in a  1D array y_pred with numToPredict entries*     \n",
    "FOREACH newItem in (0...,numToPredict-1)  \n",
    "...SET labelcounts = CREATE(1D array with m zero values)  \n",
    "...FOREACH  k in (0,...K-1)   \n",
    "...... SET thisindex = cloesetK[newItem][k]  \n",
    "...... SET thislabel = y_train[thisindex]  \n",
    "...... INCREMENT labelcounts[thislabel]  \n",
    "...SET thisPrediction = READ(index of labelcounts with highest value)    \n",
    "...SET y_pred[newItem] = thisPrediction  \n",
    " \n",
    "RETURN y_pred  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for K = 1 \n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "class simple_1NN:\n",
    "\n",
    "    def __init__():\n",
    "        self.K=1\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.numExemplars = X.shape[0]\n",
    "        self.numFeatures = X.shape[1]\n",
    "        self.modelX = X\n",
    "        self.modelY = y\n",
    "        \n",
    "    def predict(self,newItems):\n",
    "        numToPredict = newItems.shape[0]\n",
    "        yPred = np.zeros((numToPredict,1))\n",
    "        \n",
    "        # measure distances - creates an array with numToPredict rows and num_trainItems columns\n",
    "        dist = euclidean_distances(newItems,self.modelX)\n",
    "\n",
    "        #make predictions: This is K=1, TO DO- in your own time extend to work with K>1\n",
    "        closest = np.argmin(dist, axis=1) \n",
    "        # this is a 1D array with numToPredict entries, \n",
    "        # closest i holds the index (0...self.Numexplars -1) of the column j with the smalles value of dist[i][j] \n",
    "        for item in range(numToPredict):\n",
    "            yPred[item] = self.modelY [ closest[item]]\n",
    "        \n",
    "        return yPred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.1 Run the code provided for K=1 with the two datasets and make sure you understand the outputs and how they are produced\n",
    "- for the marks dataset this creates a plot to show a decision surface\n",
    "- for the  iris data set this uses a confision matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Marks dataset - illustrating a 2D Decision surface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create and train the classifier\n",
    "myKNNmodel = simple_1NN()\n",
    "myKNNmodel.fit(grades,simpleResult) \n",
    "\n",
    "#visualise the decision surface\n",
    "PlotDecisionSurface(grades, simpleResult, myKNNmodel,\"1-NN simplified outcomes\", (\"exam\",\"cw\"),minZero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Iris dataset - illustrating a confusion matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix for lateral flow tests\n",
    "\n",
    "\n",
    " Actual |  Predicted Covid.  | Predicted Not Covid |\n",
    " ---|---|---|\n",
    " covid.  | 50. | 50 |\n",
    " not covid| 0 | 100 |\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train/test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "irisX,irisy = load_iris(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(irisX, irisy, test_size=0.33,stratify=irisy)\n",
    "\n",
    "\n",
    "model = simple_1NN()\n",
    "model.fit(X_train,y_train)\n",
    "ypred = model.predict(X_test)\n",
    "confusionMatrix = np.zeros((3,3),int)\n",
    "for i in range(50):\n",
    "    actual = int(y_test[i])\n",
    "    predicted = int(ypred[i])\n",
    "    confusionMatrix[actual][predicted] += 1\n",
    "print(confusionMatrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.2 (stretch): edit the code in the cells above to produce:\n",
    "- a confusion matrix for the marks dataset\n",
    "- a decision surface for the four-class version of the marks dataset  \n",
    "  i.e. using the labels held in the array \"results\" instead of \"simplifiedResults\"\n",
    "- a decision surface for the Iris Data  \n",
    "  you will need to choose just two features and use 'slicing' create a training set with just those columns\n",
    "  - you could use the first two, or the best two you identified last week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.3: Create your own implementation of K-Nearest Neighbours\n",
    "Using the code above,  extend the predict method for the class simple_1NN  to use the votes from K>1 neighbours.\n",
    "\n",
    "\n",
    "These are the lines you will need to change:\n",
    "````  \n",
    "#make predictions: This is K=1, TO DO- in your own time extend to work with K>1\n",
    "        for item in range(numToPredict):\n",
    "            closest = np.argmin(dist, axis=1) \n",
    "            yPred[item] = self.modelY [ closest[item]]\n",
    "```` \n",
    "\n",
    "Some hints: \n",
    "- Make sure you change the class name to something appropriate\n",
    "- You should  set (and store) the value of K in the constructor method\n",
    "- If I have an array of myDist of (say, for simplicity) five values,  \n",
    "   and I want to resort them by size, keep track of the item ids  \n",
    "   so i can find the K with the smallest values in the original array. \n",
    "   \n",
    "   The image shows the idea of how to do this <img src=\"howto-get-K-closest.png\" style=\"float:right\" width = 50%>\n",
    "\n",
    "\n",
    "   I can do it using the code below, which:\n",
    "   - creates a 2d array holding each value and its index in the original array\n",
    "   - then makes a new array which is the 'unSorted' 2d array, but sorted by the values in the first column ([0])\n",
    "   - then looks in the second column (which holds the indexes) for the first K rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This cells just contains some hints about how to prodice a sorted array\n",
    "myDist = np.array([4,7,1,3,9])\n",
    "\n",
    "myUnsortedArray = np.empty((5,2))\n",
    "print(myUnsortedArray.shape)\n",
    "for row in range (5):\n",
    "    myUnsortedArray [row][0] = myDist[row]\n",
    "    myUnsortedArray [row][1] = row\n",
    "print('myUnsortedArray contents before sorting')\n",
    "print(myUnsortedArray)\n",
    "\n",
    "in==\n",
    "\n",
    "print('mySortedArray contents after sorting')\n",
    "print(mySortedArray)\n",
    "\n",
    "print('Last week we learned about slicing- we will now use slicing to pull out all rows but just specific columns')\n",
    "print(' the values in array myDist in ascending order are: {}'.format(mySortedArray[:,0]))\n",
    "print(' the positions those values were in, again by ascending order of value are : {}'.format(=))\n",
    "print('Now remember you do not have to choose every row ... ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your KNN class code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.4: Test your impementation on the two example datasets\n",
    "\n",
    "**Use the student marks for qualititative judgements** : how does the decision rurface change?  \n",
    "**Use the Iris data set for quantitative judgements** :  how does the confusion matrix change?\n",
    "just ot visualise what is hapening,  the Iris data to \n",
    "- use the toolbar to copy and paste the two cells from activity 2.1 below here\n",
    "- then edit them so that they create and use objects of your new class, instead of the class simple_1NN\n",
    "\n",
    "- start with K=1 - this should produce the same results as you got in activity 2.1\n",
    "- then try with K = {3,5,7}\n",
    "- what happens to the accuracy?\n",
    "- what happens to the decision surface?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3: Decision Trees\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Activity 3.1: Run the next cell to remind yourself how the decision trree model is grown\n",
    "The next cell just illustrates how the tree induction process works for the student marks dataset.\n",
    "It just calls the decision tree repeatedly for increasing depths.\n",
    "For depth 0 I've just created a text box with the relevant stats in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(18,8))\n",
    "fig.suptitle(\"Illustration of how Decision Trees select and insert nodes to increase data purity\")\n",
    "for depth in range (0,3):\n",
    "    if(depth==0):\n",
    "        ax[0].text(0.25, 0.6, \" gini=0.147\\n samples=150,\\n value=[138,12],\\n class=pass\",fontsize=14, \n",
    "        bbox={'facecolor': 'darkOrange', 'alpha': 0.5, 'pad': 10})\n",
    "        ax[0].axes.get_yaxis().set_visible(False)\n",
    "        ax[0].axes.get_xaxis().set_visible(False)\n",
    "        ax[0].set_frame_on(False)\n",
    "        ax[0].set_title(\"Depth 0\")\n",
    "    else:\n",
    "        model = DecisionTreeClassifier(random_state=1234, max_depth=depth,min_samples_split=2,min_samples_leaf=1)\n",
    "        model.fit(grades,simpleResult)\n",
    "        _ = tree.plot_tree(model, feature_names=(\"exam\",\"coursework\"), class_names= (\"pass\",\"resit\"),filled=True,ax=ax[depth])\n",
    "        ax[depth].set_title(\"Depth \"+str(depth))\n",
    "        \n",
    "fig.savefig(\"DecisionTreeExample-studentMarks.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.2: exploring how to control tree-growth to prevent over-fitting\n",
    "\n",
    "The aim of this activity is for you to experiment with what happens when you change three parameters that affect how big and complex the tree is allowed to get.\n",
    "- max_depth\n",
    "- min_samples_split, (default value is 2)\n",
    "- min_samples_leaf, (default value is 1)\n",
    "\n",
    "Experiment with the Iris data set below to see if you can work out what each of these parameters does, and how it affects the tree \n",
    "\n",
    "- Each time you run the  cell below, it will give you a different train-test split of the Iris data.\n",
    "  Does this affect what tree you get?\n",
    "  \n",
    "- Is there a combination of values that means you consistently get similar trees?\n",
    "- What is a good way of judging 'similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load iris dataset and split into train:test\n",
    "iris = sklearn.datasets.load_iris()\n",
    "irisX = iris.data\n",
    "irisy = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(irisX, irisy, test_size=0.33,stratify=irisy)\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=1234, max_depth=None,min_samples_split=2,min_samples_leaf=1)\n",
    "model.fit(X_train,y_train)\n",
    "ypred = model.predict(X_test)\n",
    "confusionMatrix = np.zeros((3,3),int)\n",
    "for i in range(50):\n",
    "    actual = int(y_test[i])\n",
    "    predicted = int(ypred[i])\n",
    "    confusionMatrix[actual][predicted] += 1\n",
    "print(confusionMatrix)\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "_ = tree.plot_tree(model, \n",
    "                   feature_names=iris.feature_names,  \n",
    "                   class_names=iris.target_names,\n",
    "                   filled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 4: (stretch)\n",
    "Using the code from last week,  apply a StandardScaler to the Iris data set and evaluate the effect this has on the accuracy.\n",
    "\n",
    "Because there is a random element in how  the data set is split into training / test split,  it is not valid just to split the data once then compare the results with / without scaling.\n",
    "\n",
    "Instead  you will need to do ten repeats  of:\n",
    "- Use the sklearn method to split the data into 66:34 train/test sets\n",
    "- Construct,  train, and test,  an instance of your kNN model on the unscaled data and store its accuracy \n",
    "- Create an instance of the standard scaler and then:\n",
    "  - call its fit() method to set its parameters from the training set.\n",
    "  - call its transform() method for both the traing and test sets\n",
    "  - Construct,  train, and test,  an instance of your kNN model on this scaled data and store its accuracy \n",
    "\n",
    "That should gives you ten pairs of values (one per repeat) for the scaled and raw data accuracy.  \n",
    "Use an online statistical tool (e.g. https://www.graphpad.com/quickcalcs/ttest1.cfm) that lets you copy your data in the perform a 'paired t-test\" to find out the probability that normalising the data improves prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"> Please save your work (click the save icon) then shutdown the notebook when you have finished with this tutorial (menu->file->close and shutdown notebook</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> Remember to download and save your work if you are not running this notebook locally.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
