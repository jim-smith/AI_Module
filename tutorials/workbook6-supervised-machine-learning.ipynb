{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbook 6: Supervised Machine Learning\n",
    "\n",
    "## Description and aims\n",
    "\n",
    "This tutorial is designed to give you your first experience of machine learning in practice by implementing a simple nearest-neighbour classifier.\n",
    "\n",
    "The learning outcomes are:\n",
    "- experience of implementing the K Nearest Neighbours classification algorithm\n",
    "- experience of using the sklearn DecisionTree classification algorithm\n",
    "-  experience of working through different preprocessing steps to try and improve the performance of your classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\">\n",
    "    <h1>Activity 1: Loading and Visualising Data</h1>\n",
    "   We will start by importing and visualising the two datasets used as examples in the lecture: students marks,  and Iris\n",
    "<ul>\n",
    "    <li>You should already have uploaded the data and figures from the lecture materials folder - if not, do so now.</li>\n",
    "    <li>Then run the 5 code cells below to load and display the two datasets</li>\n",
    "            </ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import workbook6_utilities as wb6\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Student marks dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grades, result, simpleResult = wb6.load_student_marks_dataset(\"../lectures/data/assessment-grades-2features.csv\")\n",
    "\n",
    "wb6.plot_student_marks(grades,result,simpleResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:  Iris flowers <img src=\"../lectures/figures/ML/Iris-image.png\" style=\"float:right\">\n",
    "- classic Machine Learning Data set\n",
    "- 4 measurements: sepal and petal width and length\n",
    "- 50 examples  from each 3 sub-species for iris flowers\n",
    "- three class problem:\n",
    " - so for some types of algorithm have to decide whether to make  \n",
    "   a 3-way classifier or nested 1-vs-rest classifers\n",
    "- most ML classifiers can get over 90%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "irisX,irisy = sklearn.datasets.load_iris(return_X_y=True)\n",
    "columnLabels= (\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\")\n",
    "title=\"Scatterplots of 2D slices through the 4D Iris data\"\n",
    "wb6.show_scatterplot_matrix(irisX,irisy,columnLabels,title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\">\n",
    "    <h1>Activity 2: Implementing K-Nearest Neighbours</h1>\n",
    "</div>\n",
    "            \n",
    "Basic process for predicting the label of a new point from the trainig set\n",
    "1. Measure distance to new poitn from every member of the trainig set\n",
    "2. Find the K Nearest Neighbours  \n",
    "   in other words, the K members of the trainig set with the smallest distances  (*calculated in step 1*)\n",
    "3. Count the labels that were provided for those K trainig items,  \n",
    "   and return themost common one as the predicted label.\n",
    "\n",
    "Below is a figure illustrating the start and first two steps of process.  \n",
    "It is followed by a code cell with a simple implementation of a class for 1-Nearest neighbours. \n",
    "\n",
    "Read through the code  to get a sense for how it implements the algorithm.  \n",
    "Your tutor will discuss it with you in the lab sessions.\n",
    "<img src=\"../lectures/figures/ML/kNN-steps.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for K = 1 \n",
    "\n",
    "class simple_1NN:\n",
    "\n",
    "    def __init__(self,verbose = True):\n",
    "        # this version only looks at the single nearest neighbour\n",
    "        self.K=1\n",
    "        self.verbose= verbose\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        # ask the data how big it is and store that info\n",
    "        self.numExemplars = X.shape[0]\n",
    "        self.numFeatures = X.shape[1]\n",
    "        # store a copy of the data (X) and the labels (y)\n",
    "        self.modelX = X\n",
    "        self.modelY = y\n",
    "        self.labelsPresent = np.unique(self.modelY) # list the unique values found in the labels provided\n",
    "        if (self.verbose):\n",
    "            print(\"There are {} training examples, each described by values for {} features\".format(self.numExemplars,self.numFeatures))\n",
    "            print(\"So self.modelX is a 2D array of shape {}\".format(self.modelX.shape))\n",
    "            print(\"self.modelY is a list with {} entries, each being one of these labels {}\".format(len(self.modelY), self.labelsPresent))\n",
    "        \n",
    "    def predict(self,newItems):\n",
    "        # read how many  newitems there are\n",
    "        numToPredict = newItems.shape[0]\n",
    "        # make an empty list to hold their predicted labels\n",
    "        predictions = np.empty(numToPredict)\n",
    "        \n",
    "        #loop through each new item each one\n",
    "        for item in range(numToPredict):\n",
    "            # predicting its label\n",
    "            thisPrediction = self.PredictNewItem ( newItems[item])\n",
    "            # adding that predictin to our list\n",
    "            predictions[item] = thisPrediction\n",
    "        return predictions\n",
    "    \n",
    "    def PredictNewItem(self,newItem):\n",
    "        \n",
    "        # Step 1: measure and store distance to each training item\n",
    "        distFromNewItem = np.zeros((self.numExemplars)) # array with one entry for each trainig set item, intialised to zero\n",
    "        for exemplar in range (self.numExemplars):\n",
    "            distFromNewItem[exemplar] = self.EuclideanDistance(newItem,  self.modelX[exemplar])\n",
    "  \n",
    "        # Step 2: find the one closest training example: This is K=1, \n",
    "        closest = 0\n",
    "        for trainingExample in range (0, self.numExemplars):\n",
    "            if  ( distFromNewItem[trainingExample] < distFromNewItem[closest] ):\n",
    "                closest=trainingExample\n",
    " \n",
    "        # step 3: count the votes - because this is for K=1 so we don't need to take a vote\n",
    "        labelOfClosest = self.modelY[closest]\n",
    "        return labelOfClosest\n",
    "    \n",
    "    def EuclideanDistance(self,a,b):\n",
    "        ## this numpy function calculates the euclidean distance\n",
    "        return np.linalg.norm(a-b)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-warning\" style=\"color:black\" >\n",
    "<h2> Activity 2.1</h2>\n",
    "    Run the code provided for K=1 with the two datasets and make sure you understand the outputs and how they are produced\n",
    "<ul>\n",
    "    <li>For the marks dataset this creates a plot to show a decision surface</li>\n",
    "    <li>For the  iris data set this uses a confusion matrix <br> (google what a confusion matrix is if you're not sure)</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Marks dataset - illustrating a 2D Decision surface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# create and train the classifier\n",
    "myKNNmodel = simple_1NN()\n",
    "myKNNmodel.fit(grades,simpleResult) \n",
    "\n",
    "\n",
    "#visualise the decision surface\n",
    "wb6.PlotDecisionSurface(grades, simpleResult, myKNNmodel,\"1-NN simplified outcomes\", (\"exam\",\"cw\"),minZero=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Iris dataset - illustrating a confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train/test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "irisX,irisy = load_iris(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(irisX, irisy, test_size=0.33,stratify=irisy)\n",
    "\n",
    "irisClassNames = (\"setosa\",\"versicolor\",\"virginica\")\n",
    "\n",
    "model = simple_1NN()\n",
    "model.fit(X_train,y_train)\n",
    "ypred = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, ypred)\n",
    "CMPlot=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=irisClassNames)\n",
    "CMPlot.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\" >\n",
    "<h2> Activity 2.3: Create your own implementation of K-Nearest Neighbours</h2>\n",
    "    Using the code above,  extend the predict method for the class simple_1NN  to use the votes from K>1 neighbours.\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li>Start by creating an empty class called Simple_KNN and copying in the pseudo-code as comments</li>\n",
    "    <li>Then copy the code from the simple_1NN class into the relevant places</li>\n",
    "    <li> You should only need to make minor changes to the __init__ method to set the value of K </li>\n",
    "    <li> in the predictNewItem() method you will need to change step 2  and step 3 </li>\n",
    "    <li> The pseudocode suggests one possible ways of doing step 2. </li>\n",
    "    </ul>\n",
    "    <b> It's often helpful to put in some print() statements to show what is going on as you develop your code</b><br>\n",
    "        And if you can write your code  so that it runs in 'partially completed' state then you can build it up in bits.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode for KNearest Neighbours\n",
    "**init()**  :  \n",
    "SPECIFY function to calculate distance metric d(i,j) for any two items *i* and *j*     \n",
    "  e.g. Euclidean (continuous variables) or Hamming (categorical)  \n",
    "SET value of K\n",
    "\n",
    "**fit(trainingData)** :  \n",
    "\n",
    "SET numExemplars = READ(number of rows in training data)  \n",
    "SET numFeatures = READ(number of columns in training data) \n",
    "\n",
    "*#Just store a local copy of the training data as two arrays:*   \n",
    "CREATE_AND_FILL(X_train of shape (numExemplars , numFeatures)).     \n",
    "CREATE_AND_FILL(y_train of shape( numExemplars))\n",
    "  \n",
    "**predict(newItems)** :  \n",
    "SET numToPredict = READ(number of rows in newItems)  \n",
    "SET predictions = CREATE_EMPTYARRAY( numToPredict)\n",
    " \n",
    "FOREACH item in (0...,numToPredict-1)    \n",
    "...SET predictions[item] = predictNewItem ( newItems[item]) \n",
    " \n",
    "RETURN predictions  \n",
    "\n",
    "\n",
    "**predictNewItem(newItem)**:\n",
    "\n",
    "*Step 1:   Make 1D array distances from newItem to each trainig set item*   \n",
    "FOREACH exemplar in (0,...,numExemplars -1  \n",
    "...SET distFromNewItem [exemplar] = d (newItem , X_train[exemplar] )   \n",
    "\n",
    "*Step 2: Get indexes of the k nearestk neighbours for our new item*        \n",
    "SET closestK = GET_IDS_OF_K_CLOSEST(K,distFromNewItem)\n",
    " \n",
    "  \n",
    "*Step 3: Store majority vote in a  1D array y_pred with numToPredict entries*     \n",
    "SET labelcounts = CREATE(1D array with m zero values)  \n",
    "\n",
    "FOREACH  k in (0,...K-1)   \n",
    "... SET thisindex = closestK[newItem][k]  \n",
    "... SET thislabel = y_train[thisindex]  \n",
    "... INCREMENT labelcounts[thislabel]  \n",
    "\n",
    "SET thisPrediction = READ(index of labelcounts with highest value)    \n",
    "\n",
    "RETURN thisPrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTION GET_IDS_OF_K_CLOSEST  \n",
    "PARAMETER distFromNewItem # distance matrix  \n",
    "PARAMETER K  \n",
    "\n",
    "\n",
    "\n",
    "SET closestK= EMPTYLIST  \n",
    "SET arraySize = len(distFromNewItem)  \n",
    "\n",
    "FOR k in (0,...,K-1)  \n",
    "... SET thisClosest=0  \n",
    "... FOR exemplar in (1,...,arraySize -1)  \n",
    "......IF ( distFromNewItem[exemplar] < distFromNewItem[thisClosest]  )  \n",
    "......... SET thisClosest = exemplar  \n",
    "... SET closestK[k] = thisClosest # store this id  \n",
    "... SET distFromNewItem[thisClosest] = BigNumber # so we don't pick it again in next loop\n",
    "\n",
    "RETURN closestK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your KNN class code here\n",
    "\n",
    "class simple_KNN:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\">\n",
    "<h2> Activity 2.4: Test your implementation on the two example datasets</h2>\n",
    "Use the toolbar to copy and paste the two cells from activity 2.1 below here. <br>\n",
    "Then edit them so that they create and use objects of your new class, instead of the class simple_1NN\n",
    "\n",
    "Start with K=1 - this should produce the same results as you got in activity 2.1, then try with K = {3,5,7}\n",
    "<ul>\n",
    "    <li>Use the student marks for <b>qualititative</b> judgements : how does the decision surface change?</li>\n",
    "    <li>Use the Iris data set for <b>quantitative</b> judgements :  how does the confusion matrix change?</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\" >\n",
    "<h1> Activity 3: Decision Trees</h1></div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The next image below  illustrates how the tree induction process works for the student marks dataset.  \n",
    "- It was generated by calling the decision tree repeatedly for increasing depths.\n",
    "- For depth 0 I've just created a text box with the relevant stats in.\n",
    "<img src=\"DecisionTreeExample-studentMarks.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\">\n",
    "<h2> Activity 3.1: exploring how to control tree-growth to prevent over-fitting</h2>\n",
    "The aim of this activity is for you to experiment with what happens when you change three parameters that affect how big and complex the tree is allowed to get.\n",
    "<ul>\n",
    "    <li> max_depth</li>\n",
    "    <li>min_samples_split, (default value is 2)</li>\n",
    "    <li>min_samples_leaf, (default value is 1)</li>\n",
    "    </ul>\n",
    "\n",
    "\n",
    "Experiment with the Iris data set below to see if you can work out what each of these parameters does, and how it affects the tree \n",
    "<ul>\n",
    "<li> Each time you run the  cell below, it will give you a different train-test split of the Iris data.<br>\n",
    "    Does this affect what tree you get? </li>\n",
    "    <li> Is there a combination of values that means you consistently get similar trees?</li>\n",
    "    <li>    What is a good way of judging 'similarity?</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "# load iris dataset and split into train:test\n",
    "iris = sklearn.datasets.load_iris()\n",
    "irisX = iris.data\n",
    "irisy = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(irisX, irisy, test_size=0.33,stratify=irisy)\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=1234, max_depth=None,min_samples_split=2,min_samples_leaf=1)\n",
    "model.fit(X_train,y_train)\n",
    "ypred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, ypred)\n",
    "CMPlot=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)\n",
    "CMPlot.plot()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "_ = tree.plot_tree(model, feature_names=iris.feature_names,  class_names=iris.target_names, filled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\" > <h1> Activity 4: (stretch)</h1></div>\n",
    "Using the code from last week,  apply a StandardScaler to the Iris data set and evaluate the effect this has on the accuracy.\n",
    "\n",
    "Because there is a random element in how  the data set is split into training / test split,  it is not valid just to split the data once then compare the results with / without scaling.\n",
    "\n",
    "Instead  you will need to do ten repeats  of:\n",
    "- Use the sklearn method to split the data into 66:34 train/test sets\n",
    "- Construct,  train, and test,  an instance of your kNN model on the unscaled data and store its accuracy \n",
    "- Create an instance of the standard scaler and then:\n",
    "  - call its fit() method to set its parameters from the training set.\n",
    "  - call its transform() method for both the traing and test sets\n",
    "  - Construct,  train, and test,  an instance of your kNN model on this scaled data and store its accuracy \n",
    "\n",
    "That should gives you ten pairs of values (one per repeat) for the scaled and raw data accuracy.  \n",
    "Use an online statistical tool (e.g. https://www.graphpad.com/quickcalcs/ttest1.cfm) that lets you copy your data in the perform a 'paired t-test\" to find out the probability that normalising the data improves prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"> Please save your work (click the save icon) then shutdown the notebook when you have finished with this tutorial (menu->file->close and shutdown notebook</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> Remember to download and save your work if you are not running this notebook locally.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
