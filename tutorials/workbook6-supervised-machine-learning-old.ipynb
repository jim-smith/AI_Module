{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbook 6: Supervised Machine Learning\n",
    "\n",
    "## Description and aims\n",
    "\n",
    "This tutorial is designed to give you your first experience of machine learning in practice by implementing a simple nearest-neighbour classifier.\n",
    "\n",
    "The learning outcomes are:\n",
    "- experience of implementing the K Nearest Neighbours classification algorithm\n",
    "- experience of using the sklearn DecisionTree classification algorithm\n",
    "-  experience of working through different preprocessing steps to try and improve the performance of your classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h1>Activity 1: Loading and Visualising Data</h1>\n",
    "   We will start by importing and visualising the two datasets used as examples in the lecture: students marks,  and Iris\n",
    "<ul>\n",
    "    <li>You should already have uploaded the data and figures from the lecture materials folder - if not, do so now.</li>\n",
    "    <li>Then run the 5 code cells below to load and display the two datasets</li>\n",
    "            </ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_scatterplot_matrix(X,y,featureNames,title=None):\n",
    "    f = X.shape[1]\n",
    "    if(len(y) != X.shape[0]):\n",
    "        print(\"Error,   the y array  must have the same length as there are rows in X\")\n",
    "        return\n",
    "    fig, ax = plt.subplots(f,f,figsize=(12,12))\n",
    "    plt.set_cmap('jet')\n",
    "    for feature1 in range(f):\n",
    "        ax[feature1,0].set_ylabel( featureNames[feature1])\n",
    "        ax[0,feature1].set_xlabel( featureNames[feature1])\n",
    "        ax[0,feature1].xaxis.set_label_position('top') \n",
    "        for feature2 in range(f):\n",
    "            xdata = X[:,feature1]\n",
    "            ydata = X[:,feature2]\n",
    "            ax[feature1, feature2].scatter(xdata,ydata,c=y)\n",
    "    if title != None:\n",
    "        fig.suptitle(title,fontsize=16,y=0.925)\n",
    "        \n",
    "        \n",
    "# simple function - currently only works for 2D data - but could easily be extended\n",
    "def PlotDecisionSurface(trainX,trainy,theClassifier,theTitle,featureNames,xvar=0,yvar=1,stepSize=2.0,minZero=False):\n",
    "    #create and prettify the plot\n",
    "    cmap=\"Set3\"\n",
    "    fig,ax= plt.subplots(figsize=(8, 8))\n",
    "    ax.set_title(theTitle)\n",
    "    ax.set_xlabel(featureNames[xvar])\n",
    "    ax.set_ylabel(featureNames[yvar])\n",
    "\n",
    "    #define a grid we use to plot the decision boundaries\n",
    "      #get max/min values for gri edges\n",
    "    columnMax,columnMin = np.max(trainX,axis=0), np.min(trainX,axis=0)\n",
    "    if(minZero==True):\n",
    "        x_min , y_min= 0,0\n",
    "    else:\n",
    "        x_min, y_min = columnMin[ xvar]*0.95, columnMin[yvar]*0.95\n",
    "    x_max, y_max = columnMax[xvar]*1.05, columnMax[yvar]*1.05 \n",
    "    #make the grid\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, stepSize),np.arange(y_min, y_max, stepSize))\n",
    "\n",
    "    #predict and plotfor evey point on the grid\n",
    "    Z = theClassifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z,cmap=cmap)\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(x=trainX[:,xvar ],y= trainX[:, yvar], c=trainy.astype(float), alpha=1.0, cmap=cmap, edgecolor=\"black\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Student marks dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grades= np.genfromtxt(\"../lectures/data/assessment-grades-2features.csv\", delimiter= ',',skip_header=1)\n",
    "\n",
    "featureNames=(\"exam\", \"CW_mean\")\n",
    "nStudents = grades.shape[0]\n",
    "\n",
    "outcomes= (\"Pass\",\"Resit Exam\", \"Resit Coursework\",\"Resit Both\")\n",
    "simpleoutcomes= (\"pass\",\"resit\")\n",
    "\n",
    "# make target labels\n",
    "result = np.empty(nStudents, dtype=np.int8)\n",
    "\n",
    "for row in range (nStudents):\n",
    "    exam = grades[row][0]\n",
    "    cw   = grades[row][1]\n",
    "    if (exam>=35 and cw>=35 and (exam +cw >=80) ):\n",
    "        result[row] = 0 # PASS \n",
    "\n",
    "    elif ( cw>=40 and exam < 40):\n",
    "        result[row] = 1 #resit just exam \n",
    "    elif ( cw<40 and exam>=40):\n",
    "        result[row]= 2 # resit just coursework\n",
    "    else:\n",
    "        result[row] = 3  # resit both\n",
    "        \n",
    "simpleResult = np.where(result<1,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easiest to split the data into 4/2 subgroups ot plot the outomes /simplified outcomes\n",
    "\n",
    "passStudents = np.empty((0,2))\n",
    "resitCWStudents = np.empty((0,2))\n",
    "resitExamStudents = np.empty((0,2))\n",
    "resitBothStudents = np.empty((0,2))\n",
    "\n",
    "for student in range (nStudents):\n",
    "    if (result[student]==0):\n",
    "        passStudents = np.vstack( (passStudents,grades[student]) )\n",
    "    elif (result[student]==1):\n",
    "        resitExamStudents = np.vstack( (resitExamStudents,grades[student]) )\n",
    "    elif (result[student]==2):\n",
    "        resitCWStudents = np.vstack( (resitCWStudents,grades[student]) )\n",
    "    else:\n",
    "        resitBothStudents = np.vstack( (resitBothStudents,grades[student]) )\n",
    "simpleResitStudents = np.vstack( (resitExamStudents,resitCWStudents,resitBothStudents))\n",
    "\n",
    "print(passStudents.shape)\n",
    "print(resitExamStudents.shape)\n",
    "print(resitCWStudents.shape)\n",
    "print(resitBothStudents.shape)\n",
    "print(simpleResitStudents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(14,5))\n",
    "plt.xlabel(\"Exam\")\n",
    "plt.ylabel(\"Coursework\")\n",
    "ax[0].set_title(\"Outcomes\")\n",
    "ax[1].set_title(\"Simplified Outcomes\")\n",
    "\n",
    "ax[0].scatter(passStudents[:,0],passStudents[:,1],label = \"Pass\" )\n",
    "ax[0].scatter(resitExamStudents[:,0],resitExamStudents[:,1],label = \"Resit Exam\" )\n",
    "ax[0].scatter(resitCWStudents[:,0],resitCWStudents[:,1],label = \"Resit CW\" )\n",
    "ax[0].scatter(resitBothStudents[:,0],resitBothStudents[:,1],label = \"Resit Both\" )\n",
    "ax[1].scatter(passStudents[:,0],passStudents[:,1],label = \"Resit\" )\n",
    "ax[1].scatter(simpleResitStudents[:,0],simpleResitStudents[:,1],label = \"Pass\" )\n",
    "\n",
    "ax[0].legend(loc='lower right')\n",
    "ax[1].legend(loc='lower right') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:  Iris flowers <img src=\"../lectures/figures/ML/Iris-image.png\" style=\"float:right\">\n",
    "- classic Machine Learning Data set\n",
    "- 4 measurements: sepal and petal width and length\n",
    "- 50 examples  from each 3 sub-species for iris flowers\n",
    "- three class problem:\n",
    " - so for some types of algorithm have to decide whether to make  \n",
    "   a 3-way classifier or nested 1-vs-rest classifers\n",
    "- most ML classifiers can get over 90%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "irisX,irisy = sklearn.datasets.load_iris(return_X_y=True)\n",
    "columnLabels= (\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\")\n",
    "title=\"Scatterplots of 2D slices through the 4D Iris data\"\n",
    "show_scatterplot_matrix(irisX,irisy,columnLabels,title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h1>Activity 2: Implementing K-Nearest Neighbours</h1>\n",
    "</div>\n",
    "            \n",
    "Basic process for predicting the label of a new point from the trainig set\n",
    "1. Measure distance to new poitn from every member of the trainig set\n",
    "2. Find the K Nearest Neighbours  \n",
    "   in other words, the K members of the trainig set with the smallest distances  (*calculated in step 1*)\n",
    "3. Count the labels that were provided for those K trainig items,  \n",
    "   and return themost common one as the predicted label.\n",
    "\n",
    "Below is a figure illustrating the start and first two steps of process.  \n",
    "It is followed by a code cell with a simple implementation of a class for 1-Nearest neighbours. \n",
    "\n",
    "Read through the code  to get a sense for how it implements the algorithm.  \n",
    "Your tutor will discuss it with you in the lab sessions.\n",
    "<img src=\"../lectures/figures/ML/kNN-steps.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for K = 1 \n",
    "\n",
    "class simple_1NN:\n",
    "\n",
    "    def __init__(self,verbose = True):\n",
    "        # this version only looks at the single nearest neighbour\n",
    "        self.K=1\n",
    "        self.verbose= verbose\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        # ask the data how big it is and store that info\n",
    "        self.numExemplars = X.shape[0]\n",
    "        self.numFeatures = X.shape[1]\n",
    "        # store a copy of the data (X) and the labels (y)\n",
    "        self.modelX = X\n",
    "        self.modelY = y\n",
    "        self.labelsPresent = np.unique(self.modelY) # list the unique values found in the labels provided\n",
    "        if (verbose):\n",
    "            print(\"There are {} training examples, each described by values for {} features\".format(self.numExemplars,self.numFeatures))\n",
    "            print(\"So self.modelX is a 2D array of shape {}\".format(self.modelX.shape))\n",
    "            print(\"self.modelY is a list with {} entries, each being one of these labels {}\".format(len(self.modelY), self.labelsPresent))\n",
    "        \n",
    "    def predict(self,newItems):\n",
    "        # read how many  newitems there are\n",
    "        numToPredict = newItems.shape[0]\n",
    "        # make an empty list to hold their predicted labels\n",
    "        predictions = []\n",
    "        \n",
    "        #loop through each new item each one\n",
    "        for item in range(numToPredict):\n",
    "            # predicting its label\n",
    "            thisOPrediction = self.PredictNewItem ( newItems[item])\n",
    "            # adding that predictin to our list\n",
    "            predictions = predictions.append(thePrediction)\n",
    "        return predictions\n",
    "    \n",
    "    def PredictNewItem(self,newItem):\n",
    "        \n",
    "        # Step 1: measure and store distance to each training item\n",
    "        distFromNewItem = np.zeros((self.numExemplars)) # array with one entry for each trainig set item, intialised to zero\n",
    "        for exemplar in range (self.numExemplars):\n",
    "            distFromNewItem[exemplar] = EuclideanDistance(newItem,  self.modelX[exemplar])\n",
    "  \n",
    "        # Step 2: find the one closest training example: This is K=1, \n",
    "        closest = 0\n",
    "        for trainingExample in range (0, self.numExemplars):\n",
    "            if  ( distFromNewItem[trainingExample] < distFromNewItem[closest] ):\n",
    "                closest=trainingExample\n",
    " \n",
    "        # step 3: count the votes - because this is for K=1 so we don't need to take a vote\n",
    "        labelOfClosest = self.modelY[closest]\n",
    "        return labelOfClosest\n",
    "    \n",
    "    def EuclideanDistance(a,b):\n",
    "        ## this numpy function calculates the euclidean distance\n",
    "        return np.linalg.norm(a-b)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-warning\" >\n",
    "<h2> Activity 2.1</h2>\n",
    "    Run the code provided for K=1 with the two datasets and make sure you understand the outputs and how they are produced\n",
    "<ul>\n",
    "    <li>For the marks dataset this creates a plot to show a decision surface</li>\n",
    "    <li>For the  iris data set this uses a confusion matrix <br> (google what a confusion matrix is if you're not sure)</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Marks dataset - illustrating a 2D Decision surface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# create and train the classifier\n",
    "myKNNmodel = simple_1NN()\n",
    "myKNNmodel.fit(grades,simpleResult) \n",
    "\n",
    "#visualise the decision surface\n",
    "PlotDecisionSurface(grades, simpleResult, myKNNmodel,\"1-NN simplified outcomes\", (\"exam\",\"cw\"),minZero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Iris dataset - illustrating a confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train/test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "irisX,irisy = load_iris(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(irisX, irisy, test_size=0.33,stratify=irisy)\n",
    "\n",
    "\n",
    "model = simple_1NN()\n",
    "model.fit(X_train,y_train)\n",
    "ypred = model.predict(X_test)\n",
    "confusionMatrix = np.zeros((3,3),int)\n",
    "for i in range(50):\n",
    "    actual = int(y_test[i])\n",
    "    predicted = int(ypred[i])\n",
    "    confusionMatrix[actual][predicted] += 1\n",
    "print(confusionMatrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" >\n",
    "<h2> Activity 2.3: Create your own implementation of K-Nearest Neighbours</h2>\n",
    "    Using the code above,  extend the predict method for the class simple_1NN  to use the votes from K>1 neighbours.\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li>Start by creating an empty class called Simple_KNN and copying in the pseudo-code as comments</li>\n",
    "    <li>Then copy the code from the simple_1NN class into the relevant places</li>\n",
    "    <li> You should only need to make minor changes to the __init__ method to set the value of K </li>\n",
    "    <li> in the predictNewItem() method you will need to change step 2  and step 3 </li>\n",
    "    <li> In the pseudocode you were suggested two possible ways of doing step 2. </li>\n",
    "    <li> If you choose to do step 2 the <emph>pythonic</emph> way, the next markdown cell discusses how to do this and the following code cell gives an example.  </li>\n",
    "    </ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Pseudocode for KNearest Neighbours\n",
    "**init()**  :  \n",
    "SPECIFY function to calculate distance metric d(i,j) for any two items *i* and *j*     \n",
    "  e.g. Euclidean (continuous variables) or Hamming (categorical)  \n",
    "SET value of K\n",
    "\n",
    "**fit(trainingData)** :  \n",
    "\n",
    "SET numExemplars = READ(number of rows in training data)  \n",
    "SET numFeatures = READ(number of columns in training data) \n",
    "\n",
    "*#Just store a local copy of the training data as two arrays:*   \n",
    "CREATE_AND_FILL(X_train of shape (numExemplars , numFeatures)).     \n",
    "CREATE_AND_FILL(y_train of shape( numExemplars))\n",
    "  \n",
    "**predict(newItems)** :  \n",
    "SET numToPredict = READ(number of rows in newItems)  \n",
    "SET predictions = CREATE_EMPTYARRAY( numToPredict)\n",
    " \n",
    "FOREACH item in (0...,numToPredict-1)    \n",
    "...SET predictions[item] = predictNewItem ( newItems[item]) \n",
    " \n",
    "RETURN predictions  \n",
    "\n",
    "\n",
    "**predictNewItem(newItem)**:\n",
    "\n",
    "*Step 1:   Make 1D array distances from newItem to each trainig set item*   \n",
    "FOREACH exemplar in (0,...,numExemplars -1  \n",
    "...SET distFromNewItem [exemplar] = d (newItem , X_train[exemplar] )   \n",
    "\n",
    "*Step 2: Get indexes of the k nearest neighbours for our new item*        \n",
    "SET closestK = GET_IDS_OF_K_SMALLEST(distFromNewItem)\n",
    " \n",
    "  \n",
    "*Step 3: Store majority vote in a  1D array y_pred with numToPredict entries*     \n",
    "SET labelcounts = CREATE(1D array with m zero values)  \n",
    "\n",
    "FOREACH  k in (0,...K-1)   \n",
    "... SET thisindex = closestK[newItem][k]  \n",
    "... SET thislabel = y_train[thisindex]  \n",
    "... INCREMENT labelcounts[thislabel]  \n",
    "\n",
    "SET thisPrediction = READ(index of labelcounts with highest value)    \n",
    "\n",
    "RETURN thisPrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Two possible Versions of GET_IDS_OF_K_SMALLEST for  Step 2 \n",
    "\n",
    "**V1: The \"pythonic\" way using sorting**  \n",
    "SET distFromNewItemWithIndex = CREATE(2D array with  2 columns, and numExemplars rows)  \n",
    "FOREACH exemplar in (0,...,numExemplars -1)    \n",
    "...SET distFromNewItemWithIndex[exemplar][0] = distFromNewItem[exemplar] *#column 0 holds distance*   \n",
    "...SET distFromNewItemWithIndex[exemplar][1] = exemplar          *#column 1 holds Id of the exemplar*  \n",
    "\n",
    "SET  sortedByDist = SORT (rows of distFromNewItemWithIndex by increasing distances (values in column 0) )  \n",
    "\n",
    "SET closestK = SELECT (first K elements from column 1 of  sortedByDist) # can do in one go using slicing\n",
    "RETURN closestK \n",
    "\n",
    "\n",
    "**V2: More explicit version using loops**\n",
    "SET closestK= EMPTYLIST\n",
    "\n",
    "FOR k in (0,...,K-1)  \n",
    "... SET thisClosest=0  \n",
    "... FOR exemplar in (1,...,K-1)  \n",
    "......IF ( distFromNewItem[exemplar] < distFromNewItem[thisClosest]  )  \n",
    "......... SET thisClosest = exemplar  \n",
    "... SET closestK[k] = thisClosest # store this id  \n",
    "... SET distFromNewItem[thisClosest] = BigNumber # so we don't pick it again in next loop\n",
    "\n",
    "RETURN closestK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your KNN class code here\n",
    "\n",
    "class simple_KNN:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints on how to do step 2 v1 - the 'pythonic' way**  \n",
    "\n",
    "\n",
    "This may be slightly more efficient **but is more suited to people confident in coding**\n",
    "\n",
    "If I have an array of myDist of (say, for simplicity) five values,  and I want to resort them by size, keeping track of the item ids  so I can find the K with the smallest values in the original array.\n",
    "\n",
    "What I need to do is:\n",
    "1. Create a new array with two columns - the distances in the first colum, and the Ids in ther second column\n",
    "2. Resort the rows of this array in ascending order of the first column. \n",
    "   i.e. the row with the smallest ditacnes moves ot the top etc.\n",
    "3. Then the second column now holds the index/ids sorted according to size of their contents\n",
    "\n",
    "<img src=\"howto-get-K-closest.png\" style=\"float:right\" width = 50%>\n",
    "The image shows the idea of how to do this.\n",
    "- it starts by grabbing a row from a 2D distacnes array,    \n",
    "  but the simple_1NN class shows how to calculate this row directly\n",
    "\n",
    "\n",
    "\n",
    "   I can do it using the code below, which:\n",
    "   - creates a 2d array **myUnsortedArray** holding each value and its index in the original array\n",
    "   - then makes a new array **mySortedArray**  by sorting according to the values in the first column \n",
    "   - then looks in the second column (which holds the indexes) for the first K rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cells just contains some hints about how to prodice a sorted array\n",
    "myDist = np.array([4,7,1,3,9])\n",
    "\n",
    "myUnsortedArray = np.empty((5,2))\n",
    "print(myUnsortedArray.shape)\n",
    "for row in range (5):\n",
    "    myUnsortedArray [row][0] = myDist[row]\n",
    "    myUnsortedArray [row][1] = row\n",
    "print('myUnsortedArray contents before sorting')\n",
    "print(myUnsortedArray)\n",
    "\n",
    "mySortedArray=myUnsortedArray[np.argsort(myUnsortedArray[:, 0])]\n",
    "\n",
    "print('mySortedArray contents after sorting')\n",
    "print(mySortedArray)\n",
    "\n",
    "print('Last week we learned about slicing- we will now use slicing to pull out all rows but just specific columns')\n",
    "print(' the values in array myDist in ascending order are: {}'.format(mySortedArray[:,0]))\n",
    "print(' the positions those values were in, again by ascending order of value are : {}'.format(mySortedArray[:,1]))\n",
    "print('Now remember you do not have to choose every row ... ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" >\n",
    "<h2> Activity 2.4: Test your implementation on the two example datasets</h2>\n",
    "Use the toolbar to copy and paste the two cells from activity 2.1 below here. <br>\n",
    "Then edit them so that they create and use objects of your new class, instead of the class simple_1NN\n",
    "\n",
    "Start with K=1 - this should produce the same results as you got in activity 2.1, then try with K = {3,5,7}\n",
    "<ul>\n",
    "    <li>Use the student marks for <b>qualititative</b> judgements : how does the decision surface change?</li>\n",
    "    <li>Use the Iris data set for <b>quantitative</b> judgements :  how does the confusion matrix change?</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" >\n",
    "<h1> Activity 3: Decision Trees</h1></div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\" >\n",
    "<h2> Activity 3.1: Run the next cell to remind yourself how the decision trree model is grown</h2>\n",
    "\n",
    "The next cell just illustrates how the tree induction process works for the student marks dataset.<br>\n",
    "It just calls the decision tree repeatedly for increasing depths.<br>\n",
    "For depth 0 I've just created a text box with the relevant stats in.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(18,8))\n",
    "fig.suptitle(\"Illustration of how Decision Trees select and insert nodes to increase data purity\")\n",
    "for depth in range (0,3):\n",
    "    if(depth==0):\n",
    "        ax[0].text(0.25, 0.6, \" gini=0.147\\n samples=150,\\n value=[138,12],\\n class=pass\",fontsize=14, \n",
    "        bbox={'facecolor': 'darkOrange', 'alpha': 0.5, 'pad': 10})\n",
    "        ax[0].axes.get_yaxis().set_visible(False)\n",
    "        ax[0].axes.get_xaxis().set_visible(False)\n",
    "        ax[0].set_frame_on(False)\n",
    "        ax[0].set_title(\"Depth 0\")\n",
    "    else:\n",
    "        model = DecisionTreeClassifier(random_state=1234, max_depth=depth,min_samples_split=2,min_samples_leaf=1)\n",
    "        model.fit(grades,simpleResult)\n",
    "        _ = tree.plot_tree(model, feature_names=(\"exam\",\"coursework\"), class_names= (\"pass\",\"resit\"),filled=True,ax=ax[depth])\n",
    "        ax[depth].set_title(\"Depth \"+str(depth))\n",
    "        \n",
    "fig.savefig(\"DecisionTreeExample-studentMarks.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" >\n",
    "<h2> Activity 3.2: exploring how to control tree-growth to prevent over-fitting</h2>\n",
    "The aim of this activity is for you to experiment with what happens when you change three parameters that affect how big and complex the tree is allowed to get.\n",
    "<ul>\n",
    "    <li> max_depth</li>\n",
    "    <li>min_samples_split, (default value is 2)</li>\n",
    "    <li>min_samples_leaf, (default value is 1)</li>\n",
    "    </ul>\n",
    "\n",
    "\n",
    "Experiment with the Iris data set below to see if you can work out what each of these parameters does, and how it affects the tree \n",
    "<ul>\n",
    "<li> Each time you run the  cell below, it will give you a different train-test split of the Iris data.<br>\n",
    "    Does this affect what tree you get? </li>\n",
    "    <li> Is there a combination of values that means you consistently get similar trees?</li>\n",
    "    <li>    What is a good way of judging 'similarity?</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load iris dataset and split into train:test\n",
    "iris = sklearn.datasets.load_iris()\n",
    "irisX = iris.data\n",
    "irisy = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(irisX, irisy, test_size=0.33,stratify=irisy)\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=1234, max_depth=None,min_samples_split=2,min_samples_leaf=1)\n",
    "model.fit(X_train,y_train)\n",
    "ypred = model.predict(X_test)\n",
    "confusionMatrix = np.zeros((3,3),int)\n",
    "for i in range(50):\n",
    "    actual = int(y_test[i])\n",
    "    predicted = int(ypred[i])\n",
    "    confusionMatrix[actual][predicted] += 1\n",
    "print(confusionMatrix)\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "_ = tree.plot_tree(model, \n",
    "                   feature_names=iris.feature_names,  \n",
    "                   class_names=iris.target_names,\n",
    "                   filled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" > <h1> Activity 4: (stretch)</h1></div>\n",
    "Using the code from last week,  apply a StandardScaler to the Iris data set and evaluate the effect this has on the accuracy.\n",
    "\n",
    "Because there is a random element in how  the data set is split into training / test split,  it is not valid just to split the data once then compare the results with / without scaling.\n",
    "\n",
    "Instead  you will need to do ten repeats  of:\n",
    "- Use the sklearn method to split the data into 66:34 train/test sets\n",
    "- Construct,  train, and test,  an instance of your kNN model on the unscaled data and store its accuracy \n",
    "- Create an instance of the standard scaler and then:\n",
    "  - call its fit() method to set its parameters from the training set.\n",
    "  - call its transform() method for both the traing and test sets\n",
    "  - Construct,  train, and test,  an instance of your kNN model on this scaled data and store its accuracy \n",
    "\n",
    "That should gives you ten pairs of values (one per repeat) for the scaled and raw data accuracy.  \n",
    "Use an online statistical tool (e.g. https://www.graphpad.com/quickcalcs/ttest1.cfm) that lets you copy your data in the perform a 'paired t-test\" to find out the probability that normalising the data improves prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"> Please save your work (click the save icon) then shutdown the notebook when you have finished with this tutorial (menu->file->close and shutdown notebook</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> Remember to download and save your work if you are not running this notebook locally.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
