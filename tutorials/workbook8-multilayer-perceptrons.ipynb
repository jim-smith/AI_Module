{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aims of this tutorial\n",
    "The aim of this tutorial is to illustrate how Perceptrons can be combined into Neural Networks to solve problems that are not linearly separable, such as XOR.  \n",
    "We will look at the key differences between the two algorithms and also consider how network architecture and training parameters affects the outcome.\n",
    "\n",
    "## Learning Objectives:\n",
    "1. Understand the key differences between the Neural Network and Perceptron algorithms:\n",
    "- Non-linear activation functions.\n",
    "- Using Backpropagation to update (learn) the weights.\n",
    "- configuring MLP with more than one output node when there are more than two different output labels (multi-class learning)\n",
    "2. Understand how different nodes learn different aspects of the problem.\n",
    "\n",
    "3. Consider the need for different network architectures and learning parameters for different problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "<img src=\"ANN-2-Node.png\" style=\"float:right\">\n",
    "\n",
    "As we have seen, Perceptrons are only capable of solving linearly separable problems.   \n",
    "To overcome this limitation, we can connect Perceptrons together into a network.  \n",
    "Each one becomes a Node in the network, and they are connected together into Layers. \n",
    "\n",
    "In standard Artificial Neural Network (ANN) architecture there is one input, one output and one or more hidden layers.  \n",
    "- Though the term *input layer* is a bit misleading, it doesn't actually do any computation, it is just the inputs to the network.\n",
    "- So, outputs of hidden layers become the inputs to subsequent hidden layers, or the final output layer. \n",
    "- Hidden nodes tend to learn different aspects of the problem space, building more complex decision boundaries and are therefore able to solve more complex problems.\n",
    "\n",
    "Note: \n",
    "- The number of nodes in the input layer must equal the number of inputs/features in the data. \n",
    "- One output node can discriminate between two classes (classification problems),  \n",
    "  or predict a value for one continuous variable (regression problems).  \n",
    "  If your data  has more than two classes (or variables to predict),  \n",
    "  the number of output nodes must equal the number of classes/regression variables. \n",
    "- The number of hidden layers and nodes in the layers is arbitrary, and selecting this architecture is part of building an ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Training Algorithm  \n",
    "Similar to Perceptrons, ANN are trained in two 'phases'. \n",
    "- The forward pass, where data is input into the network to produce an output. \n",
    "- The backward pass, where the error in output is used to update the weights using Backpropagation and Gradient Descent.\n",
    "  - note that to calculate what the sum of  inputs was going *in* to a node we apply the *sigmoid derivative* to the signal coming *out* of that node \n",
    "\n",
    "<img src=\"ann-pseudocode.png\" style=\"float:center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Solving XOR\n",
    "As an introduction to the ANN algorithm, and to give you an intuition for how different nodes and layers in the network learn different aspects of the problem space, we are going to look at how a small network can solve the XOR problem.\n",
    "\n",
    "Running the code will train an ANN to solve the XOR problem and produces a visualisation to show how different nodes have learned different aspects of the problem to create a more complex decision boundary (in this case different logical functions.\n",
    "\n",
    "- You do not need to understand *how* the graphs/visualisations are produced.\n",
    "\n",
    "- You should try and understand *what* the graphs/visualisations output means.\n",
    "\n",
    "### Activity 1: Train MLP with one hidden layer and see (through experimentation) how many nodes are needed to reliably solve x-or\n",
    "- Run the next two cells below once to import the libraries and define the function that plots the decision surface.\n",
    "- if the first cell reports an error trying to import VisualiseNN, make sure you have downloaded the file VisualiseNN.py and it is in the same directory as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics for manipulating and outputting arrays etc\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import random\n",
    "%matplotlib inline\n",
    "\n",
    "## MLP specific stuff\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import VisualiseNN as VisNN\n",
    "\n",
    "\n",
    "# useful sklearn functions for preprocessing data and sahowing results\n",
    "from  sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "#the iris data\n",
    "from sklearn.datasets import load_iris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDecisionSurface(model,X,y):\n",
    "    min1, max1 = X[:, 0].min() - 1, X[:, 0].max() + 1 #1st feature\n",
    "    min2, max2 = X[:, 1].min() - 1, X[:, 1].max() + 1 #2nd feature\n",
    "    x1_scale = np.arange(min1, max1, 0.1)\n",
    "    x2_scale = np.arange(min2, max2, 0.1)\n",
    "    x_grid, y_grid = np.meshgrid(x1_scale, x2_scale)\n",
    "    # flatten each grid to a vector\n",
    "    x_g, y_g = x_grid.flatten(), y_grid.flatten()\n",
    "    x_g, y_g = x_g.reshape((len(x_g), 1)), y_g.reshape((len(y_g), 1))\n",
    "    # stack to produce hi-res grid in form like dataset\n",
    "    grid = np.hstack((x_g, y_g))\n",
    "\n",
    "    # make predictions for the grid\n",
    "    y_pred_2 = model.predict(grid)\n",
    "    \n",
    "    #predict the probability\n",
    "    p_pred = model.predict_proba(grid)\n",
    "    # keep just the probabilities for class 0\n",
    "    p_pred = p_pred[:, 0]\n",
    "    # reshaping the results\n",
    "    p_pred.shape\n",
    "    pp_grid = p_pred.reshape(x_grid.shape)\n",
    "\n",
    "    # plot the grid of x, y and z values as a surface\n",
    "    levels=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "    surface = plt.contourf(x_grid, y_grid, pp_grid, levels,cmap='Pastel1')\n",
    "    plt.colorbar(surface)\n",
    "    # create scatter plot for samples from each class\n",
    "    for class_value in range(2):\n",
    "        # get row indexes for samples with this class\n",
    "        row_ix = np.where(y == class_value)\n",
    "        # create scatter of these samples\n",
    "        plt.scatter(X[row_ix, 0], X[row_ix, 1], cmap='Pastel1')\n",
    "    # show the plot\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.1 Investigating repeatability\n",
    "Now run the cell below - it will try and learn the XOR problem and show you a plot of how the error rate changes over *time* measured in epochs.  \n",
    "- Nne epoch means that all the training data is shown to the system once and the weights are updated\n",
    "- We know. that *in theory* it should be able to learn XOR with 2 hidden nodes - **but is there a difference between theory and what happens in practice?**\n",
    "- Each time you run the cell it starts the whole process from new, so the error curve will be different and you might get different final accuracy scores.\n",
    "- As there are only four cases, we do not have any test data for this problem - we are just looking at how reliably different sized networks can learn a simple problem\n",
    "\n",
    "You should:\n",
    "1. Run the cell 10 times with 2 nodes in the hidden layer ( the parameter in the MLP constructor set to *hidden_layer_sizes=(2,)*. and note how many times it ended up with no errors (training set accuracy = 100%).  \n",
    "**remember that jupyter will move on to the next cell , so you need to select  the cell to re-run it**\n",
    "\n",
    "2. Now repeat, changing the constructor to change the  with the size of the hidden  layer to 4,6,8,10 nodes - and again note how many times out of 10 it  successfully learned the problem.\n",
    "3. Edit the second cell below, changing the values in xor_success to record your actual results and run the cell to produce a **sensitivity analysis** - a plot showing how much youyr results depend on a network parameter- thre number of hidden nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the four input cases form our training data\n",
    "train_X = np.array( [[0,0],[0,1],[1,0],[1,1]])\n",
    "# and her eare the labels our network should learn for the XOR problem\n",
    "xor_y = np.array([0,1,1,0])\n",
    "\n",
    "train_y= xor_y\n",
    "\n",
    "# one hidden layer with one hidden layer of 2 neurons with logistic (sigmoid) activation and Stochastic Gradient Descent (backprop)\n",
    "\n",
    "xorMLP =  MLPClassifier(hidden_layer_sizes=(5,), max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=0, \n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "\n",
    "xorMLP.fit(train_X, train_y)\n",
    "    \n",
    "lossplot=plt.plot(xorMLP.loss_curve_) \n",
    "training_accuracy = 100* xorMLP.score(train_X, train_y)\n",
    "print(\"Training set accuracy: \" + str(training_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the  array xor_success to  repalce the 'dummy' values(1,6,4,8,10) with youirt results, i.e.  the number of times your algorithm reached 100% accuracy on the training set\n",
    "\n",
    "num_hidden_nodes = [2,4,6,8,10]\n",
    "xor_success = [1,6,4,8,10]\n",
    "plt.plot(num_hidden_nodes, xor_success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.2: Visualising what the network is doing\n",
    "After a successful run of the cell above (i.e. one ending with training set accuracy 100%) run the cell below.\n",
    "- The top plot shows the output of the final node for different inputs.  \n",
    "  In this case we only have the four inputs marked by circles\n",
    "- The bottom plot showes a visualisation of the network structure and weights \n",
    "  - blue ones are *negative*, so will be suppressing the output of the cell they lead to if there is a signal down that connection\n",
    "  - red ones are *positive - so will be stimulating the node they lead to if there is a signal present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theMLP=xorMLP # change this line to reuse the code below for a different problem\n",
    "num_output_nodes = 1 # and this one for multi-class problems\n",
    "\n",
    "plotDecisionSurface(theMLP,train_X,train_y)\n",
    "\n",
    "\n",
    "#network_structure = np.hstack(([train_X.shape[1]], np.asarray(myMLP.hidden_layer_sizes), [train_y.shape[0]]))\n",
    "network_structure = np.hstack((2, np.asarray(theMLP.hidden_layer_sizes), 1))\n",
    "# Draw the Neural Network with weights\n",
    "network=VisNN.DrawNN(network_structure, theMLP.coefs_)\n",
    "network.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Using MLP for multiclass problems:  iris data\n",
    "<img src=\"cascading.png\" style=\"float:right\">\n",
    "\n",
    "So far we have used multilayer perceptrons for learning binary (two-class) problems.  \n",
    "Last week you should have discussed how you could solve a multi-class problem,  \n",
    "by 'cascading' binary classifiers. \n",
    "This is shown in the image for a three class problem.  \n",
    "Here the diamonds represent classifiers, each doing a \"this class or not\" decision.\n",
    "\n",
    "\n",
    "In this part we will introduce a different idea, which is to use a  parallel classifier using softmax and one-hot encoding.\n",
    "\n",
    "Not only is this simpler to manage, it  has the benefit that the classifiers can all share the feature creation done in previous layers\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "\n",
    "irisX,irisy = load_iris(return_X_y = True)\n",
    "feature_names = ['sepal width','sepal_length','petal_width','petal_length']\n",
    "irisLabels = np.array(('setosa','versicolor','virginica'))\n",
    "# show what the labels look like\n",
    "print(irisy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming our label data to a format for training a MLP with three output nodes\n",
    "As you can see when you run the cell above, the labels is a 1-D array with labels of 0, 1, or 2.  \n",
    "This is fine for models like nearest neighbours, rule sets or decision trees.\n",
    "However, (crudely speaking) the output from a neuron tends to be *off* (0) or *on*(1).  \n",
    "So if we want our network to make a choice of three predictions, then we need a node for each class.\n",
    "\n",
    "So there are two changes we make:\n",
    "1. We configure the network to have three output nodes  and use 'softmax' ('winner-takes-all') activation.  \n",
    "    i.e. Each node outputs a value, and we take as our final output the class whose node has the highest output signal\n",
    "2. We convert our labels tell the network what *each of the nodes* should ideally output for each training example.  \n",
    "   In other words, if the label is 0 the then output should be [0,0,1],  \n",
    "   if the label is 1 it should be [0,1,0], and if it is 2 the output shoulfd be [1,0,0].\n",
    "\n",
    "Sklearn comes with a module sklearn.preprocessing.onehotencoder() to do this,   \n",
    "but the cell below does it explicitly to illustrate what is going on. \n",
    "\n",
    "I've made it generic so that you can easily reuse it for different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numcases = len(irisy)\n",
    "print('there are ' +str(numcases) +' training examples')\n",
    "thelabels = np.unique(irisy)\n",
    "numlabels = len(thelabels)\n",
    "print( 'there are ' + str(numlabels) + ' labels: ' + str(thelabels))\n",
    "\n",
    "# make a 2d array with numcases rows. and numlabels columns\n",
    "irisy_onehot = np.zeros((numcases,numlabels))\n",
    "\n",
    "\n",
    "# Now loop through the rows of the new array setting the appropriate column value to 1\n",
    "for row in range(numcases):\n",
    "    label = irisy[row]\n",
    "    irisy_onehot[row][label]= 1\n",
    "\n",
    "print('This is what  rows 45-55 look like')\n",
    "print(irisy_onehot[44:55,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting our data into a training and a test set\n",
    "As you can see from the output of the cells above, the iris data has groups all the classes i.e. rows 0-49 are 'iris-setosa', 50-99 are 'iris versicolor'. and rows 100-149 are 'iris-virginica'.\n",
    "\n",
    "So if we want to train our network  and then estimate how well it will do on new data, we need to split this into a training and test set.  \n",
    "Again, we could do this manually:\n",
    "- first shuffling the rows so that we got a mixture of classes, \n",
    "- then taking the first part of the data for training and the second for testing.\n",
    "\n",
    "If the data are not so well organised, or the numbers of examples of different classes are not roughly equal, then that code gets trickier.  \n",
    "So the cell below shows how to do this using a method from sklearn.   \n",
    "The parameters are, in order:\n",
    "- the feature values (irisx)\n",
    "- the onehot-encoded set of labels (irisy_onehot)\n",
    "- what proportion of our data we holdback from training, so we can use it for test. We'll use 1/3rd ( test_size=0.33)\n",
    "- the array holding the labels that we want to be evenl;y represented in both our training and test sets. (stratify=irisy_onehot)\n",
    "\n",
    "This function returns the four different arrays - train and test, x and y.  \n",
    "Noe that this function also works if your data is not one-hot encoded - it figures that out for itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris_train_X, iris_test_X, iris_train_y, iris_test_y = train_test_split(irisX,irisy_onehot, test_size=0.33, stratify=irisy_onehot )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.1 Training a MLP to learn the iris classification problem\n",
    "1. Start by using the  settings for the MLPClassifier that we had before and just change the size of the hidden layer to five or ten\n",
    "- you will probably see that the training stops making improvements before the problem has been fully learned.\n",
    "- this is an example of the backpropagation getting 'stuck' in a **local optimum** (we'll talk about these more next week)\n",
    "- it happens becuase the basid 'stochastic gradient descent' algorithm *'sgd'* is a local search method with only crude methods for getting out of 'traps' \n",
    "- try changing the solver to 'adam' and see if this gives better performance\n",
    "\n",
    "**Remember** to run a few times with each setting - this is a randomised algorithm and the random set of initial weights makes a huge difference.  \n",
    "\n",
    "**Question**: what do you understand by *better*\n",
    "\n",
    "2. Now try adding a second hidden layer - for example by changing that parameter in the constructor to *hidden_layer_sizes=(3,3)*.  \n",
    "- Experiment to see if it is better to have one hidden layer of 10 nodes or 2 layers of 5 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an MLP object-  you will want to change the number of hidden nodes\n",
    "irisMLP =  MLPClassifier(hidden_layer_sizes=(5,), max_iter=1000, alpha=1e-4,\n",
    "                    solver='adam', verbose=0, \n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "irisMLP.fit(iris_train_X, iris_train_y)\n",
    "print('number of output nodes = ' +str(irisMLP.n_outputs_))\n",
    "    \n",
    "lossplot=plt.plot(irisMLP.loss_curve_)    \n",
    "\n",
    "# report how well it does on the training set\n",
    "training_accuracy = 100* irisMLP.score(iris_train_X, iris_train_y)\n",
    "print(\"Training set accuracy: \" + str(training_accuracy) + \"%\")\n",
    "\n",
    "\n",
    "# now how good is our network at predicting data it has never seen before\n",
    "test_accuracy = 100* irisMLP.score(iris_test_X, iris_test_y)\n",
    "print(\"Estimated (Test set) accuracy: \" + str(test_accuracy) + \"%\")\n",
    "\n",
    "# this bit of code prints a simple confusion matrix showing how the predicted labels correspond to the 'real' ones\n",
    "predictions=irisMLP.predict(iris_test_X)\n",
    "confusion = np.zeros((3,3))\n",
    "for row in range (predictions.shape[0]):\n",
    "    actual = np.argmax(iris_test_y[row])\n",
    "    predicted = np.argmax(predictions[row])\n",
    "    confusion [actual] [predicted] += 1\n",
    "\n",
    "print( '\\nPredicted->   Setosa  Versicolor  Virginica')\n",
    "print( 'Actual \\|/ ')\n",
    "for i in range(3):\n",
    "    print( '{:<10}       {:2.0f}       {:2.0f}       {:2.0f}'.format(irisLabels[i], confusion[i][0], confusion[i][1],confusion[i][2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.2 Discussion\n",
    "Try to come up with answers to these questions. (these are the sorts of things you might be asked in an exam)\n",
    "\n",
    "1. Why is the test accuracy sometimes much lower than the training accuracy?\n",
    "\n",
    "2. Why is it sometimes less reliable train a network with multiple hidden layers when learning the iris data?  \n",
    "Hint: how many connections are you trying to learn?  how much data have you got?\n",
    "\n",
    "### Activity2.3 (stretch): Does it help if you normalise the data like we did in week 5?\n",
    "In Activity 2.3 of the unsupervised learning tutorial (workbook5) we used a Minmax scaler so that each feature was transformed to the range (0,1).  \n",
    "Reusing snippets of code from that workbook,  try adding a few lines to the cell at the start of this section (Part 2), so that scaling gets applied to irisX before you make the call to train_test_split().\n",
    "- Does this improve learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Learning to recognise hand-written digits:  MNIST\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.1: Loading and visualising the data\n",
    "The next cell  downloads and savs the data locally.\n",
    "- if I can sort out shared storage on ther Jupyterhub server, I will create an option to use that to save time\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# example code to run on the server where i will put a version of the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example code from https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html#sphx-glr-auto-examples-neural-networks-plot-mnist-filters-py\n",
    "\n",
    "# the data to download is about 33Mb \n",
    "# so I've put this code in its own cell so you can just do it once.\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True,cache=True,data_home=\"data\")\n",
    "\n",
    "X = X / 255.\n",
    "\n",
    "# rescale the data, use the traditional train/test split\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "print('data loaded and saved locally')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell  shows us some example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display ten random images from each class\n",
    "print('The test data has {} images, each described as a {} features (pixel values)'.format(X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for label in range(10):\n",
    "    imagesForLabel= np.empty((0,784))\n",
    "    \n",
    "    for possible in range (200):\n",
    "        if (int(y_test[possible])==int(label)):\n",
    "            imagesForLabel = np.vstack((imagesForLabel, X_test[possible]))\n",
    "    for col in range(5):\n",
    "        exampleplot = plt.subplot(10, 5, (label*5 +col+1) )\n",
    "        exampleplot.imshow(imagesForLabel[col].reshape(28, 28), \n",
    "                   cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.2 : Visualising what features the hidden layers learn to respond to \n",
    "We will now configure a multilayer perceptron  and training it with all 60,000 images from the standard MNIST training set.\n",
    "\n",
    "The idea for you to learn here, is that each hidden node is effectively acting as a feature detector: \n",
    " - Consider just one hidden layer node: \n",
    "   - and a simple pattern where the weights from pixels in the top left and bottom right quadrant are all +1, \n",
    "   - and the weights from pixels in the top-right and bottom-left quadrants are all -1.\n",
    "\n",
    "- Now consider an input image that has some constant value for every pixel (feature) - i.e. is all the same colour. \n",
    "  - when these inputs to the node  are multiplied by their weights and summed, they will cancel each other.\n",
    "  - so the sum will be zero and the output will be sigmoid(0) = 0.5.\n",
    "\n",
    "- Next consider an the image  of a simple 'chequer' pattern with  white (255) in the top-left and bottom-right quadrants,  \n",
    "  and black (0)  in the other two.\n",
    "  - In this case  the pattern of  pixel intensities (features) in the image  maches match the pattern in the weights.\n",
    "   - So then the weighted sum will be at its maximum, and the node will output +1.\n",
    "\n",
    "So we can consider each hidden node as a 'feature detector' that responds to how well the input image matches a particular pattern.\n",
    "\n",
    "The next set of cells:\n",
    "- Set up and train the network with 16 nodes (so we cna visualsie it). \n",
    "- Then output the pattern  weights from each of the nodes as an image.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> In year 2, the Machine Learning module will explain how this concept of feature detectors has been extended  in Deep Convolutional Networks. <br>\n",
    "In these features (called 'filters') can be a smaller size than the image and a process of Convolution (rather than straighforward multiplying) lets them detect small local features anywhere in the image.<br>  Convolutional Neural Networks have completely revolutionised the field of image processing and AI for visual tasks.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up and train network\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(16), max_iter=25, alpha=1e-4,\n",
    "                    solver='sgd', verbose=1, random_state=10,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "# this example won't converge because of CI's time constraints, so we catch the\n",
    "# warning and are ignore it here\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n",
    "                            module=\"sklearn\")\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the weights from the input nodes to the first hidden layer\n",
    "coef = mlp.coefs_.copy()[0].T\n",
    "\n",
    "# scale the weights so they all lie in the same range for di\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "scale = np.abs(coef).max()\n",
    "for i in range(16):\n",
    "    l1_plot = plt.subplot(4, 4, i + 1)\n",
    "    l1_plot.imshow(coef[i].reshape(28, 28), \n",
    "                   cmap=plt.cm.RdBu)#, vmin=-scale, vmax=scale)\n",
    "    l1_plot.set_xticks(())\n",
    "    l1_plot.set_yticks(())\n",
    "    #l1_plot.set_xlabel('Hidden Node %i' % i)\n",
    "title= 'Learned weights from pixels to each hidden node. have been trained to respond to ...\\n'\n",
    "title = title + 'Blue indicates negative weights: signals from these pixels suppress the node.\\n'\n",
    "title=title+ 'Red indicates positive weights; pixels from these pixels stimulate the  hidden node.'\n",
    "\n",
    "_=plt.suptitle(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.3: Discussion\n",
    "Iris is a simple problems with only 4 features and three classes.\n",
    "MNIST is a much more complicated problem with 784 features and ten classes - some of which (e.g. 4s and sevens) can be drawn in completely different ways.\n",
    "\n",
    "So how come the accuracy is roughly the same on thes two problems?\n",
    "\n",
    "Can you predict wehat the effect on training and test accuracy might be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.4: examining the effect of having less data\n",
    "- run the cell below  times, and make sure you can explain the pattenrs of changing training and test scor youy see.\n",
    "\n",
    "### (Stretch) Activity 3.5\n",
    "- run the cell n times, saving the training and test accuracy from each runc\n",
    "- capture the data and display it as two different lines in the same plot, with error bars for each.\n",
    "  HINT: google is good to find code snippets to make plots with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trSetSize in (100,600,1000,6000,10000,50000):\n",
    "    split= trSetSize/60000\n",
    "    _,X_train_small,_,y_train_small = train_test_split(X_train,y_train, test_size=split,stratify=y_train)\n",
    "    smallMnistMLP = MLPClassifier(hidden_layer_sizes=(16), max_iter=25, alpha=1e-4,\n",
    "                    solver='sgd', verbose=0, random_state=10,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "#put a loop of n runs here\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n",
    "                            module=\"sklearn\")\n",
    "        smallMnistMLP.fit(X_train_small, y_train_small)\n",
    "    print('With a training set of {} examples'.format(trSetSize))\n",
    "    print(\"    Training set score: %f\" % smallMnistMLP.score(X_train_small, y_train_small))\n",
    "    print(\"    Test set score: %f\" % smallMnistMLP.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"> Please save your work (click the save icon) then shutdown the notebook when you have finished with this tutorial (menu->file->close and shutdown notebook</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> Remember to download and save your work if you are not running this notebook locally.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
