{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aims of this tutorial\n",
    "The aim of this tutorial is to illustrate how Perceptrons can be combined into Neural Networks to solve problems that are not linearly separable, such as XOR.  \n",
    "We will look at the key differences between the two algorithms and also consider how network architecture and training parameters affects the outcome.\n",
    "\n",
    "## Learning Objectives:\n",
    "1. Understand the key differences between the Neural Network and Perceptron algorithms:\n",
    "- Non-linear activation functions.\n",
    "- Using Backpropagation to update (learn) the weights.\n",
    "- configuring MLP with more than one output node when there are more than two different output labels (multi-class learning)\n",
    "2. Understand how different nodes learn different aspects of the problem.\n",
    "\n",
    "3. Consider the need for different network architectures and learning parameters for different problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "<img src=\"ANN-2-Node.png\" style=\"float:right\">\n",
    "\n",
    "As we have seen, Perceptrons are only capable of solving linearly separable problems.   \n",
    "To overcome this limitation, we can connect Perceptrons together into a network.  \n",
    "Each one becomes a Node in the network, and they are connected together into Layers. \n",
    "\n",
    "In standard Artificial Neural Network (ANN) architecture there is one input, one output and one or more hidden layers.  \n",
    "- Though input layer is a bit misleading, it doesn't actually do any computation, it is just the inputs to the network.\n",
    "- So, outputs of hidden layers become the inputs to subsequent hidden layers, or the final output layer. \n",
    "- Hidden nodes tend to learn different aspects of the problem space, building more complex decision boundaries and are therefore able to solve more complex problems.\n",
    "\n",
    "Note: \n",
    "- The number of nodes in the input layer must equal the number of inputs/features in the data. \n",
    "- The number of output nodes must equal the number of labels/classes in the data. \n",
    "- The number of hidden layers and nodes in the layers is arbitrary, and selecting this architecture is part of building an ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Training Algorithm  \n",
    "Similar to Perceptrons, ANN are trained in two 'phases'. \n",
    "- The forward pass, where data is input into the network to produce an output. \n",
    "- The backward pass, where the error in output is used to update the weights using Backpropagation and Gradient Descent.\n",
    "  - note that to calculate what the sum of  inputs was going *in* to a node we apply the *sigmoid derivative* to the signal coming *out* of that node \n",
    "\n",
    "<img src=\"ann-pseudocode.png\" style=\"float:center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Solving XOR\n",
    "As an introduction to the ANN algorithm, and to give you an intuition for how different nodes and layers in the network learn different aspects of the problem space, we are going to look at how a small network can solve the XOR problem.\n",
    "\n",
    "Running the code will train an ANN to solve the XOR problem and produces a visualisation to show how different nodes have learned different aspects of the problem to create a more complex decision boundary (in this case different logical functions.\n",
    "\n",
    "You do not need to understand how the graphs/visualisations are produced.\n",
    "\n",
    "You should try and understand what the graphs/visualisations output means.\n",
    "\n",
    "### Activity 1: Train MLP with one hidden layer and see (through experimentation) how many nodes are needed to reliably solve x-or\n",
    "- Run the next two cells below once to import the libraries and define the finction that pltos the decision surface\n",
    "- if the first cell reports an error trying to import VisualiseNN, make sure you have downloaded the file VisualiseNN.py and it is in the same directory as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics for manipulating and outputting arrays etc\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import random\n",
    "%matplotlib inline\n",
    "\n",
    "## MLP specific stuff\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import VisualiseNN as VisNN\n",
    "\n",
    "\n",
    "# useful sklearn functions for preprocessing data and sahowing results\n",
    "from  sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "#the iris data\n",
    "from sklearn.datasets import load_iris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDecisionSurface(model,X,y):\n",
    "    min1, max1 = X[:, 0].min() - 1, X[:, 0].max() + 1 #1st feature\n",
    "    min2, max2 = X[:, 1].min() - 1, X[:, 1].max() + 1 #2nd feature\n",
    "    x1_scale = np.arange(min1, max1, 0.1)\n",
    "    x2_scale = np.arange(min2, max2, 0.1)\n",
    "    x_grid, y_grid = np.meshgrid(x1_scale, x2_scale)\n",
    "    # flatten each grid to a vector\n",
    "    x_g, y_g = x_grid.flatten(), y_grid.flatten()\n",
    "    x_g, y_g = x_g.reshape((len(x_g), 1)), y_g.reshape((len(y_g), 1))\n",
    "    # stack to produce hi-res grid in form like dataset\n",
    "    grid = np.hstack((x_g, y_g))\n",
    "\n",
    "    # make predictions for the grid\n",
    "    y_pred_2 = model.predict(grid)\n",
    "    \n",
    "    #predict the probability\n",
    "    p_pred = model.predict_proba(grid)\n",
    "    # keep just the probabilities for class 0\n",
    "    p_pred = p_pred[:, 0]\n",
    "    # reshaping the results\n",
    "    p_pred.shape\n",
    "    pp_grid = p_pred.reshape(x_grid.shape)\n",
    "\n",
    "    # plot the grid of x, y and z values as a surface\n",
    "    levels=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "    surface = plt.contourf(x_grid, y_grid, pp_grid, levels,cmap='Pastel1')\n",
    "    plt.colorbar(surface)\n",
    "    # create scatter plot for samples from each class\n",
    "    for class_value in range(2):\n",
    "        # get row indexes for samples with this class\n",
    "        row_ix = np.where(y == class_value)\n",
    "        # create scatter of these samples\n",
    "        plt.scatter(X[row_ix, 0], X[row_ix, 1], cmap='Pastel1')\n",
    "    # show the plot\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.1 Investigating repeatability\n",
    "Now run the cell below - it will try and learn the XOR problem and show you a plot of how the error rate changes over *time* measured in epochs.  \n",
    "- one epoch means that all the training data is shown to the system once and the weights are updated\n",
    "- we know. that *in theory* it should be able to learn XOR with 2 hidden nodes - **but is there a difference between theory and what happens in practice?**\n",
    "- Each time you run the cell it starts the whole process from new, so the error curve will be different and you might get different final accuracy scores.\n",
    "- as there are only four cases, we do not have any test data for this problem - we are just looking at how reliably different sized networks can learn a simple problem\n",
    "\n",
    "You should:\n",
    "1. Run the cell 10 times with 2 nodes in the hidden layer ( the parameter in the MLP constructor set to *hidden_layer_sizes=(2,)*. and note how many times it ended up with no errors (training set accuracy = 100%).  \n",
    "**remember to click on the cell the press run or shifdt-return to run it**\n",
    "\n",
    "2. Now repeat, changing the constructor to change the  with the size of the hidden  layer to 4,6,8,10 nodes - and again note how many times out of 10 it  successfully learned the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the four input cases form our training data\n",
    "train_X = np.array( [[0,0],[0,1],[1,0],[1,1]])\n",
    "# and her eare the labels our network should learn for the XOR problem\n",
    "xor_y = np.array([0,1,1,0])\n",
    "\n",
    "train_y= xor_y\n",
    "\n",
    "# one hidden layer with one hidden layer of 2 neurons with logistic (sigmoid) activation and Stochastic Gradient Descent (backprop)\n",
    "\n",
    "xorMLP =  MLPClassifier(hidden_layer_sizes=(5,), max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=0, \n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "\n",
    "xorMLP.fit(train_X, train_y)\n",
    "    \n",
    "lossplot=plt.plot(xorMLP.loss_curve_) \n",
    "training_accuracy = 100* xorMLP.score(train_X, train_y)\n",
    "print(\"Training set accuracy: \" + str(training_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.2: Visualising what the network is doing\n",
    "After a successful run ofthe cell above (i.e. one ending with training set accuracy 100%) run the cell below.\n",
    "- The top plot shows the output of the final node for different inputs.  \n",
    "  In this case we only have the four inputs marked by circles\n",
    "- The bottom plot showes a visualsiation of the betwork structure and weights \n",
    "  - blue ones are *negative*, so will be suppressing the output of the cell they lead to if there is a signal down that connection\n",
    "  - red ones are *positive - so will betrying to turn on ther node they lead to if there is a signal present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theMLP=xorMLP # change this line to reuse the code below for a different problem\n",
    "num_output_nodes = 1 # and this one for multi-class problems\n",
    "\n",
    "plotDecisionSurface(theMLP,train_X,train_y)\n",
    "\n",
    "\n",
    "#network_structure = np.hstack(([train_X.shape[1]], np.asarray(myMLP.hidden_layer_sizes), [train_y.shape[0]]))\n",
    "network_structure = np.hstack((2, np.asarray(theMLP.hidden_layer_sizes), 1))\n",
    "# Draw the Neural Network with weights\n",
    "network=VisNN.DrawNN(network_structure, theMLP.coefs_)\n",
    "network.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Using MLP for multiclass problems:  iris data\n",
    "\n",
    "- introduce idea of parallel classifier using softmax and one-hot encoding\n",
    "  - benefit that the classifiers can all share the feature creation done in prrevious layers\n",
    "- same visualisations as first case \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "\n",
    "irisX,irisy = load_iris(return_X_y = True)\n",
    "feature_names = ['sepal width','sepal_length','petal_width','petal_length']\n",
    "irisLabels = ['iris-setosa','iris-versicolor','iris-virginica']\n",
    "# show what the labels look like\n",
    "print(irisy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming our label data to a format for training a MLP with three output nodes\n",
    "As you can see when you run the cell above, the labels is a 1-D array with labels of 0, 1, or 2.  \n",
    "However, if we want our network to make a choice of three predictions, then we need a node for each class.\n",
    "\n",
    "So there are two changes we make:\n",
    "1. We tell ther network to have three output nodes  sandf use 'softmax' activiation.  \n",
    "    i.e. Each node outputs a value, and we take asour final output the class whose node has ther highest output signal\n",
    "2. We convert our labels tell the network what *each of the nodes* should ideally output for each training example.  \n",
    "   In other words, if the label is 0 the then output should be [0,0,1], if the label is 1 it should be [0,1,0], and if it is 2 the output shoulfd be [1,0,0].\n",
    "\n",
    "sklearn comes with a module sklearn.preprocessing.onehotencoder() to do this,   but the cell below does it explicitly to illustrate what is going on. \n",
    "\n",
    "I've made it generic so that you can easily reuse it for different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numcases = len(irisy)\n",
    "print('there are ' +str(numcases) +' training examples')\n",
    "thelabels = np.unique(irisy)\n",
    "numlabels = len(thelabels)\n",
    "print( 'there are ' + str(numlabels) + ' labels: ' + str(thelabels))\n",
    "# make a 2d array with numcases rows. and numlabels columns\n",
    "irisy_onehot = np.zeros((numcases,numlabels))\n",
    "\n",
    "\n",
    "# Now loop through the rows of the new array setting the appropriate column value to 1\n",
    "for row in range(numcases):\n",
    "    label = irisy[row]\n",
    "    irisy_onehot[row][label]= 1\n",
    "\n",
    "#print(irisy_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting our data into a training and a test set\n",
    "As you can see from the output of the cells above, the iris data has groups all the classes i.e. rows 0-49 are 'iris-setosa', 50-99 are 'iris versicolor'. and rows 100-149 are 'iris-virginica'.\n",
    "\n",
    "So if we want to train our network  and then estiamte how well it will do on new data, we need to split this into a training and test set.\n",
    "Again, we could od this manually - first shuffling the rows so that we got a mixture of classes, then taking the first part of the data for training and the second for testing.\n",
    "\n",
    "If the data are not so well organised, or the numbers of examples of different classes are not roughly equal, then that code gets trickier.\n",
    "So the cell below shows how to do this using a method from sklearn.  The parameters are, in order:\n",
    "- the feature values (irisx)\n",
    "- the onehot-encoded set of labels (irisy_onehot)\n",
    "- what proportion of our data we holdback from training, so we can use it for test. We'll use 1/3rd: test_size=0.33\n",
    "- the array holding the labels that we want to be evenl;y represented in both our training and test sets; stratify=itrisy_onehot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris_train_X, iris_test_X, iris_train_y, iris_test_y = train_test_split(irisX,irisy_onehot, test_size=0.33, stratify=irisy_onehot )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.1 Training a MLP to learn the iris classification problem\n",
    "1. Start by using the  settings for the MLPClassifier that we had before and just change the size of the hidden layer tofive or ten\n",
    "- you will probably see that the training stops making improvements before the problem has been fully learned.\n",
    "- this is an example of the backpropagation getting 'stuck' in a **local optimum** (we'll talk about these more next week)\n",
    "- it happens becuase the basid 'stochastic gradient descent' algorithm *'sgd'* is fairly crude local search method with only crude methods for getting out of 'traps' \n",
    "- try changing the solver to 'adam' and see if this gives better performance\n",
    "\n",
    "**Remember** to run a few times with each setting - this is a stochasdtic algorithm and the random set of initial weights makes a huge difference.  \n",
    "\n",
    "**Question**: what do you understand by *better*\n",
    "\n",
    "2. Now try adding a second hidden layer - for example by changing that parameter in the constructor to *hidden_layer_sizes=(3,3)*.  \n",
    "- Experiment to see if it is better to have one hidden layer of 10 nodes or 2 layers of 5 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an MLP object-  you will want to change the number of hidden nodes\n",
    "irisMLP =  MLPClassifier(hidden_layer_sizes=(5,5), max_iter=1000, alpha=1e-4,\n",
    "                    solver='adam', verbose=0, \n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "irisMLP.fit(iris_train_X, iris_train_y)\n",
    "print('number of output nodes = ' +str(irisMLP.n_outputs_))\n",
    "    \n",
    "lossplot=plt.plot(irisMLP.loss_curve_)    \n",
    "\n",
    "# report how well it does on the trainig set\n",
    "training_accuracy = 100* irisMLP.score(iris_train_X, iris_train_y)\n",
    "print(\"Training set accuracy: \" + str(training_accuracy) + \"%\")\n",
    "\n",
    "# now how good is our network at predicting data it has never seen before\n",
    "test_accuracy = 100* irisMLP.score(iris_test_X, iris_test_y)\n",
    "print(\"Estimated (Test set) accuracy: \" + str(test_accuracy) + \"%\")\n",
    "\n",
    "\n",
    "# print a confusion matrix showing where the errors occur\n",
    "#plot_confusion_matrix(irisMLP,iris_test_X,iris_test_y,labels=irisLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activityy 2.2 Discussion\n",
    "Try to come up with answers to these questions. (these are the sorts of things you might be asked in an exam)\n",
    "\n",
    "1. Why is the test accuracy sometimes much lower than the trainig accuracy?\n",
    "\n",
    "2. Why is it sometimes less reliable train a network with multiple hidden layers when learning the iris data?  \n",
    "Hint: how many connections are you trying to learn?  how much data have you got?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity3: Solving MNIST\n",
    "\n",
    "## TO DO - tidy up, show ten classes in in parallel,  translate code bleow so it doesnt use keras\n",
    "### activitiy is finding out how many layers, and how wide, are needed for this more complex task\n",
    "\n",
    "\n",
    "The aim of this activity is to give you some experience selecting the training parameters and network architecture for applying neural networks to a classification task.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network (MLP) Visualization for Digit Recognition\n",
    "loosely based on example visualisation code from a Kaggle example and \"towards data science\"\n",
    "This notebook contrasts how simple Multi-layer Perceptron (MLP) Neural Networks and Convolutional Neural Networks recognize hand-written digits from the MNIST data set.\n",
    "A two-layer MLP does a decent job at recognizing hand-written digits.\n",
    "A very simple convolutional model does even better (though still not close to the 'state of the art')\n",
    "There is considerable difference in the interpretability of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exanmple code from https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html#sphx-glr-auto-examples-neural-networks-plot-mnist-filters-py\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X / 255.\n",
    "\n",
    "# rescale the data, use the traditional train/test split\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "# this example won't converge because of CI's time constraints, so we catch the\n",
    "# warning and are ignore it here\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n",
    "                            module=\"sklearn\")\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
    "\n",
    "fig, axes = plt.subplots(4, 4)\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
    "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "               vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist dataset\n",
    "data = pd.read_csv('mnist_train.csv')\n",
    "\n",
    "#next few lines added by jim to estiamte vanialla cnn performance\n",
    "x_train = data.iloc[:,1:785]\n",
    "y_train = data.iloc[:,0]\n",
    "\n",
    "testdata = pd.read_csv('mnist_test.csv')\n",
    "x_test = testdata.iloc[:,1:785]\n",
    "y_test = testdata.iloc[:,0]\n",
    "\n",
    "\n",
    "#data = data.head(30000)\n",
    "# split data into train and test sample\n",
    "#x_train, x_test, y_train, y_test = train_test_split(data.iloc[:,1:785], data.iloc[:,0],  \n",
    "                                                    #test_size = 0.1, random_state = 42)\n",
    "\n",
    "dataSamples = data.shape[0]\n",
    "trnum = dataSamples#trnum = int(dataSamples*0.9)\n",
    "tenum = testdata.shape[0]#tenum= dataSamples-trnum\n",
    "x_train = x_train.values.reshape(trnum, 784)\n",
    "x_test = x_test.values.reshape(tenum, 784)\n",
    "\n",
    "# compute the number of labels\n",
    "num_labels = len(np.unique(y_train))\n",
    "\n",
    "# convert to one-hot vector\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# normalize\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "X_train_img = x_train.reshape(x_train.shape[0],28,28,1)\n",
    "X_test_img = x_test.reshape(x_test.shape[0],28,28,1)\n",
    "\n",
    "\n",
    "\n",
    "f, axes = plt.subplots(2, 10, sharey=True,figsize=(20,5))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    ax.axis('off')\n",
    "    ax.imshow(X_test_img[i,:,:,0],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying and Training a simple Neural Network\n",
    "\n",
    "<img align=\"right\" src=\"simple_MLP_for_Mnist.png\" alt=\"Architecture of simple MLP, only 28 inputs shown\" width=\"400\"/>\n",
    "\n",
    "We use a simple two-layer MLP with sigmoid activation Architecture of simple MLP, only 28 inputs shown\n",
    "In the first hidden layer, each neuron takes every pixel value as input parameter.\n",
    "Every neuron in the second hidden layer then takes all the outputs of the first layer (after activation using sigmoid) as input parameters. * After applying a softmax activation, these results form the final output layer.\n",
    "The optimizer then derives linear weights in such a way as to minimize the loss function (in this case the categorical crossentropy) and thus maximizing the accuracy of the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_size = x_train.shape[1]\n",
    "batch_size = 64\n",
    "activation = 'sigmoid'\n",
    "# this model is a 3-layer MLP with sigmoid activation each layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=input_size, activation='sigmoid'))\n",
    "model.add(Dense(25,activation='sigmoid'))\n",
    "model.add(Dense(num_labels,activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "# loss function for one-hot vector using adam optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=batch_size)\n",
    "\n",
    "# validate the model on test dataset to determine generalization\n",
    "loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slide Type\n",
    "This model implementation reaches an accuracy of ~96% if we give it the whole training set, which is remarkable, but better results are possible using, e.g., a Convolutional Neural Network. We use the simpler MLP to make the interpretation of the visualization results easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Extract Output and Group Information\n",
    "We extract the outputs generated by each individual neuron and for each frame in the MNIST training sample and store them on a per-layer basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_layer_output = K.function([model.layers[0].input, model.layers[0].input, model.layers[0].input],\n",
    "                              [model.layers[0].output, model.layers[1].output, model.layers[2].output])\n",
    "\n",
    "layer1_output, layer2_output, layer3_output = get_layer_output([x_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we extract and store the indices of frames showing the same digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [np.arange(len(y_train))[y_train[:,i] == 1] for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of Individual Frames\n",
    "In this visualization, we focus on individual training data (i.e., individual frames with hand-written digits).\n",
    "The following panel shows from left to right\n",
    "the original 28x28 pixel frame depicting a hand-written figure,\n",
    "the output values of all neurons of the first hidden layer,\n",
    "the output values of all neurons of the second hidden layer, and\n",
    "the one-hot encoded output layer indicating the model classification result.\n",
    "Note that in those plots showing network layers, each pixel stands for the output of a single neuron. This output is based on the input parameters passed on from the previous layer, the trained weights for each neuron, and the activation function used in this layer. Dark blue pixels stand for low output values, while yellow pixels stand for high output values. The pixels have been arranged in two dimensions to save space; just think of these layers in linear arrangements to stay in the typical picture of layers in a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "\n",
    "# digit to be plotted\n",
    "digit = 6\n",
    "\n",
    "# indices of frames to be plotted for this digit\n",
    "n = range(50)\n",
    "\n",
    "# initialize plots\n",
    "f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15,4))\n",
    "\n",
    "# prepare plots\n",
    "ax1.set_title('Input Layer', fontsize=16)\n",
    "ax1.axes.get_xaxis().set_visible(False)\n",
    "ax1.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "ax2.set_title('Hidden Layer 1', fontsize=16)\n",
    "ax2.axes.get_xaxis().set_visible(False)\n",
    "ax2.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "ax3.set_title('Hidden Layer 2', fontsize=16)\n",
    "ax3.axes.get_xaxis().set_visible(False)\n",
    "ax3.axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "ax4.set_title('Output Layer', fontsize=16)\n",
    "ax4.axes.get_xaxis().set_visible(False)\n",
    "ax4.axes.get_yaxis().set_visible(False)   \n",
    "\n",
    "# add numbers to the output layer plot to indicate label\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        text = ax4.text(j, i, [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, '', '']][i][j],\n",
    "                        ha=\"center\", va=\"center\", color=\"w\", fontsize=16)    \n",
    "        \n",
    "def animate(id):\n",
    "    # plot elements that are changed in the animation\n",
    "    digit_plot = ax1.imshow(x_train[train_ids[digit][id]].reshape((28,28)), animated=True)\n",
    "    layer1_plot = ax2.imshow(layer1_output[train_ids[digit][id]].reshape((5,5)), animated=True)\n",
    "    layer2_plot = ax3.imshow(layer2_output[train_ids[digit][id]].reshape((5,5)), animated=True)\n",
    "    output_plot = ax4.imshow(np.append(layer3_output[train_ids[digit][id]], \n",
    "                                       [np.nan, np.nan]).reshape((3,4)), animated=True)\n",
    "    return digit_plot, layer1_plot, layer2_plot, output_plot,\n",
    "\n",
    "# define animation\n",
    "ani = matplotlib.animation.FuncAnimation(f, animate, frames=n, interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scrolling through the animation, it becomes clear that in most cases the same subset of neurons fires, while other neurons remain quiescent. This is much more obvious in the second hidden layer than in the first hidden layer and can be interpreted as the first layer pre-processesing the pixel data, while the second layer deals with pattern recognition. Note that in most cases the recognition of the digit shown is unambiguous; ambiguity only occurs in somewhat pathologic cases.\n",
    "You can change the digit shown by changing the digit value in the code block above.\n",
    "\n",
    "Conclusions\n",
    "Despite variations in the shapes of hand-written digits, the same groups of neurons is involved in the identification of the same digits.\n",
    "Similarities in the shapes of digits translate into similarities in the groups of neurons that are involved in their identification in the first hidden layer, but not so much in the second hidden layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
