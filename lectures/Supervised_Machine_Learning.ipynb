{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Machine Learning\n",
    "### Artificial Intelligence 1, Week 6\n",
    "\n",
    "\n",
    "### Learning models for **classification** or **regression** from a set of labelled instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This week\n",
    "Learning outcomes:\n",
    "\n",
    "- Identify formulate and apply the basic processes of supervised machine learning\n",
    "- Understand the role of data in estimating accuracy \n",
    "\n",
    "Lectures:\n",
    "- Basic model building process: train and test \n",
    "- Types of model: instance-based ( e.g. kNN) vs explicit (e.g. decision trees,rules, ...) \n",
    "- Example:   greedy rule induction as compared to expert system\n",
    "\n",
    "## Attendance code: GT-NH-ZO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Paradigm\n",
    "- Completely different paradigm to symbolic AI\n",
    "- Create a system with the ability to learn\n",
    "- Present the system with series of examples\n",
    "- System builds up its own model of the world\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"figures/ML/PersonThinkingAboutDogs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/ML/idealisedDog.png\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Video (6:52): Hello World of Machine Learning Recipes\n",
    "\n",
    "\n",
    "https://youtu.be/cKxRvEZd3Mw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## It's all about the data\n",
    "- Computers cannot experience artefacts of the real world directly\n",
    "- Instead they just deal with a few variables that represent them\n",
    "- ML algorithms learn from a “training set” containing digital representations of examples to learn from\n",
    "- Outcomes depend entirely on:\n",
    " - What you choose to measure\n",
    " - And how representative your training set is\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Top Trumps for Cats and Dogs! \n",
    "<img src=\"figures/ML/TopTrumps.png\" style= \"float:right\" width = 50%>\n",
    "\n",
    "We can't put real animals into a computer!\n",
    "\n",
    "So we have to chose some features, in this case:\n",
    " - some numerical ones (with different scales)\n",
    " - and a \"free text\" field\n",
    " - and an image \n",
    " \n",
    "Perhaps for animal recognition we could have had: \n",
    "   - map (geographical distribution)\n",
    "   - categorical variables: [carnivore | omnivore | herbivore]  \n",
    "   ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More formally\n",
    "\n",
    "We have a set of *n* examples., and for each one  we have: \n",
    "- a value for each of *f* features \n",
    "- a label\n",
    "\n",
    "The data set *X* is usually 2-D array of *n* rows and *f* columns.   \n",
    "The label set *y* is usually a 1-D array with *n* entries.   \n",
    "For now we'll assume the features are *continous* (e.g. floating point values)\n",
    "\n",
    "\n",
    "If the label comes from a discrete unordered set of *m* values, e.g.  (\"Cat\", \"Dog\"): \n",
    "- we have a **Classification** problem.  \n",
    "- We learn a model *M* that is a mapping from a *f*-dimensional continuous space (the feature values) onto a finite set\n",
    "- *M*: R<sup>f</sup> --> \\{1,...,m\\}\n",
    "\n",
    "If the label is an ordinal value (integer,    floating point):\n",
    "- we have a **Regression** problem.\n",
    "- *M*:R<sup>f</sup>->R\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 1:  Student marks from a previous year of the level 1 AI module\n",
    "This data set has just two features: the exam mark and the average of the two parts of coursework.\n",
    "We can assign an outcome knowing that to pass students need:\n",
    "- an average of over 40\n",
    "- and at least 35 on each part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "grades= np.genfromtxt(\"data/assessment-grades-2features.csv\", delimiter= ',',skip_header=1)\n",
    "nStudents = grades.shape[0]\n",
    "\n",
    "featureNames=(\"exam\", \"CW_mean\")\n",
    "outcomes= (\"Pass\",\"Resit Exam\", \"Resit Coursework\",\"Resit Both\")\n",
    "simpleoutcomes= (\"pass\",\"resit\")\n",
    "\n",
    "# make target labels\n",
    "result = np.empty(nStudents, dtype=np.int8)\n",
    "for row in range (nStudents):\n",
    "    exam = grades[row][0]\n",
    "    cw   = grades[row][1]\n",
    "    if (exam>=35 and cw>=35 and (exam +cw >=80) ):\n",
    "        result[row] = 0 # PASS \n",
    "    elif ( cw>=40 and exam < 40):\n",
    "        result[row] = 1 #resit just exam \n",
    "    elif ( cw<40 and exam>=40):\n",
    "        result[row]= 2 # resit just coursework\n",
    "    else:\n",
    "        result[row] = 3  # resit both\n",
    "        \n",
    "simpleResult = np.where(result<1,0,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# easiest to split the data into 4/2 subgroups to plot the outomes /simplified outcomes\n",
    "\n",
    "passStudents = np.empty((0,2))\n",
    "resitCWStudents = np.empty((0,2))\n",
    "resitExamStudents = np.empty((0,2))\n",
    "resitBothStudents = np.empty((0,2))\n",
    "\n",
    "for student in range (nStudents):\n",
    "    if (result[student]==0):\n",
    "        passStudents = np.vstack( (passStudents,grades[student]) )\n",
    "    elif (result[student]==1):\n",
    "        resitExamStudents = np.vstack( (resitExamStudents,grades[student]) )\n",
    "    elif (result[student]==2):\n",
    "        resitCWStudents = np.vstack( (resitCWStudents,grades[student]) )\n",
    "    else:\n",
    "        resitBothStudents = np.vstack( (resitBothStudents,grades[student]) )\n",
    "simpleResitStudents = np.vstack( (resitExamStudents,resitCWStudents,resitBothStudents))\n",
    "\n",
    "print(passStudents.shape)\n",
    "print(resitExamStudents.shape)\n",
    "print(resitCWStudents.shape)\n",
    "print(resitBothStudents.shape)\n",
    "print(simpleResitStudents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(14,5))\n",
    "plt.xlabel(\"Exam\")\n",
    "plt.ylabel(\"Coursework\")\n",
    "ax[0].set_title(\"Outcomes\")\n",
    "ax[1].set_title(\"Simplified Outcomes\")\n",
    "\n",
    "ax[0].scatter(passStudents[:,0],passStudents[:,1],label = \"Pass\" )\n",
    "ax[0].scatter(resitExamStudents[:,0],resitExamStudents[:,1],label = \"Resit Exam\" )\n",
    "ax[0].scatter(resitCWStudents[:,0],resitCWStudents[:,1],label = \"Resit CW\" )\n",
    "ax[0].scatter(resitBothStudents[:,0],resitBothStudents[:,1],label = \"Resit Both\" )\n",
    "ax[1].scatter(passStudents[:,0],passStudents[:,1],label = \"Resit\" )\n",
    "ax[1].scatter(simpleResitStudents[:,0],simpleResitStudents[:,1],label = \"Pass\" )\n",
    "\n",
    "ax[0].legend(loc='lower right')\n",
    "ax[1].legend(loc='lower right') \n",
    "fig.savefig(\"figures/ML/student-outcomes-scatter.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 2:  Iris flowers <img src=\"figures/ML/Iris-image.png\" style=\"float:right\">\n",
    "- classic Machine Learning Data set\n",
    "- 4 measurements: sepal and petal width and length\n",
    "- 50 examples  from each 3 sub-species for iris flowers\n",
    "- three class problem:\n",
    " - so for some types of algorithm have to decide whether to make  \n",
    "   a 3-way classifier or nested 1-vs-rest classifers\n",
    "- most ML classifiers can get over 90%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The next slide shows a function to visualise the data set, you don't need to follow this code right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_scatterplot_matrix(X,y,featureNames,title=None):\n",
    "    f = X.shape[1]\n",
    "    if(len(y) != X.shape[0]):\n",
    "        print(\"Error,   the y array  must have the same length as there are rows in X\")\n",
    "        return\n",
    "    fig, ax = plt.subplots(f,f,figsize=(12,12))\n",
    "    plt.set_cmap('jet')\n",
    "    for feature1 in range(f):\n",
    "        ax[feature1,0].set_ylabel( featureNames[feature1])\n",
    "        ax[0,feature1].set_xlabel( featureNames[feature1])\n",
    "        ax[0,feature1].xaxis.set_label_position('top') \n",
    "        for feature2 in range(f):\n",
    "            xdata = X[:,feature1]\n",
    "            ydata = X[:,feature2]\n",
    "            ax[feature1, feature2].scatter(xdata,ydata,c=y)\n",
    "    if title != None:\n",
    "        fig.suptitle(title,fontsize=16,y=0.925)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "irisX,irisy = sklearn.datasets.load_iris(return_X_y=True)\n",
    "irisFeaturenames= (\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\")\n",
    "title=\"Scatterplots of 2D slices through the 4D Iris data\"\n",
    "show_scatterplot_matrix(irisX,irisy,irisFeaturenames,title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Timeout?\n",
    "\n",
    "basic workflow.  train\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap so far\n",
    "Machine Learning is about learning patterns from data. In supervised ML this means:\n",
    "\n",
    "**Training Data**: set of labelled examples, each characterised by values for *f* features  \n",
    "**X**: data - usually a 2D array with one row per example, one column for each feature  \n",
    "( even images can be 'flattened' i.e. written out pixel by pixel, row by row into a 1D array for each row).   \n",
    "**y** : the labels/target \n",
    "\n",
    "A supervised Machine Learning **Algorithm**\n",
    "\n",
    "A **performance criteria**: used to drive training and then estimate quality of model.  \n",
    "Depending on the **context** this might be accuracy,  precision, recall,...\n",
    "\n",
    "\n",
    "A **test set** to estimate the performance of the model on unseen data.  \n",
    "If this is not available separately, have to take out some data from the training set\n",
    " - crude way; single 70:30 train:test split, making sure you preserve the proportions of different classes\n",
    " - better way: split data into ten\n",
    "   - repeatedly train on 9/10 test on remaining 1/10, \n",
    "   - \"headline\" result is mean, but keep split results for statistical testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Important Idea!  Decision Surfaces\n",
    "<img src=\"figures/ML/decisionRegions.png\" style=\"float:right\" width=40%>\n",
    "\n",
    "Each feature defines a dimension in *feature space*.\n",
    "\n",
    "Each example has specific values for each feature\n",
    "- so it occupies one point in feature space\n",
    "\n",
    "The aim of our model is to let us predict labels for any item\n",
    "- so it puts decision boundaries into that space to divide it into regions\n",
    "\n",
    "Symbolic Reasoning: \n",
    "- boundaries defined by our 'knowledge' \n",
    "- so can plot without needing data!\n",
    "\n",
    "Machine Learning: \n",
    "- use the training data to **estimate** where the boundaries should be\n",
    "- then plots model's predcition for lots of points over a grid  \n",
    "  to find the decision surface and boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# simple function - currently only works for 2D data - but could easily be extended\n",
    "def PlotDecisionSurface(trainX,trainy,theClassifier,theTitle,featureNames,xvar=0,yvar=1,stepSize=2.0,minZero=False):\n",
    "    #create and prettify the plot\n",
    "    cmap=\"Set3\"\n",
    "    fig,ax= plt.subplots(figsize=(8, 8))\n",
    "    ax.set_title(theTitle)\n",
    "    ax.set_xlabel(featureNames[xvar])\n",
    "    ax.set_ylabel(featureNames[yvar])\n",
    "\n",
    "    #define a grid we use to plot the decision boundaries\n",
    "      #get max/min values for gri edges\n",
    "    columnMax,columnMin = np.max(trainX,axis=0), np.min(trainX,axis=0)\n",
    "    if(minZero==True):\n",
    "        x_min , y_min= 0,0\n",
    "    else:\n",
    "        x_min, y_min = columnMin[ xvar]*0.95, columnMin[yvar]*0.95\n",
    "    x_max, y_max = columnMax[xvar]*1.05, columnMax[yvar]*1.05 \n",
    "    #make the grid\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, stepSize),np.arange(y_min, y_max, stepSize))\n",
    "\n",
    "    #predict and plotfor evey point on the grid\n",
    "    Z = theClassifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z,cmap=cmap)\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(x=trainX[:,xvar ],y= trainX[:, yvar], c=trainy.astype(float), alpha=1.0, cmap=cmap, edgecolor=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning Algorithms\n",
    "Typically a ML method consists of:\n",
    "\n",
    "1: A  representation for the decision boundaries\n",
    " - Each different arrangement of boundaries defines a unique model\n",
    " - Each unique model is defined by the set of values for variables specifying where they are\n",
    " \n",
    "2: A learning algorithm to deciding how to change values to move between models\n",
    " - last week we saw how the KMeans clustering algorirthm uses \"local search with random restarts\"\n",
    "\n",
    "ML Algorithms build models in different ways\n",
    "- but they don’t care what it is they are grouping\n",
    " - and its meaningless to say they “understand”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some example ML methods\n",
    "The field of ML is fast growing and contains many complex methods and representations\n",
    "In this module I will just focus on a few simple ideas to give you a feel for what is out there.  \n",
    "- Instance-based learning (k-Nearest Neighbours) - this week\n",
    "- Decision trees and rule induction algorithms- this week\n",
    "- Artificial Neural Networks - weeks 7 and 8\n",
    "\n",
    "Next year: \n",
    "- Artificial Intelligence 2:  15 credits, semester 1 (AI and \"General\" pathways)\n",
    "and in particular\n",
    "- Machine Learning: 15 credits, semester 2     ( AI pathway)\n",
    "\n",
    "will cover more algorithms in greater depth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instance-based Methods: Nearest Neighbour Methods\n",
    "- Do not explicitly represent class boundaries  \n",
    "  Construct them “on-the-fly” when queried\n",
    "- Store the set of training examples  \n",
    "  More efficient methods may not store all points\n",
    "- Use a metric to calculate distance between two points  \n",
    "  e.g. Euclidean (continuous), Hamming (binary), ...\n",
    "\n",
    "<img src=\"figures/ML/kNN-steps.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Nearest Neighbour Classification \n",
    "<img src=\"figures/ML/voronoi.png\" style=\"float:right\" width = 400>\n",
    "\n",
    "**init()**  :  \n",
    "Specify a distance metric d(i,j) for any two items *i* and *j*     \n",
    "\n",
    "**fit(trainingData)** :  \n",
    "Just store a local copy of the training data as two arrays:  \n",
    "X_train of shape (numTrainingItems , numFeatures),  \n",
    "y_train of shape( numTrainingItems)\n",
    "  \n",
    "**predict(newItems)** :  \n",
    "*Step 1:   Make 2D array distances of shape (num_newItems , numTrainingItems)*   \n",
    "FOREACH newItem i  \n",
    "...FOREACH trainingitem j  \n",
    ".....SET distances [i] [j] = d (i,j) \n",
    "\n",
    "*Step 2: Get labels of the k nearest neighbours*  \n",
    "FOREACH newItem i  \n",
    "...Find the *k* columns for row i with the smallest values  \n",
    "...Get the corresponding *k* labels from y_train  \n",
    "\n",
    "*Step 3: Store majority vote in a  1D array y_pred of size (numToPredict)*   \n",
    "FOREACH newItem i  \n",
    "...FOREACH label m  \n",
    "......Count votes amongst the k Nearest neightbour of i  \n",
    "...... SET y_pred[i] = value of m with highest count\n",
    " \n",
    "RETURN y_pred\n",
    "\n",
    "Image adapted from Vornoi tesselation for kNN from https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Example for K = 1 \n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "class simple_1NN:\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self.numExemplars = X.shape[0]\n",
    "        self.numFeatures = X.shape[1]\n",
    "        self.modelX = X\n",
    "        self.modelY = y\n",
    "        \n",
    "    def predict(self,newItems):\n",
    "        numToPredict = newItems.shape[0]\n",
    "        yPred = np.zeros((numToPredict,1))\n",
    "        \n",
    "        # measure distances - creates an array with numToPredict rows and num_trainItems columns\n",
    "        dist = euclidean_distances(newItems,self.modelX)\n",
    "\n",
    "        #make predictions: This is K=1, TO DO- in your own time extend to work with K>1\n",
    "        for item in range(numToPredict):\n",
    "            closest = np.argmin(dist, axis=1) \n",
    "            yPred[item] = self.modelY [ closest[item]]\n",
    "        \n",
    "        return yPred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## kNN for the student marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "myKNNmodel = simple_1NN()\n",
    "myKNNmodel.fit(grades,simpleResult)\n",
    "\n",
    "testCases=(\"Just Under\", \"Just over\")\n",
    "newStudents = np.zeros((2,2))\n",
    "newStudents[0] = (39.5,39.5)\n",
    "newStudents[1]= (40.5,40.5)\n",
    "prediction = myKNNmodel.predict(newStudents)\n",
    "\n",
    "print(prediction)\n",
    "for mytest in range (2):\n",
    "    pred = int(prediction[mytest])\n",
    "    print(\" the prediction for the test case {} is {} \".format(testCases[mytest],simpleoutcomes[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#visualise the decision surface\n",
    "PlotDecisionSurface(grades, simpleResult, myKNNmodel,\"1-NN simplified outcomes\", (\"exam\",\"cw\"),minZero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### and the more complex outcomes case for illustration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create and train the classifier\n",
    "myKNNmodel = simple_1NN()\n",
    "myKNNmodel.fit(grades,result)\n",
    "PlotDecisionSurface(grades,result, myKNNmodel,\"1-NN full outcomes\", (\"exam\",\"cw\"),minZero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  What are the problems here?\n",
    "\n",
    "1. Our training data is not very representative so **The model is having to extrapolate (makes guesses) a lot in some regions**\n",
    " - it doesn't  include students who did not take the exam\n",
    " - students only tended to do the exam if they had done well on the coursework\n",
    "\n",
    "2. We have a very **imbalanced** data set - not many examples of resits - so hard to split into train and test\n",
    "\n",
    " \n",
    "These can only be addressed by good practice.  \n",
    "- **Fairness**:  We should include more examples from previous years, we should add in 'no shows' and students with mitigating circumstances\n",
    "- **Accountability**: can you explain the decision?  \n",
    "  Actually \"for these features \"a studentX is most like student Y\" is not too bad\n",
    "- **Trust**: can you make a convincing argument that you have carefully chosen the right examples **and features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-NN works just as well as more complex algorithms  for many data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Iris\n",
    "\n",
    "We'll use a function from sklearn to do our train/test split here.\n",
    "\n",
    "This is handy because it shuffles the data and has options to make sure that we keep the same proportion of different classes in our training and testing data.\n",
    "\n",
    "\n",
    "            \n",
    "           \n",
    "We'll also make a **confusion matrix** to examine the predictions it makes\n",
    "rows = target labels,  columns = predicted labels\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# make train/test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "irisX,irisy = load_iris(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(irisX, irisy, test_size=0.33,stratify=irisy)\n",
    "\n",
    "\n",
    "myKNNmodel = simple_1NN()\n",
    "myKNNmodel.fit(X_train,y_train)\n",
    "ypred = myKNNmodel.predict(X_test)\n",
    "confusionMatrix = np.zeros((3,3),int)\n",
    "for i in range(50):\n",
    "    actual = int(y_test[i])\n",
    "    predicted = int(ypred[i])\n",
    "    confusionMatrix[actual][predicted] += 1\n",
    "print(confusionMatrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rule Induction Algorithms\n",
    "\n",
    "In Topic One we looked at 'Knowledge-Based systems'  \n",
    "where **humans provided the rules** for a situation.\n",
    "<img src=\"figures/ML/rule-representation.png\" style=\"float:right\" width=50%>\n",
    "\n",
    "\n",
    "In supervised learning we are interested in how we can make   \n",
    "**machines learn the rules** for an application.  \n",
    "To do that we need to have:\n",
    "1. A representation for rules\n",
    "2. A way of assigning \"goodness\" to (sets of) rules.\n",
    "3. A way of algorithmically generating possible rules  \n",
    "   We have fixed sets of features,operators,outputs,  \n",
    "   We can **discretize** the thresholds for each feature    \n",
    "   So we can use nested loops to create all possible rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"Greedy\" rule induction: keep choosing the next best rule\n",
    "- Typically exploit this in a greedy constructive hill climbing approach:  \n",
    "    Repeatedly generate all the rules we could add to existing set of rules (model),   \n",
    "    Then select and adding the one that discriminates most of the remaining unclassified data \n",
    "\n",
    "- Most existing algorithms tend to use rules built up of lots of axis-perpendicular decisions.  e.g.,*If( exam > 80) THEN (\"Pass\")*   \n",
    "  Draws a line through feature space, perpendicular to the maxWeight axis, crossing it at 80.  \n",
    "  Puts the label \"pass\" on one side, nothing on the other\n",
    "\n",
    "- As more rules are added, the model effectively builds labelled (hyper) boxes in space.  \n",
    "  Rest of space is given with the default (majority) label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example for simplifiedStudentResults <img src=\"figures/ML/simplifiedStudentResultsScatter.png\" style=\"float:right\" width=500>\n",
    "Chart shows scatter plot of coursework (y) vs exam(x) \n",
    "\n",
    "Start by adding rule that classifies most examples: \n",
    "- **if(exam >60) outcome = pass**\n",
    "\n",
    "Then add next best rule to and repeat until all the unclassified items are the same label\n",
    "- **if(coursework < 38) outcome = resit**\n",
    "\n",
    "- **if(exam > 38) outcome= pass** \n",
    "\n",
    "- rest  all have **outcome=resit**\n",
    "\n",
    "This example the model learnt consits of the following  three rules:\n",
    "\n",
    "`IF (exam > 60 ) THEN outcome = \"pass\" \n",
    "ELSE IF (coursework < 38 ) THEN outcome = \"resit\" \n",
    "ELSE IF (exam > 38) then outcome = \"pass\"\n",
    "ELSE (default) type = \"resit\" `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pseudocode\n",
    "Model holds a set of rules and a score.  \n",
    "Score() uses ruleset in candidate solution to make predictions on training set  \n",
    " and sets model.score to -1 if any errors,  else number of correct predictions\n",
    " \n",
    "**Note that a set of rules may not cover every training example**\n",
    "\n",
    "    Preprocess (trainingset)  \n",
    "    SET currentModel with empty ruleset, score = 0   \n",
    "    WHILE (currentModel.score<trainingsetSize) DO  \n",
    "        SET bestchild = emptyModel\n",
    "        FOR newRule in  (all_possible_rules)  \n",
    "            SET newModel = COPY(currentModel)\n",
    "            SET newModel = ADDRULE (newModel, newRule)\n",
    "            SET score = SCORE(newModel)\n",
    "            IF (newModel.score > bestChild.score)\n",
    "               SET bestChild= COPY(newModel)\n",
    "         IF (bestChild.score > currentModel.score)\n",
    "            SET currentModel=COPY (bestChild)\n",
    "    RETURN currentModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees \n",
    "Tree-based structure can capture rules and more.\n",
    "\n",
    "Basic idea: divide input space using a set of axis-parallel lines by **\"growing\"** a tree\n",
    "\n",
    "1. Start with single node that predicts majority class label.\n",
    "2. Recursively:\n",
    " 1. measure the \"data purity\"  or \"information content\"  of the data that arrives at that node\n",
    " 2. examine each way of splitting data  you could put into that node, and measure the information content of the left and right child nodes you would get from the split\n",
    " 4. if the  \"best\" split is above some threshold then add it and repeat\n",
    " \n",
    "**This criteria for adding nodes is different to the rule induction algorithm, and gives you different trees**\n",
    "\n",
    "**Interior nodes** are equivalent to conditions in a rule  \n",
    "**Leaf Nodes** are the outputs: \n",
    " - class labels (classification tree), or \n",
    " - equation for predicting values (regression tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision trees for our example datasets\n",
    "using code from sklearn \n",
    "`class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)`\n",
    "\n",
    "Like all sklearn models it implements a fit() and predict() method\n",
    "\n",
    "Note the default criteria for splitting is the 'gini' indes = there are many available, this is a popular one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Student Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(18,8))\n",
    "fig.suptitle(\"Illustration of how Decision Trees select and insert nodes to increase data purity\")\n",
    "for depth in range (0,3):\n",
    "    if(depth==0):\n",
    "        ax[0].text(0.25, 0.6, \" gini=0.147\\n samples=150,\\n value=[138,12],\\n class=pass\",fontsize=14, \n",
    "        bbox={'facecolor': 'darkOrange', 'alpha': 0.5, 'pad': 10})\n",
    "        ax[0].axes.get_yaxis().set_visible(False)\n",
    "        ax[0].axes.get_xaxis().set_visible(False)\n",
    "        ax[0].set_frame_on(False)\n",
    "        ax[0].set_title(\"Depth 0\")\n",
    "    else:\n",
    "        DTmodel = DecisionTreeClassifier(random_state=1234, max_depth=depth,min_samples_split=2,min_samples_leaf=1)\n",
    "        DTmodel.fit(grades,simpleResult)\n",
    "        _ = tree.plot_tree(DTmodel, feature_names=(\"exam\",\"coursework\"), class_names= (\"pass\",\"resit\"),filled=True,ax=ax[depth])\n",
    "        ax[depth].set_title(\"Depth \"+str(depth))\n",
    "        \n",
    "fig.savefig(\"figures/ML/DecisionTreeExample-studentMarks.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "PlotDecisionSurface(grades,simpleResult,DTmodel, \"Decision Tree: simplified outcomes\", (\"exam\",\"cw\"),minZero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iris\n",
    "- Note how the default settings give us trees with very few examples in some leaf nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# load iris dataset and split into train:test\n",
    "iris = datasets.load_iris()\n",
    "irisX = iris.data\n",
    "irisy = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(irisX, irisy, test_size=0.33,stratify=irisy)\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=1234)\n",
    "model.fit(X_train,y_train)\n",
    "ypred = model.predict(X_test)\n",
    "confusionMatrix = np.zeros((3,3),int)\n",
    "for i in range(50):\n",
    "    actual = int(y_test[i])\n",
    "    predicted = int(ypred[i])\n",
    "    confusionMatrix[actual][predicted] += 1\n",
    "print(confusionMatrix)\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "_ = tree.plot_tree(model, feature_names=iris.feature_names,  \n",
    "                   class_names=iris.target_names,filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So how do  we learn models?\n",
    "**Construction**:  add boundaries to make models more complex\n",
    "- Add examples to kNN\n",
    "- Repeatedly add nodes to trees, splitting on new variables\n",
    "- Repeatedly add rules that classify as-yet unclassified data\n",
    " - Add nodes to an artifical neural network\n",
    " \n",
    "**Perturbation**: Move existing boundaries to change model\n",
    "- Change value of K or distance function in kNN\n",
    "- Change rule/treenode thresholds: *if  exam < 40*  &rarr; *if exam < 38*\n",
    "- Change operators in rules/ tree nodes:  *if exam < 38* &rarr; *if exam &leq; 38*\n",
    "- Change variables considered in rules/tree nodes: *if exam < 38* &rarr; *if coursework < 38*\n",
    "- Change weights in MLP, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "Supervised Machine Learning is concerned with learning predictive models from datasets\n",
    "- Different algorithms use different representations of decision boundaries\n",
    "- Regions inside the boundaries contain **Class labels** or **(formulas leading to) continuous values** (regression)\n",
    "\n",
    "Algorithms **fit** models to data by repeatedly:\n",
    "  - making and testing small changes,  \n",
    "  - and then selecting the ones that improve accuracy on the training set\n",
    "  - until some stop criteria is met\n",
    "\n",
    "  - They do this by either adding complexity or changing the parameters of an existing model\n",
    "  - This is equivalent to moving through “model space”\n",
    "\n",
    "Once the model has been learned (fit) we leave it unchanged  \n",
    "  - and use it to **predict** the labels for new data points\n",
    "\n",
    "Next week:   Neural Networks\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
