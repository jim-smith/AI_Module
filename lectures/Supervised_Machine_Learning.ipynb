{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Learning Outcomes\n",
    "- Identify and illustrate the legal and ethical issues around the use of supervised machine learning\n",
    "- Identify formulate and apply the basic processes of supervised machine learning\n",
    "- Understand the role of data in estimating accuracy \n",
    "Lectures: recap then\n",
    "- 15 minutes: basic model building process: train and test (Validation and model selection are in L2)\n",
    "- Activity:\n",
    "- 15 minutes:  types of model: instance-based (kNN) vs explicit (decision trees,rules) \n",
    "- Activity:\n",
    "- 15 minutes: Exaxmple-  greedy rule induction as compared to expert system\n",
    "- Activity:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Machine Learning\n",
    "### Artificial Intelligence 1, Week 6\n",
    "\n",
    "\n",
    "### Learning models for **classification** or **regression** from a set of labelled instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This week\n",
    "Learning outcomes:\n",
    "- Identify and illustrate the legal and ethical issues around the use of supervised machine learning\n",
    "- Identify formulate and apply the basic processes of supervised machine learning\n",
    "- Understand the role of data in estimating accuracy \n",
    "\n",
    "Lectures:\n",
    "- Basic model building process: train and test \n",
    "- Types of model: instance-based ( e.g. kNN) vs explicit (e.g. decision trees,rules, ...) \n",
    "- Example:   greedy rule induction as compared to expert system\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Paradigm\n",
    "- Completely different paradigm to symbolic AI\n",
    "- Create a system with the ability to learn\n",
    "- Present the system with series of examples\n",
    "- System builds up its own model of the world\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"figures/ML/PersonThinkingAboutDogs.png\" syle=\"float:left\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/ML/idealisedDog.png\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Video (6:52): Hello World of Machine Learning Recipes\n",
    "\n",
    "\n",
    "https://youtu.be/cKxRvEZd3Mw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## It's all about the data\n",
    "- Computers cannot experience artefacts of the real world directly\n",
    "- Instead they just deal with a few variables that represent them\n",
    "- ML algorithms learn from a “training set” containing digital representations of examples to learn from\n",
    "- Outcomes depend entirely on:\n",
    " - What you choose to measure\n",
    " - And how representative your training set is\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Top Trumps for Cats and Dogs! \n",
    "<img src=\"figures/ML/TopTrumps.png\" width = 80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More formally\n",
    "\n",
    "We have a set of *n* examples., and for each one  we have: \n",
    "- a value for each of *f* features \n",
    "- a label\n",
    "\n",
    "If the label comes from a discrete unordered set of *m* values, e.g.  (\"Cat\", \"Dog\"), we have a **Classification** problem.  \n",
    "We learn a model *M* that is a mapping from a *f*-dimensional continous space (the feature values) onto a finite set   *M*: R<sup>f</sup> --> \\{1,...,m\\}\n",
    "\n",
    "If the label is an ordinal value (integer,    floating point) we have a **Regression** problem. *M*:R<sup>f</sup>->R\n",
    "\n",
    "The data set *X* is a 2-D array of *n* rows and *f* columns. \n",
    "The label set *y* is a 2D array with *n* entries\n",
    "\n",
    "### The next slide shows a function to visualise the data set, the ones after  illustrates this for the Cats and Dogs/IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_scatterplot_matrix(X,y,featureNames,title=None):\n",
    "    f = X.shape[1]\n",
    "    if(len(y) != X.shape[0]):\n",
    "        print(\"Error,   the y array  must have the same length as there are rows in X\")\n",
    "        return\n",
    "    fig, ax = plt.subplots(f,f,figsize=(12,12))\n",
    "    plt.set_cmap('jet')\n",
    "    for feature1 in range(f):\n",
    "        ax[feature1,0].set_ylabel( featureNames[feature1])\n",
    "        ax[0,feature1].set_xlabel( featureNames[feature1])\n",
    "        ax[0,feature1].xaxis.set_label_position('top') \n",
    "        for feature2 in range(f):\n",
    "            xdata = X[:,feature1]\n",
    "            ydata = X[:,feature2]\n",
    "            ax[feature1, feature2].scatter(xdata,ydata,c=y)\n",
    "    if title != None:\n",
    "        fig.suptitle(title,fontsize=16,y=0.925)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cats and Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "columnLabels = [\"Top Speed\",\"Max. Length\", \"Max. Weight\",\"Food Intake\",\"LifeSpan\"]\n",
    "\n",
    "X = np.empty((8,5)) \n",
    "X[0] = [48,0.9,11,1,14]   # Red Fox\n",
    "X[1] = [40,0.6,8,0.75,12] # Arctic Fox\n",
    "X[2] = [60,2.5,250,20,29] # Jaguar\n",
    "X[3] = [60,2.5,250,20,29] # Lion\n",
    "X[4] = [70,1.6,80,10,2]  # \"Grey Wolf\"\n",
    "X[5] = [56,1.1,36,4.5,12] # \"African Wild Dog\"\n",
    "X[6] = [60,1.9,91,8,21] # Leopard\n",
    "X[7] = [50,3,320,40,26] # Tiger\n",
    "\n",
    "title = \"All the 2-d slices through the 5-dimensional feature space\"\n",
    "y = [0,0,1,1,0,0,1,1] # y[i] will be 1 for cat,  0 for dog\n",
    "show_scatterplot_matrix(X,y,columnLabels,title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "X,y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "title=\"Scatterplots of 2D slices through the 4D Iris data\"\n",
    "show_scatterplot_matrix(X,y,columnLabels,title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Timeout?\n",
    "\n",
    "basic workflow.  train\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap so far\n",
    "Machine Learning is about learning patterns from data. In supervised ML this means:\n",
    "\n",
    "**Training Data**: set of labelled examples, each characterised by values for *f* features  \n",
    "**X**: data - usually a 2D array with one row per example, one column for each feature  \n",
    "( even images can be 'flattened' i.e. written out pixel by pixel, row by row into a 1D array for each row).   \n",
    "**y** : the labels/target \n",
    "\n",
    "A supervised Machine Learning **Algorithm**\n",
    "\n",
    "A **performance criteria**: used ot drive training and then estimate quality of model.  \n",
    "Dependiong on the **contexzt** this might be accuracy,  precision, recall,...\n",
    "\n",
    "\n",
    "A **test set** to estimate the performance of the model on unseen data.  \n",
    "If this is not available separately, have to take out some data from the training set\n",
    " - crude way; single 70:30 train:test split, making sure you preserve the proportions of different classes\n",
    " - better way: split data into ten repeatedly train on 9/10 test on remaining 1/10, \n",
    " - \"headline\" result is mean, but keep split results for statistical testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning Algorithms\n",
    "Typically a ML method consists of:\n",
    "\n",
    "1: A  representation for the decision boundaries\n",
    " - Each different arrangement of boundaries defines a unique model\n",
    " - Each unique model is defined by the set of values for variables specifying where they are\n",
    " \n",
    "2: A learning algorithm to deciding how to change values to move between models\n",
    " - last week we saw how the KMeans clustering algorirthm uses \"local search with random restarts\"\n",
    "\n",
    "ML Algorithms build models in different ways\n",
    "- but they don’t care what it is they are grouping\n",
    " - and its meaningless to say they “understand”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some example ML methods\n",
    "The field of ML is fast growing and contains many complex methods and representations\n",
    "In this module I will just focus on a few simple ideas to give you a feel for what is out there.  \n",
    "- Instance-based learning (k-Nearest Neighbours) - this week\n",
    "- Decision trees and rule induction algorithms- this week\n",
    "- Artificial Neural Networks - weeks 7 and 8\n",
    "\n",
    "Next year: \n",
    "- Artificial Intelligence 2:  15 credits, semester 1 (AI and \"General\" pathways)\n",
    "and in particular\n",
    "- Machine Learning: 15 credits, semester 2     ( AI pathway)\n",
    "\n",
    "will cover more algorithms in greater depth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instance-based Methods: Nearest Neighbour Methods\n",
    "- Do not explicitly represent class boundaries  \n",
    "  Construct them “on-the-fly” when queried\n",
    "- Store the set of training examples  \n",
    "  More efficient methods may not store all points\n",
    "- Use a metric to calculate distance between two points  \n",
    "  e.g. Euclidean (continuous), Hamming (binary), ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Nearest Neighbour Classification <img src=\"figures/ML/voronoi.png\" style=\"float:right\" width = 400>\n",
    "\n",
    "**init()**  :Specify a distance metric d(i,j) for any two items *i* and *j*     \n",
    "  e.g. Euclidean (continuous variables) or Hamming (categorical)\n",
    "\n",
    "**fit(trainingData)** :\n",
    "Just store a local copy of the training data as two arrays:  \n",
    "  X_train of size (N * numFeatures),  \n",
    "  y_train of size( N * 1)\n",
    "  \n",
    "**Predict(newItems)**\n",
    "\n",
    "    *Step 1*:   Make 2D array distances of size (num_newItems * N)  \n",
    "    For each newItem i  \n",
    "        for each trainingitem j  \n",
    "            set distances [i] [j] = d (i,j) \n",
    "\n",
    "    *Step 2*: Get labels of the k nearest neighbours  \n",
    "    For each newItem i  \n",
    "        Find the *k* columns for row i with the smallest values  \n",
    "            Get the corresponding *k* labels from y_train  \n",
    "\n",
    "    *Step 3*: Store majority vote in a  1D array y_pred of size (numToPredict *1)   \n",
    "        For each newItem i  \n",
    "            For each label m  \n",
    "                count votes amongst the k Nearest neightbour of i  \n",
    "            Set y_pred[i] = value of m with highest count\n",
    " \n",
    "    return y_pred\n",
    "\n",
    "Image adapted from Vornoi tesselation for kNN from https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Example for K = 1 \n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "class simple_1NN:\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self.numExemplars = X.shape[0]\n",
    "        self.numFeatures = X.shape[1]\n",
    "        self.modelX = X\n",
    "        self.modelY = y\n",
    "        \n",
    "    def predict(self,newItems):\n",
    "        numToPredict = newItems.shape[0]\n",
    "        yPred = np.empty((numToPredict,1))\n",
    "        dist = euclidean_distances(newItems,self.modelX)\n",
    "        closest = np.argmin(dist, axis=1) \n",
    "        for item in range (numToPredict):\n",
    "            yPred[item] = self.modelY [ closest[item]]\n",
    "        return yPred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cats and dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Scottish wildcat 14kg, 0.8m, 48kph, 8 years, 1.5kg food a day\n",
    "\n",
    "model = simple_1NN()\n",
    "model.fit(X,y)\n",
    "\n",
    "newAnimals = np.empty((2,5))\n",
    "scottish_wildcat = np.array([14,0.8,48,8,1.5])\n",
    "newAnimals[0] = scottish_wildcat\n",
    "\n",
    "prediction = model.predict(newAnimals)\n",
    "\n",
    "if(prediction[0]==0):\n",
    "    decision=\"dog\"\n",
    "else:\n",
    "    decision =\"cat\"\n",
    "    \n",
    "print(\" the scottish wildcat is a \" + decision)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  What are the problems here?\n",
    "\n",
    "\n",
    "\n",
    "### Cats and Dogs\n",
    "\n",
    "1. Some variables (e.g. weight) have bigger ranges that otherrs so dominate the distance calculation\n",
    " - solve this by **normalising** the data so that each feature is mapped onto the range [0-1]\n",
    " \n",
    "2.   Our training data is not very representative\n",
    " - it doesn't even include the house cat!\n",
    " \n",
    "Problem 1 can be addressed by algorithmic methods  - and is standard practice\n",
    "\n",
    "Problem 2 can only be addressed by good practice.  \n",
    "It relates to the issues of:\n",
    "- **Fairness**:  big cats may be exciting but they are not representative of cats as whole\n",
    "- **Accountability**: can you explain the decision.  \n",
    "  Actually \"for these features a wildcat is most like a red fox\" is not too bad\n",
    "- **Trust**: can you make a convincing argument that you have carefully chosen the right examples **and features**\n",
    "\n",
    "## K-NN works just as well as more complex algorithms  for many data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Illustrating k-NN on a classic data set: Iris flowers <img src=\"figures/ML/Iris-image.png\" style=\"float:right\">\n",
    "- classic Machine Learning Data set\n",
    "- 4 measurements: sepal and petal width and length\n",
    "- 50 examples  from each 3 sub-specieis for iris flowers\n",
    "- three class problem:\n",
    " - so for some trypes of alghortityhm have to decide whether to a 3-way classifier or bnest 1-vs-rest classifers\n",
    "- most ML classifieers can get over 90%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Iris\n",
    "\n",
    "We'll use a function from sklearn to do our train/test split here.\n",
    "\n",
    "This is handy because it shiuffles the data and has options to make sure that we keep the same proportion of different classes in our training anad testing data.\n",
    "\n",
    "The alternative woul be to do it by hand e.g.  since we know the data is written out by classes first\n",
    "\n",
    "    X_train=np.empty((0,4))\n",
    "    y_train=[]\n",
    "    X_test=np.empty((0,4))\n",
    "    y_test=[]\n",
    "    for i in range(X.shape[0]):\n",
    "        if ( i%3==0):\n",
    "            X_test=np.vstack((X_test,X[i]))\n",
    "            y_test.append(y[i])\n",
    "        else:\n",
    "            X_train= np.vstack((X_train,X[i]))\n",
    "            y_train.append(y[i])\n",
    "            \n",
    "           \n",
    "We'll also make a **confusiuon matrix** to examine the predictions it makes\n",
    "rows = target labels,  columns = predicted labels\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# make train/test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "X,y = load_iris(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,stratify=y)\n",
    "\n",
    "\n",
    "model = simple_1NN()\n",
    "model.fit(X_train,y_train)\n",
    "ypred = model.predict(X_test)\n",
    "confusionMatrix = np.zeros((3,3),int)\n",
    "for i in range(50):\n",
    "    actual = int(y_test[i])\n",
    "    predicted = int(ypred[i])\n",
    "    confusionMatrix[actual][predicted] += 1\n",
    "print(confusionMatrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rule Induction Algorithms\n",
    "\n",
    "In Topic One we looked at 'Knowledge-Based systems' where **humans provided the rules** for a situation.\n",
    "\n",
    "In supervised learning we are interested in how we can make **machines learn the rules** for an application.  \n",
    "To do that we need to be able to:\n",
    "\n",
    "1 Have a way of assigning \"goodness\" to (sets of) rules.\n",
    "\n",
    "2 Algorithmically generate possible rules\n",
    "  - Often, we have fixed sets of features,operators, and outputs, and can discretize the thresholds for each feature.  \n",
    "    So we can use nested loops to create all the possible rules as tuples of {feature_id,operator_id,threshold_id, label_id}\n",
    "  - Typically exploit this in a greedy constructive hill climbing approach:  \n",
    "    Repeatedly generate all the rules we could add to existing set of rules (model),   \n",
    "    Then select and adding the one that discriminates most of the remaining unclassified data \n",
    "\n",
    "- Most existing algorithms tend to use rules built up of lots of axis-perpendicular decisions.  e.g.,*If( maxWeight > 100) THEN (\"cat\")*   \n",
    "  Draws a line through feature space, perpendicular to the maxWeight axis, crossing it at 100.  \n",
    "  Puts the label \"Cat\" on one side, \"Dog\" on the other\n",
    "\n",
    "- As more rules are added, the model effectively builds labelled (hyper) boxes in space.  \n",
    "  Rest of space is given with the default (majority) label\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example for cats and dogs in 2D <img src=\"figures/ML/cat-dogs-rules.png\" style=\"float:right\" width=500>\n",
    "Chart shows scatter plot of lifespan (y) vs top speed \n",
    "\n",
    "Start by adding rule that classifies most examples: \n",
    "- **if(lifespan < 15) type = dog**\n",
    "\n",
    "Then add next best rule to discriminate rest \n",
    "- **if(speed >65) type = dog**\n",
    "\n",
    "This example the model learnt consits of the following  three rules:\n",
    "\n",
    "`IF (lifespan < 15 ) THEN type = \"dog\" \n",
    "ELSE IF (speed > 65) THEN type = \"dog\" \n",
    "ELSE (default) type = \"cat\" `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pseudocode\n",
    "rule = {variable, operator, threshold, prediction}  \n",
    "type candidateSolution = list of rules + score   \n",
    "\n",
    "Score() uses ruleset in candidate solution to make predictions on training set  \n",
    " and returns -1 if any errors,  else number of correct predictions\n",
    " \n",
    "**Note that a set of rules may not cover every training example**\n",
    "\n",
    "    Preprocess (trainingset)  \n",
    "    SET workingCandidate=[], score = 0   \n",
    "    WHILE (workingCandidate.score<trainingsetSize) DO  \n",
    "        SET tmp = workingCandidate; //make a copy so we can repeatedly edit it  \n",
    "        SET nextWC = workingCandidate //store the most promising offspring  \n",
    "        FOR newRule in  (all_possible_rules)  \n",
    "            SET workingCandidate = tmp //reset to original  \n",
    "            SET workingCandidate = workingCandiudate + newRule //extend working candidate\n",
    "            Score(workingCandidate)  \n",
    "            IF  workingCandidate.score > nextWC.score  \n",
    "                SET nextwc = workingCandidate    \n",
    "        SET workingCandidate = nextWC  \n",
    "    RETURN workingCandidate  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees <img src=\"figures/ML/cats-dogs-tree.png\" style= \"float:right\">\n",
    "Tree-based structure can capture rules and more.\n",
    "\n",
    "Basic idea: divide input space using a set of axis-parallel lines by **\"growing\"** a tree\n",
    "\n",
    "1. Start with single node that predicts majority class label.\n",
    "2. Recursively:\n",
    " 1. measure the \"data purity\"  or \"information content\"  \n",
    "  of the data that arrives at that node\n",
    " 2. examine each way of splitting data  you could put into that node\n",
    " 3. measure the information content of ther l;eft and right chil nodes you would get\n",
    " 4. if the  \"best\" split is above some threshold then add it and repeat\n",
    "\n",
    "**Interior nodes** are equivalent to conditions in a rule  \n",
    "**Leaf Nodes** are the outputs: class labels (classification tree) or equation for predicting values (regression tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision trees example on the iris data\n",
    "using code from sklearn \n",
    "`class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)`\n",
    "\n",
    "Like all sklearn models it implements a fit() and predict() method\n",
    "\n",
    "Note the default criteria for splitting is the 'gini' indes = there are many available, this is a popular one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# load iris dataset and split into train:test\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,stratify=y)\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=1234)\n",
    "model.fit(X_train,y_train)\n",
    "ypred = model.predict(X_test)\n",
    "confusionMatrix = np.zeros((3,3),int)\n",
    "for i in range(50):\n",
    "    actual = int(y_test[i])\n",
    "    predicted = int(ypred[i])\n",
    "    confusionMatrix[actual][predicted] += 1\n",
    "print(confusionMatrix)\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(model, \n",
    "                   feature_names=iris.feature_names,  \n",
    "                   class_names=iris.target_names,\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So how do  we learn models?\n",
    "**Construction**:  add boundaries to make models more complex\n",
    "- Add examples to kNN\n",
    "- Repeatedly add nodes to trees, splitting on new variables\n",
    "- Repeatedly add rules that classify as-yet unclassified data\n",
    " - Add nodes to an artifical neural network\n",
    " \n",
    "**Perturbation**: Move existing boundaries to change model\n",
    "- Change value of K or distance function in kNN\n",
    "- Change rule/treenode thresholds: *if lifespan < 15*  => *if lifespan < 18*\n",
    "- Change operators in rules/ tree nodes:  *if lifespan < 15* => *if lifespave &leq; 15*\n",
    "- Change variables considered in rules/tree nodes: *if lifespan < 15* => *if dailyfood < 15*\n",
    "- Change weights in MLP, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "Supervised Machine Learning is concerned with learning predictive models from datasets\n",
    "- Different algorithms use different representations of decision boundaries\n",
    "- inside the boundaries are:\n",
    " - **Class labels** for a classification problem\n",
    " - **(formulas leading to) continuous values** for a regression problem\n",
    "\n",
    "Algorithms **fit** models to data by repeatedly:\n",
    "  - making and testing small changes,  \n",
    "  - and then selecting the ones that improve accuracy on the training set\n",
    "  - until some stop criteria is met\n",
    "\n",
    "  - They do this by either adding complexity or changing the parameters of an existing model\n",
    "  - This is equivalent to moving through “model space”\n",
    "\n",
    "Once the model has been learned (fit) we leave it unchanged  \n",
    "  - and use it to **predict** the labels for new data points\n",
    "\n",
    "Next week:   Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importthe apples-ornages-bananas data set from the first week\n",
    "# class labels are 0:apple, 1:orange 2: banana\n",
    "# columns in X are Red,Green,Blue,Width,Height,Weight,Type\n",
    "import  numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# read in all thedata fro mthe apples-oranges-banas dataset\n",
    "alldata = np.genfromtxt('data/fruits2.csv', delimiter=',')\n",
    "\n",
    "#pull out the first two  feature vales and the labels into two differnt arrays\n",
    "\n",
    "X = alldata[:,:-1]\n",
    "y = alldata[:,-1]\n",
    "\n",
    "\n",
    "numItems = X.shape[0]\n",
    "\n",
    "train_x,test_x,train_y,test_y = train_test_split(X,y,test_size=0.33,stratify=y)\n",
    "\n",
    "\n",
    "model = simple_1NN()\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "\n",
    "ypred = model.predict(test_x)\n",
    "confusionMatrix = np.zeros((3,3),int)\n",
    "for i in range(len(test_y)):\n",
    "    actual = int(test_y[i])\n",
    "    predicted = int(ypred[i])\n",
    "    confusionMatrix[actual][predicted] += 1\n",
    "print(confusionMatrix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
