{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Artificial Intelligence Topic 4: Machine Learning\n",
    "\n",
    "## Week 5: Introduction, Ethics and types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Week:\n",
    "###  Background\n",
    "- What is ML: recap\n",
    "- Types of ML\n",
    "- Ethical Considerations\n",
    "- Creating and Using Data\n",
    "\n",
    "###  Types of Machine Learning\n",
    "- Unsupervised Learning\n",
    " - K-Means as an example\n",
    "- Reinforcement Learning\n",
    "\n",
    "\n",
    "### Next few weeks: \n",
    "- Supervised Machine Learning, \n",
    "- Artificial Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap:<img style=\"float:right\" width=300 src=\"./figures/ML/basic-model-fitting.png\">\n",
    "- Machine Learning is:\n",
    " - the application of inductive logic to a dataset\n",
    " -  to create useful predictive models.  \n",
    " - So it is about solving  modelling problems\n",
    " \n",
    "\n",
    "- In week 1 we learned that problem solving is what you do when one of  \n",
    "  Input > Model –> Output     is missing\n",
    "- In the last topic we looked at how we **manually** create models encoding human expertise\n",
    "- Machine Learning is about how you **automatically** create models  from data (inputs and outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So it’s all about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Yes!\n",
    "\n",
    "The aim is to build ML systems that can be used to do things when data or scenarios arise. \n",
    "\n",
    "So we need data to: \n",
    "- train them on,  \n",
    "- choose between models, \n",
    "- Know (estimate) how well they are going to do when we start using them\n",
    "\n",
    "We may not always have an output for every input \n",
    "- Because they’re not possible to capture\n",
    "  - e.g. data from scientific experiments such as genomics, astrophysics,...\n",
    "- Because sometimes we have to wait a while e.g.,\n",
    "  - game playing\n",
    "  - finding human volunteers to label images/ caption videos,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of Machine Learning\n",
    "Type | Inputs | Outputs | Feedback | What drives search? | Examples\n",
    "-----|--------|---------|----------------|---------------------|---------------------\n",
    "**Supervised** | Data |Predictions for each case | Correct labels | Accuracy of predictions made | **Recognition**  speech, images, actions, **Forecasting**\n",
    "**Reinforcement**| Scenarios | Actions to take in different states | Periodic Rewards | Expected future feedback | Learning game strategy\n",
    "**Unsupervised** | Data| Groupings of similar items | None | Statistics about cluster *coherence* and separation | Recommender systems, search engines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ethics: *Can* we use the data?\n",
    "- GDPR,  Privacy policies affect how it is collected\n",
    "- The law is very clear that we have to give people the right to:\n",
    "  - Provide Informed Consent about  how we are going to use their personal data at the time we collect it\n",
    "  - Find out what information we hold about them\n",
    "  - Withdraw their data (e.g. “right to forget”)\n",
    "\n",
    "- Examples of unethical use:    \n",
    " - Cambridge Analytica,   \n",
    " - targeting of fake news, propaganda on social media\n",
    "\n",
    "- Nowadays there should be clear collaboration agreements describing who is the data controller and who is the data processor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ethics: *Should* we use the data? <img src=\"figures/ML/algorithms-of-oppression.png\" style=\"float:right\" width = 100> \n",
    "<img style=\"float:right\" src=\"figures/ML/protected-characteristics.png\" width = 400>\n",
    "\n",
    "- ML is only as good as the data we give it\n",
    "- So we have to be very careful that the data is representative\n",
    "\n",
    "- Examples of problems:\n",
    "  - Microsoft's (abandoned) Tay Bot\n",
    "  - Google's (heavily modified) Image Recognition\n",
    "  - Police offender profiling\n",
    "  - Amazon's (abandoned) hiring policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Quick Video from Cog-x\n",
    "\n",
    "[![AI Ethics with Dong Nguyen, The Alan Turing Institute | CogX17 Highlight | CogX](https://img.youtube.com/vi/v=M-ko82Y0GUQ/0.jpg)](https://www.youtube.com/watch?v=M-ko82Y0GUQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning <img src=\"figures/ML/clustering.png\" style=\"float:right\" width=400>\n",
    "- Don’t have labels but we still want to find useful groups\n",
    "- All data is defined in terms of values for features\n",
    "  - Numbers,  categories (colour, name, Uni course),  or even just present-absent\n",
    "- So we define distance measure d(a,b) between two data items a and b.\n",
    "  - Hamming Distance (number of features where a and b differ)\n",
    "  - Euclidean (straight line) distance for continuous numbers\n",
    "- Typically in clustering we look for a way of putting the data items into k clusters\n",
    "  - We don't have labels for outputs\n",
    "   - so drive search between models to maximise *Quality of Clustering*\n",
    "  - intercluster distance (max value of  d(a,b) for all a,b, in the cluster) is minimized\n",
    "  - Intracluster distance (min value of d(a,c) for a and c in different clusters) is maximised\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means <img src = \"https://dashee87.github.io/images/kmeans.gif\" style=\"float:right\">\n",
    "Probably the best known clustering algorithm  \n",
    "\n",
    "**Init**: Pick K data points as initial cluster “centroids”  \n",
    "\n",
    "**Loop**: until no changes in total sum_squared_distance:  \n",
    "Step 1: Assign items to clusters  \n",
    "  For each data point i:\n",
    " - calculate distance d(i,C_k) to each of the centroids C_k, k in (1,,,K)\n",
    " - assign datapoint to cluster k* with nearest centroid\n",
    " \n",
    "Step 2: Check to see if there algorithm has converged\n",
    " - if no items have moved clusters **FINISH**\n",
    " \n",
    "Step 3 Foreach cluster k in (1...K):\n",
    " - find members\n",
    " - Calculate sum of squared distances of cluster elements to centroid \\sum d(i,k)^2\n",
    " - Set new cluster centroid C_k =  mean position of  points in cluster\n",
    "   \n",
    "    \n",
    " Example from https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\n",
    "   \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import random, numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def PlotCluster(X,labels,cents, iteration):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    # show a scatter plot  coloured by labels\n",
    "    fig = plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
    "    for k in range(cents.shape[0]):\n",
    "        plt.plot(cents[k][0],cents[k][1], marker = '*',color='r',markersize=12)\n",
    "    plotTitle= \"Cluster membership and centroids: iteration \" + str(iteration) \n",
    "    title= plt.title(plotTitle)\n",
    "    #plt.show() don;t need this is we have set matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "K=4\n",
    "numItems=100\n",
    "# make a random 2D data set with 100 points and 4 clusters\n",
    "X,y = make_blobs(n_samples=numItems, n_features=2,centers= K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#  pick randomK starting points as the initial cluster centres\n",
    "idx = np.random.randint(numItems, size=K)\n",
    "centres = X[idx,:]\n",
    "\n",
    "# to start with nothing is assigned to a cluster\n",
    "labels = np.ones(X.shape[0])    \n",
    "PlotCluster(X,labels,centres,0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "    \n",
    "# now repeatedly call KMeans to do one iteration\n",
    "for iteration in range(10):\n",
    "    # Step 1: assign items to  clusters\n",
    "    dist = euclidean_distances(X, centres) # dist is an array of size [numItems][K]\n",
    "    newlabels = np.argmin(dist, axis=1) #newlabels is [numItems][1]\n",
    "\n",
    "    # Step 2: check to see if anything has moved\n",
    "    if(np.array_equal(labels, newlabels)): # we're done\n",
    "        print (\"No items changed cluster in iteration {}\".format(iteration))\n",
    "        break\n",
    "    else: # plot new clusters\n",
    "        labels=newlabels\n",
    "        PlotCluster(X,labels,centres,iteration)  \n",
    "    \n",
    "    # Step 3: loop through each cluster finding mean position of its members\n",
    "    for cluster in range (K):\n",
    "        # get members\n",
    "        clusterMembers=np.empty((0,2))\n",
    "        for item in range(numItems):\n",
    "            if (labels[item]==cluster):\n",
    "                clusterMembers = np.vstack((clusterMembers,X[item]))\n",
    "\n",
    "        # set centroids to new mean values  or [0,0] if cluster empty     \n",
    "        if(clusterMembers.shape[0]==0):\n",
    "            print('no items in cluster {}'.format(cluster))\n",
    "            centres[cluster]=[0,0]\n",
    "        else:\n",
    "            centres[cluster] = clusterMembers.mean(axis=0)\n",
    "\n",
    "\n",
    "       \n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Of course you wouldn't normally write your own version...\n",
    "\n",
    "Highly optimised versions available in well-established frameworks e.g. Weka (Java), scikit-learn (python).\n",
    "\n",
    "`class sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init=10, max_iter=300, tol=0.0001,...)`  \n",
    " - defult number of clusters\n",
    " - variety of \"smart\" initialisation schemes\n",
    " - n_init: number of repeats it does before returning the best\n",
    " \n",
    "Object attributes include: \n",
    "- `cluster_centers` :ndarray of shape (n_clusters, n_features)\n",
    "- `labels`  : ndarray of shape (n_samples,)\n",
    "- `inertia` : float (Sum of squared distances of samples to their closest cluster center.)\n",
    "\n",
    " \n",
    "Methods include: \n",
    "- `fit(X[, y, sample_weight])` : Compute k-means clustering.\n",
    "or this estimator.\n",
    "- `predict(X[, sample_weight])` : Predict the closest cluster each sample in X belongs to.\n",
    "- `fit_predict(X[, y, sample_weight])` : Compute cluster centers and predict cluster index for each sample.\n",
    "- `get_params([deep])` : Get parameters f\n",
    "- `score(X[, y, sample_weight])` : Opposite of the value of X on the K-means objective.\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means Strengths and weaknesses \n",
    "<img src=\"figures/ML/kmeans_clustering_examples.png\" style=\"float:right\">\n",
    "\n",
    "### PROS: \n",
    "- fast, \n",
    "- lots of implementations\n",
    "\n",
    "### CONS:\n",
    "- need right value of K, \n",
    "- results depend on starting points\n",
    "\n",
    "### Assumptions:\n",
    "- all features are relevant, \n",
    "- data is \"globular\" with respect to the current features\n",
    "\n",
    "### How could we fix the counter-example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning\n",
    "Five minute video.\n",
    "\n",
    "[![Reinforcement learning for bar-tenders](https://img.youtube.com/vi/v=m2weFARriE8/0.jpg)](https://www.youtube.com/watch?v=m2weFARriE8)\n",
    "https://www.youtube.com/watch?feature=oembed&v=m2weFARriE8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning <img src=\"figures/ML/RL.png\" style=\"float:right\" width=400>\n",
    "Q learning was best known initial algorithm\n",
    "- Basic idea is that you have a *Reward* table R\n",
    " - which tells you what reward you get if you are in state s and take action a\n",
    " - for a multi-step problem the immediate rewards might be zero for many states  \n",
    "  e.g. finding your way out of a maze, playing tic-tac-toe (noughts and crosses)  \n",
    "  ... \n",
    "- Uses repeated trials to learn a Quality table Q:  \n",
    "  one row for each  state *s*,  one column for each action *a*\n",
    " - Start exploring, and build up a list of what states you have been in [*s(1)*,*s(2)*,...,*s(t)*],  \n",
    "   and actions you have taken [*a(1)*, *a(2)*, ..., *a(t)*]\n",
    " - If at time  *t* you get a reward *r*  \n",
    "   then Q[*s(t)*][*a(t)*]  is increased by *r*  \n",
    "   and the previous steps get a 'discounted' reward too    \n",
    "    e.g. Q[*s(t-1)*][*a(t-1)*] is increased by  0.9 * *r*  \n",
    "    ...\n",
    "- Over time the Q table learns the best sequences of moves to take,  \n",
    "   so you can use it to pick the next move\n",
    "    \n",
    "- Problems with scalability as numbers of  \n",
    "  possible states and actions increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Reinforcement Learning  <img src=\"figures/ML/AlphaGoZero.png\" style=\"float:right\" width=200>\n",
    "- neural net rather than table \n",
    "- tends to learn “end-to-end”  rather than a Q table and a policy table\n",
    "  E.g. Alpha Go, Atari simulator\n",
    "- Relies on lots of data:  \n",
    "  e.g. Unity: ‘learning brain’\n",
    "  From ml-agents toolkit\n",
    " Links out to tensorflow model\n",
    " \n",
    "- Alpha Go Zero:   \n",
    "  learned by playing itself!\n",
    "  image from https://medium.com/syncedreview/alphago-zero-approaching-perfection-d8170e2b4e48\n",
    " \n",
    " Really nice explanation of Q-learning here: http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "Basic idea: Models divide up “decision space” into regions\n",
    "\n",
    "\n",
    "Search for model is driven by accuracy\n",
    "\n",
    "Form depends on what the ouputs can be\n",
    "- Two class:  0/1 loss \n",
    "- Many class: Cross entropy \n",
    "- Continuous: mean squared error\n",
    "\n",
    "Types of models we’ll look at:\n",
    "- K Nearest Neighbours\n",
    "- Greedy Rule Induction\n",
    "- Naïve Bayes\n",
    "- Artificial Neural Nets\n",
    "\n",
    "**Classification** algorithms put labels on regions\n",
    "\n",
    "**Regression** algorithms compute a function in regions. \n",
    "\n",
    "Image showing apples and oranges dataset with labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The basic supervised learning process\n",
    "1. Choose features according to what kind of data you have available\n",
    "2. Decide what types of model might be appropriate \n",
    " - human readable?,   \n",
    " - type and amount of data?    \n",
    "3. Initialise Model \n",
    "4.  While not finished:\n",
    "  - See how well it does on training set\n",
    " - Adapt model to reduce error on training set\n",
    "5.  Try to estimate how good it is\n",
    "\n",
    "Often do steps 3-5 above in parallel with different types of model or metaparameters\n",
    "E.g. max number of rules, max depth of trees, value of k in kNN, learning rates in ANN   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How we use our data <img src=\"figures/ML/using_data.png\" style=\"float:right\" width=300>\n",
    "\n",
    "### Unsupervised Learning: \n",
    "- Estimate of quality is based on whole dataset,  \n",
    "  so use it all for training\n",
    "\n",
    "### Reinforcement learning:\n",
    "- Problem is usually lack of data compared to size of state-action space.  \n",
    "- Because data is only generated by using the algorithm!\n",
    "- Alternate periods of:\n",
    "  - training (explore state-action-reward space to improve model)\n",
    "  - testing (choose current max predicted reward in each state) \n",
    "  \n",
    "### Supervised learning:\n",
    "Most commonly work in *off-line* or  *batch* mode \n",
    "- Random split of the data into separate test set, training set, and sometimes validation set\n",
    "- Final model then built using all the data available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
