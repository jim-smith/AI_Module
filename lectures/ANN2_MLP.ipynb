{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#    Multi Layer Perceptrons\n",
    "## Artificial Intelligence 1, week 9 \n",
    "\n",
    "This week:\n",
    "- recap on perceptrons\n",
    "- perceptrons are linear classifiers\n",
    "- multi-layer preceptrons\n",
    "   - architecture\n",
    "   - feed forward predictions\n",
    "   - back propagation for training\n",
    "   - examples\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neurons - The basis of  Neural Networks\n",
    "\n",
    "<div class=\"pull-right\"><img src=\"https://github.com/jim-smith/lecture_resources/blob/master/neuron.jpg?raw=true\"></div>\n",
    "Perceptrons, invented by Frank Rosenblatt in the late 1950's,\n",
    "are a form of supervised machine learning algorithm inspired by neuron cells.\n",
    "\n",
    "In neurons, signals come in along the dendrites and out along the axon. \n",
    "\n",
    "A synapse is the connection between the axon of one cell and the dendrites of another.\n",
    "\n",
    "Crudely, input signals are 'summed' and if they reach a certain threshold the neuron 'fires'\n",
    "and sends a signal down the synapse to the\n",
    "connected cells.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptrons - The basis of Artificial Neural Networks\n",
    "\n",
    "<div class=\"pull-right\"><img src=\"https://github.com/jim-smith/lecture_resources/blob/master/Perceptron.png?raw=true\", width = 100%/></div>\n",
    "Perceptrons are an algorithmic approximation of this process and can learn to solve simple classification problems.\n",
    "\n",
    "Input values are multiplied by a learnable parameter called a *weight*.\n",
    "If the sum of the inputs $\\times$ weights is over a certain threshold the Perceptron 'fires' and generates an output.\n",
    "\n",
    "We use the *error* in the output to change the value of the *weights* by a small amount - the *learning rate*.\n",
    "The process is repeated until the error is 0, or as small as we can get it.\n",
    "\n",
    "**Note:** The threshold which determines if the Perceptron produces an output is determined by its *activation function*.\n",
    "For Perceptrons this is often a step function which outputs a 1 or 0 i.e. 'fires' or not. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptrons Create Linear Decision Boundaries\n",
    "\n",
    "To give you an intuition for what the Perceptron is doing, consider the equation for a straight line:\n",
    "\n",
    "    y = ax + c\n",
    "\n",
    "a and c are coefficients just like the  weights and bias in the Perceptron.\n",
    "\n",
    "### The perceptron's behaviour changes when the sum of the weighted  inputs is 0\n",
    "So if we plot the output at different points in space there is a  *decision boundary* when  $input1 \\times weight1 + input2 \\times weight2 + biasweight = 0$\n",
    "\n",
    "We can rearrange this equation to see that the change of behaviour happens when  $input2 = - \\frac{weight1}{ weight2} \\times input1  - \\frac{ biasweight}{ weight2}$\n",
    "\n",
    "But this is just the equation for a straight line !\n",
    "- slope is given by weight1/ weight2\n",
    "- the intercept  = biasweight / weight2\n",
    "\n",
    "- step function => which side of the line is 1 or 0\n",
    "\n",
    "So, the Perceptron is essentially learning a function for a straight line which is called the decision boundary.\n",
    "In this case, which 'class' the set of inputs belongs to i.e. True or False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def showPerceptron( w1,w2,bias,func): \n",
    "    in1 = np.linspace(-5,5,100)\n",
    "    if (w2==0):\n",
    "        y=0\n",
    "    else:\n",
    "        y = -(bias/w2)  - in1*(w1/w2)\n",
    "    plt.plot(in1, y, '-r',label=\"Decision Boundary\")\n",
    "        # plot sample functions\n",
    "    if(func != ''):\n",
    "        plt.plot(0,0,'or')\n",
    "        if(func=='AND'):\n",
    "            m01 = m10 = 'or'\n",
    "        else:\n",
    "            m01=m10='og'\n",
    "        if(func=='XOR'):\n",
    "            m11 = 'or'\n",
    "        else:\n",
    "            m11 = 'og'\n",
    "        plt.plot(0,1,m01)\n",
    "        plt.plot(1,0,m10)\n",
    "        plt.plot(1,1,m11)\n",
    "    \n",
    "    plt.title('Graph of Perceptron decision Boundary')\n",
    "    plt.xlabel('input1', color='#1C2833')\n",
    "    plt.ylabel('input2', color='#1C2833')\n",
    "\n",
    "    plt.xlim(-1.0,2.0)\n",
    "    plt.ylim(-1.0,2.0)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "  \n",
    "weight1 = widgets.FloatSlider(value=-0.5,min = -1,max = 1)\n",
    "weight2 = widgets.FloatSlider(value=0.5,min = -1,max = 1)\n",
    "biasweight = widgets.FloatSlider(value=-0.5,min = -1,max = 1)\n",
    "funcToModel = widgets.RadioButtons(options=['OR','AND','XOR'])\n",
    "output=interact(showPerceptron, w1=weight1,w2=weight2,bias=biasweight,func = funcToModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perceptron Training Law\n",
    "Every time you present an example you compare the output to the desired value,  \n",
    "then update each weight in turn using:\n",
    "\n",
    "    ∆ω_i = ε · i_i · α\n",
    "\n",
    "    change in weight_i (for this example)\n",
    "            = error (for this example)\n",
    "              X input_i (for this example)\n",
    "              X learning rate (fixed)\n",
    "\n",
    "So this means that \n",
    "- Error = target-actual, can be negative \n",
    "- Weights only change when there is an error \n",
    "- Only active inputs are changed. \n",
    "- Inactive (x=0) inputs are not changed at all (which makes sense since they did not contribute to the error). \n",
    "\n",
    "See AI illuminated p297+ for a worked example\n",
    "\n",
    "### In the next cell we wil update the simple perceptron we created last week to have an init and a  fit() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "class two_input_perceptron:\n",
    "    def  __init__( self,learningRate):\n",
    "        self.weight1 = random()\n",
    "        self.weight2 = random()\n",
    "        self.biasweight = random()\n",
    "        self.learningRate = learningRate\n",
    "        print(\" starting with initial random weights {:.4f}, {:.4f} and {:.4f}\".format(self.weight1,self.weight2,self.biasweight))\n",
    "        \n",
    "    def predict(self, input1, input2) -> int: # let it take continuous inputs\n",
    "        summedInput = input1*self.weight1 +input2*self.weight2 + 1*self.biasweight\n",
    "        if summedInput>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def update_weights( self, in1, in2, target):\n",
    "        error = target - self.predict(in1,in2)\n",
    "        if(error == 0):\n",
    "            return 0\n",
    "        else:\n",
    "            self.biasweight += error * 1 *self.learningRate # bias is always +1\n",
    "            if (in1>0):\n",
    "                self.weight1 += error * in1 * self.learningRate\n",
    "            if (in2>0):\n",
    "                self.weight2 += error * in2 * self.learningRate           \n",
    "            return 1\n",
    "                \n",
    "    def fit(self,train_X,train_y, maxEpochs,verbose=True):\n",
    "        for epoch in range (maxEpochs):\n",
    "            output = showPerceptron\n",
    "            errors = 0\n",
    "            for testcase in range (len(train_y)):\n",
    "                errors += self.update_weights(train_X[testcase][0], train_X[testcase][1],train_y[testcase])\n",
    "            if(errors >0):\n",
    "                if( epoch < (maxEpochs-1) and verbose):\n",
    "                    print(\" in epoch {} there were {} errors\".format(epoch,errors))\n",
    "                elif(epoch==maxEpochs-1):\n",
    "                    print(\" after {} epochs there were still {} errors\".format(epoch,errors))\n",
    "            else:\n",
    "                print(\" Perceptron solved the learning problem in {} epochs\".format(epoch))\n",
    "                break\n",
    "    def getWeights(self):\n",
    "        return self.biasweight, self.weight1, self.weight2\n",
    "                \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_X = np.asarray( [0,0,0,1,1,0,1,1])\n",
    "train_X = train_X.reshape(4,2)\n",
    "\n",
    "and_y = [0,0,0,1]\n",
    "or_y = [0,1,1,1]\n",
    "xor_y = [0,1,1,0]\n",
    "\n",
    "print('Learning AND')\n",
    "myPerceptron = two_input_perceptron(0.1)\n",
    "myPerceptron.fit(train_X,and_y, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Learning XOR')\n",
    "myPerceptron2 = two_input_perceptron(0.1)\n",
    "myPerceptron2.fit(train_X,xor_y, 20,verbose=False)\n",
    "\n",
    "print('\\nLearning OR')\n",
    "myPerceptron3 = two_input_perceptron(0.1)\n",
    "myPerceptron3.fit(train_X,or_y, 20,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Perceptrons - Training and Testing on Real-Fake Data\n",
    "\n",
    "Truth table data and logical functions are a good way to learn the Perceptron algorithm but the data isn't very realistic.\n",
    "\n",
    "Most problems are much more complex and cannot be represented with binary data or solved with only 4 training examples.  \n",
    "We were also only training for one **step** (one input example) or one **epoch** (all input examples) at a time, so that we\n",
    "could see what the algorithm was doing.\n",
    "\n",
    "In supervised learning, generally we want to stop training either:\n",
    "- when there is no improvement in the number of errors on the training data\n",
    "- or after some fixed number of epochs\n",
    "\n",
    "Once training is finished we apply the model (trained weights) to some test data and\n",
    "measure its performance.  \n",
    "This gives us an indication of how well it would perform on new data it has not 'seen' before.\n",
    "\n",
    "Next we will train and then test a Perceptron on a larger, real numbered dataset so that we can see the process of \n",
    "applying machine learning in practice.\n",
    "- As we did we looked at the KMeans algorithm, we will randomly generate some training and test data.\n",
    "- This time the features of the data will be real numbers but there are still only 2 classes/labels, 0 and 1.\n",
    "- It's also helpful to plot the data so that we can see how it is distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import some needed modules\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Generate random dataset\n",
    "num_samples = 150\n",
    "features, labels = make_blobs(n_samples=num_samples, centers=2, n_features=2, cluster_std=1.25, random_state=0)\n",
    "\n",
    "# Split data to training and test data, 2/3 for training and 1/3 for testing\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.33)\n",
    "\n",
    "# Print some information about the data\n",
    "print(\"Shape of training data: \" + str(train_x.shape))\n",
    "print(\"Shape of test data: \" + str(test_x.shape))\n",
    "print(\"First 5 rows of training data:\")\n",
    "print(train_x[: 5, :])\n",
    "print(\"First 5 labels of training data:\")\n",
    "print(train_y[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the training data\n",
    "figure, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "plt.set_cmap('Set1')\n",
    "ax[0].scatter(x=train_x[:,0], y=train_x[:,1], c=train_y)\n",
    "ax[0].title.set_text(\"Train\")\n",
    "ax[1].scatter(x=test_x[:,0], y=test_x[:,1], c=test_y)\n",
    "ax[1].title.set_text(\"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print('learning more complex 2-class data')\n",
    "myPerceptron = two_input_perceptron(0.1)\n",
    "myPerceptron.fit(train_x,train_y, 50,verbose=False)\n",
    "bias_w,w1,w2= myPerceptron.getWeights()\n",
    "\n",
    "print(\"Trained model weights:\")\n",
    "print(\"w1: \" + str(w1) + \" w2: \" + str(w2) + \" bw: \" + str(bias_w))\n",
    "\n",
    "# Create the range of values for decision boundary\n",
    "x_range = np.linspace(train_x[:,0].min(), train_x[:,0].max(), 10)\n",
    "y_range = [((-w1/w2) * x) + (-bias_w/w2) for x in x_range]\n",
    "# Plot the training amnd test data and the decision boundary\n",
    "figure, ax = plt.subplots(1,2,figsize=(10, 5))\n",
    "ax[0].scatter(train_x[:,0], train_x[:,1], c=train_y)\n",
    "ax[0].set_title(\"Training Data\")\n",
    "ax[0].plot(x_range, y_range, color='r')\n",
    "ax[1].scatter(test_x[:,0],test_x[:,1],c=test_y)\n",
    "ax[1].set_title(\"Test Data\")\n",
    "ax[1].plot(x_range, y_range, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Perceptron -Estimate Performance\n",
    "\n",
    "Now we can test the trained model by making predictions on the training and test data.\n",
    "\n",
    "The main difference is that now we **don't** update the weights.\n",
    "So, we just iterate over the items in the test set calling predict to get the prediction for that value\n",
    "\n",
    "## Do you understand why the test set accuracy is a better estimate than the training set accuracy?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def measure_accuracy(model,X,y):\n",
    "    ypred = []\n",
    "    numItems = len(y)\n",
    "    for item in range (numItems):\n",
    "        predictedClass = model.predict(X[item][0],X[item][1])\n",
    "        ypred.append(predictedClass)\n",
    "\n",
    "    errors = 0\n",
    "    for i in range(numItems):\n",
    "        if (ypred[i] != train_y[i]) :\n",
    "            errors+= 1\n",
    "    accuracy= 100.0* (numItems -errors)/numItems \n",
    "    return errors, accuracy\n",
    "\n",
    "train_errors,train_accuracy = measure_accuracy(myPerceptron,train_x,train_y)\n",
    "print('On the training set there were {} errors which means the accuracy was {}%'.format(train_errors,train_accuracy))   \n",
    "\n",
    "test_errors,test_accuracy = measure_accuracy(myPerceptron,test_x,test_y)\n",
    "print('On the test set there were {} errors.\\nThis means that our best estimate for the accuracy of our classifier on new data is was {}%'.format(test_errors,test_accuracy))                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Going beyond single straight decision boundaries\n",
    "<div style=\"float: right; width: 50%\">\n",
    "<img align=\"right\", src=\"https://github.com/jim-smith/lecture_resources/blob/master/mlp.png?raw=truem\", width = 75%/,height=40%/>\n",
    "    </div>Some problems need more than one single decision boundary, or  curved (non-linear) boundaries.\n",
    "\n",
    "So we need a more flexible architecture made out of the same building blocks.\n",
    "\n",
    "Multi-Layer Perceptrons (MLPs) have:\n",
    "* lots of connected perceptrons with trainable weights arranged in layers.\n",
    "* calculations flow layer-by-layer from inputs to outputs\n",
    "\n",
    "At the output layers we know the targets and the computed activations \n",
    "* so we can use the perceptron training rule to adjust the last set of weights\n",
    "* But we need some adjustment to know how to adjust the weights from inputs to hidden layer nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Changing the activation function to tell us what the actual input signal was\n",
    "<div style=\"float: right; width: 50%\"><img src=\"https://github.com/jim-smith/lecture_resources/blob/master/sigmoid.png?raw=true\", width = 80%/></div>\n",
    "Instead of using a step function which loses all the detail, we use something with one-to-one mapping\n",
    "\n",
    "Several options, most common is the sigmoid $\\sigma(x)$\n",
    "\n",
    "$ output = \\sigma(input)$\n",
    "\n",
    " $ \\;\\;\\;\\;\\;\\;= \\frac{1}{ 1+e^{-input}}$\n",
    " \n",
    " Which has the nice property that it's derivative (useful for working backwards) is $\\sigma(x) (1- \\sigma(x)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Back-propagation of errors according to their causes \n",
    "Since we don’t know what the ‘expected’ output of the hidden layer is, we cannot calculate the error.\n",
    "\n",
    "Instead, we share out the error from an output neurone to each hidden neurones.\n",
    "\n",
    "We do this in proportion to the strength of the signal coming from that hidden neurone. \n",
    "\n",
    "In practice we can feed in lots of samples then take the average of their errors\n",
    "\n",
    "### Back-propagations is now commonly called \"Stochastic Gradient Descent\" to reflect the fact that it is just a local search algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLP Predicting output for a sample with activation 1/(1+input)\n",
    "<img style=\"float:right\" width=50% src=\"figures/ML/mlp-prediction.png\">\n",
    "\n",
    "**Input to node h1** = $w_1x_1 + w_3x_2$\n",
    "\n",
    "**Output from node h1** = $\\sigma(w_1x_1 +w_2x_2)$\n",
    "\n",
    "**Input to node h2** = $w_2x_1 + w_4x_2$\n",
    "\n",
    "**Output from node h2** = $ \\sigma(w_2x_1 + w_4x_2)$\n",
    "\n",
    "**Input to output node O** = $ w5\\ \\sigma(w_1x_1 + w_2x_2)  + w6\\ \\sigma(w_2x_1 + w_4x_2)$\n",
    "\n",
    "**Output O** =   $ \\sigma( \\ w5\\ \\sigma(w_1x_1 + w_2x_2)  + w6\\ \\sigma(w_2x_1 + w_4x_2) ) $\n",
    "\n",
    " \n",
    "In production code  we can represent the signals (inputs, each layer of hidden nodes, output nodes) as  vectors and the weights between each layer as a matrix\n",
    "\n",
    "Then we can do the whole thing as a sequence of vector-matrix multiplications: **which is exactly what GPUs are designed to do extremely efficiently!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MLP: error back propagation for weight updates with a single sample     <img style=\"float:right\" width=40% src=\"figures/ML/mlp-errors.png\">\n",
    "\n",
    "Step 1. Updates weights to final layer using 'real' diference between target and output\n",
    "- change in w5 = error * output from H1 * learning rate   \n",
    "  = $E1 * $\\sigma(w_1x_1 +w_2x_2)$ * \\alpha$ \n",
    "- change in w6 = error * output from H2  * learning rate.  \n",
    "  = $ E1 *  \\sigma(w_2x_1 + w_4x_2) * \\alpha$\n",
    "  \n",
    "\n",
    "\n",
    "Step 2. Calculate share of error to feed back to hidden nodes\n",
    "-  E2 = (signal to Output from h1)/ (total signal input to Output)  \n",
    "  = $\\frac {\\sigma(w_1x_1 +w_2x_2) }{ \\sigma(w_1x_1 +w_2x_2) +\\sigma(w_2x_1 + w_4x_2 )}$  \n",
    "- similar for E3\n",
    "\n",
    "3. Use these to update w1, w3 like perceptron training\n",
    "\n",
    "4. Same process for rest of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What does this look like in code?\n",
    "\n",
    "As we mentioned above, it is 'cleanest' to implement this using vector and matrix algebra\n",
    "\n",
    "There are many toolkits that do this - prehaps the best known is Google's tensorflow\n",
    "\n",
    "Nowadays libraries like keras let us hide many of the details to provide a quick clean coding interface\n",
    "with *lots* of options\n",
    "\n",
    "For now we will use the sklearn implemntation:\n",
    "`class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)[source]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setting up the final layer of  our network for the type of labels \n",
    "- For a binary problem:\n",
    "    - we have *one* output node with sigmoid (logistic) activation,\n",
    "     - and  interpret its output as the probabilty that the item belongs to class 1\n",
    "    - usually threshold at 0.5 to make a prediction but probabiltiies can be informative\n",
    "- For a  problem with M>2 classes:\n",
    "    - we usually have M output nodes\n",
    "    - 'onehot' encoding for y  \n",
    "       e.g. 0 1 0 0 instead of '2'\n",
    "    - 'softmax' ('all or nothing') activation function for final layer  \n",
    "      e.g. node ouptuts (0.2 0.5 0.4 0.6) get transforamed to (0 0 0 1)\n",
    "- For regression problem:\n",
    "    - usually one output node with a linear activation  \n",
    "      e.g. output = summed weighted inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#¢plot the decision surface\n",
    "## code from https://machinelearningmastery.com/plot-a-decision-surface-for-machine-learning/\n",
    "\n",
    "def plotDecisionSurface(model,X,y):\n",
    "    min1, max1 = X[:, 0].min() - 1, X[:, 0].max() + 1 #1st feature\n",
    "    min2, max2 = X[:, 1].min() - 1, X[:, 1].max() + 1 #2nd feature\n",
    "    x1_scale = np.arange(min1, max1, 0.1)\n",
    "    x2_scale = np.arange(min2, max2, 0.1)\n",
    "    x_grid, y_grid = np.meshgrid(x1_scale, x2_scale)\n",
    "    # flatten each grid to a vector\n",
    "    x_g, y_g = x_grid.flatten(), y_grid.flatten()\n",
    "    x_g, y_g = x_g.reshape((len(x_g), 1)), y_g.reshape((len(y_g), 1))\n",
    "    # stack to produce hi-res grid in form like dataset\n",
    "    grid = np.hstack((x_g, y_g))\n",
    "\n",
    "    # make predictions for the grid\n",
    "    y_pred_2 = myMLP.predict(grid)\n",
    "    \n",
    "    #predict the probability\n",
    "    p_pred = myMLP.predict_proba(grid)\n",
    "    # keep just the probabilities for class 0\n",
    "    p_pred = p_pred[:, 0]\n",
    "    # reshaping the results\n",
    "    p_pred.shape\n",
    "    pp_grid = p_pred.reshape(x_grid.shape)\n",
    "\n",
    "    # plot the grid of x, y and z values as a surface\n",
    "    surface = plt.contourf(x_grid, y_grid, pp_grid, cmap='Pastel1')\n",
    "    plt.colorbar(surface)\n",
    "    # create scatter plot for samples from each class\n",
    "    for class_value in range(2):\n",
    "        # get row indexes for samples with this class\n",
    "        row_ix = np.where(y == class_value)\n",
    "        # create scatter of these samples\n",
    "        plt.scatter(X[row_ix, 0], X[row_ix, 1], cmap='Pastel1')\n",
    "    # show the plot\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# one hidden ;ayer with 10 neurons logistic (sigmoid) activation and Stochastic Gradient Descent (backprop)\n",
    "\n",
    "myMLP = MLPClassifier( hidden_layer_sizes = (5),activation='logistic',solver='sgd', batch_size=1,max_iter=50, learning_rate_init=0.1)\n",
    "\n",
    "myMLP.fit(train_x,train_y)\n",
    "\n",
    "accuracy = 100*  myMLP.score(test_x,test_y)\n",
    "print('Estimated accuracy using the test set is {} %'.format(accuracy))\n",
    "\n",
    "plotDecisionSurface(myMLP,train_x,train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# And it's trivial to add more hidden nodes or add layers if we want to ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-Layer Perceptrons  today\n",
    "\n",
    "Since their Artifiucual Neural Networks have been hugely successdful across a range of classification, regression and control problems\n",
    "\n",
    "Only problems with MLPs were:\n",
    "- reliance on creating appropriate features (and appropriate scaling)\n",
    "- lack of interpretablility\n",
    "\n",
    "Deep Neural Networks:   MLP with >5 hidden layers\n",
    "\n",
    "Typically complex early layers to find/create features such as:\n",
    "- Convolutional Layers discover spatial patterns e.g. 1D,   2D (images) N-D (video)\n",
    "- Recurrent Layers discover patterns in time e.g. speech recognition, natural language processing\n",
    "\n",
    "Because they have more weights to learn appropriate values for, Deep Networks need:\n",
    "- *lots* of data, \n",
    "- *lots* of computational power / time to train\n",
    "\n",
    "Deep Neural Networks useually have one or more  \"dense\" layers (like in a MLP) before the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "The perceptron training algorithm is guaranteed to converge on a solution – if one exists.  \n",
    " - But this will only be the case for linearly separable problems.\n",
    " - and if more than one decision voundary separates the data, there are no guarantees which you will get\n",
    "\n",
    "Multi-Layer Peeceptrons solve this problem by combining perceptrons in a layered structure\n",
    "- The network is capable of learning making non-linear decision boundaries\n",
    "- Activation functions changed from a step, to something that preserves more information sigmoid (logistic)  \n",
    "  so you cna make big[small] changes to weights leading to a hidden node if it's summed inputs were   lots [a little] over the 'trigger point'\n",
    "\n",
    "Backpropagation (Stochastic Gradient Descent)) is the algorithm used to train the more complex, multi layered networks\n",
    "- signals propagate forwardwards through the network\n",
    "- errors are propagated back through the network  \n",
    "  in proportion ot the strength of signals coming from hidden nodes  \n",
    "  for a given input\n",
    "- It is a form of local search so can get stuck on local optima \n",
    "\n",
    "Deep Neural Networks are MLP with extra layers to learn features from complex data"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "rise": {
   "enable_chalkboard": true,
   "footer": "<h3>Intro to AI 2019-20 Jim Smith, UWE</h3>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
