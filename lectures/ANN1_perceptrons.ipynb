{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Outcomes\n",
    "- Principles of biological and artificial neural networks\n",
    "- Understand the perceptron update rule and the principles of the operations of MLPs\n",
    "Lectures: recap then\n",
    "- Perceptrons as a way of learning  rules that aren’t axis-parallel\n",
    "- Activity:\n",
    "- Second half of perceptrons\n",
    "Tutorial recap  questions using mentimeter: axis-parallel vs. oblique rules\n",
    "Tutorial activity \n",
    "- (Nathan's) Notebook applying perceptron to logical functions and then fake data\n",
    "- Description of coursework and discussion about what will be needed but in a way that makes it clear they can’t start yet e.g. framework(.py files) not released **could split into tasks e.g. design, tesat plan, spes**\n",
    "Self-study: update your revision bot with this week's core concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perceptrons : the basis of Artificial Neural networks\n",
    "\n",
    "## Artificial Intelligence 1 , week 7\n",
    "\n",
    "This week:  Artificial Neural Networks part 1\n",
    "1. Intro video\n",
    "2. Then taking you through how they developed, step by step\n",
    "  1. Logical calculus \n",
    "  1. perceptrons \n",
    "  1. multilayer perceptrons\n",
    "3. Biological metaphor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Video. https://www.youtube.com/watch?v=bxe2T-V8XRs\n",
    "[![Simple introduction ot artificial neural networks](https://img.youtube.com/vi/v=bxe2T-V8XRs/0.jpg)](https://www.youtube.com/watch?v=bxe2T-V8XRs)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Linking logic with computational units <img src=\"figures/ML/Perceptron.png\" style = \"float:right\">\n",
    "- McCulloch and Pitts: the Logical Calculus - 1940’s - so before high levvel programming languages had been invented\n",
    "- Tried to link the logic of the “mind” with the functioning of the brain\n",
    "\n",
    "    UNIT with inputs and ‘all or none’ output  \n",
    "    OUTPUT  \n",
    "    CONNECTION - weighted  \n",
    "    THRESHOLD = sum of all weighted inputs needs to meet this for unit to fire  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logical Calculus Units <img src=\"figures/ML/Perceptron.png\" style = \"float:right\">\n",
    "\n",
    "- INPUT: 0 or 1\n",
    "- OUTPUT: 0 or 1\n",
    "- SYNAPSE = weighted connection:  \n",
    "  w<sub>1</sub>, w<sub>2</sub>, bias_weight are all either  +1 or -1\n",
    "- THRESHOLD = 0\n",
    "- BIAS INPUT – clamped at 1  \n",
    "  weighting this lets us change the effective threshold\n",
    "\n",
    "- Output = 1 if sum of inputs+bias >0  \n",
    "  0 if sum of inputs +bias  < 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_input_logical_calculus_unit:\n",
    "    def  __init__(self,weight1:int, weight2:int,biasweight:int):\n",
    "        valid = [-1,0,1]\n",
    "        if(weight1 not in valid or weight2 not in valid or biasweight not in valid):\n",
    "            return(\"Error,  weights can only be +1, -1 or 0\")\n",
    "        else:\n",
    "            self.weight1 = weight1\n",
    "            self.weight2 = weight2\n",
    "            self.biasweight = biasweight\n",
    "        \n",
    "        \n",
    "    def output(self,input1:int,input2:int) -> int:\n",
    "        valid = [0,1]\n",
    "        if (input1 not in valid or input2 not in valid):\n",
    "            return(\"Errors, inputs must be 0 or 1\")\n",
    "        else:\n",
    "            summedInput = input1*self.weight1 +input2*self.weight2 + 1*self.biasweight\n",
    "            if summedInput >0:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How the logical calculus works\n",
    "\n",
    "Take the conectives from logic ( OR, AND, NOR, NAND, XOR)\n",
    "\n",
    "Look at their truth tables\n",
    "\n",
    "Input1 value | Input2 Value | OR(in1,In2) | AND (In1,In2) |NOR(In1,In2) | NAND( In1,In2) | XOR (In1,Inb2)\n",
    "-------------|--------------|-------------|---------------|-------------|----------------|---------------\n",
    "0            | 0            | 0           |    0          | 1           | 1              | 0\n",
    "0            | 1            | 1           | 0             | 0           | 1              | 1\n",
    "1            | 0            | 1           | 0             | 0           | 1              | 1     \n",
    "1            | 1            | 1           | 1             | 0           | 0              | 0     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hand-crafted examples\n",
    " <img src = \"figures/ML/logical_calculus_or.png\"  width = 30%> \n",
    " \n",
    " OR has no bias, so is off until a signal from  **either** input is enough to turn the output on.\n",
    " \n",
    " <img src=\"figures/ML/logical_calculus_nor.png\"  width = 30%>\n",
    " \n",
    " NOR is the opposite: A bias of +1 turns it on with no inputs.  But weights from both inputs are negative, so a signal from **either**  is enough to turn it off\n",
    " \n",
    " <img src = \"figures/ML/logical_calculus_and.png\" width = 30%>\n",
    " \n",
    " AND has a bias of -1, so it is off unless there is a postive signal from **both** inputs\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  in1 = 0, in2 = 0 myOR = 0, myAND = 0, myNOR = 0\n",
      "  in1 = 0, in2 = 1 myOR = 1, myAND = 0, myNOR = 1\n",
      "  in1 = 1, in2 = 0 myOR = 1, myAND = 0, myNOR = 1\n",
      "  in1 = 1, in2 = 1 myOR = 1, myAND = 1, myNOR = 1\n"
     ]
    }
   ],
   "source": [
    "orUnit = two_input_logical_calculus_unit( 1,1,0)\n",
    "andUnit = two_input_logical_calculus_unit( 1,1,-1)\n",
    "norUnit = two_input_logical_calculus_unit( 1,1,0)\n",
    "\n",
    "for in1 in range(2):\n",
    "    for in2 in range(2):\n",
    "        inputs = \"in1 = {}, in2 = {} \".format(in1,in2)\n",
    "        print (\"  {}myOR = {}, myAND = {}, myNOR = {}\".format(inputs,orUnit.output(in1,in2),andUnit.output(in1,in2),norUnit.output(in1,in2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What about XOR ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- no set of weights makes **one** unit behave like XOR\n",
    "- but we can make it by combining others\n",
    "\n",
    "    XOR(in1,in2) = AND ( OR(in1,in2), NAND(in1,in2) )\n",
    "    \n",
    "## What's the big deal? \n",
    "Logical Calculus units demonstrate how **logic** (true/false) can be implemented by **computation**\n",
    "- and so you can express  logical structures in terms of networks of computational units\n",
    "\n",
    "Used in 'binary' mode, transistors implement logical calculus units\n",
    "- and so we get modern \"Integrated Circuits\" or \"silicon chips\" which may contain millions of transistors\n",
    "\n",
    "Hence the modern digital computer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## These would be great for building models but…\n",
    "\n",
    "How do we choose the weights? \n",
    "\n",
    "Can we get the computer to do it for us?\n",
    " - Space would be all possible configurations of nodes connected by +/- 1 weights\n",
    " \n",
    "Rosenblatt claimed to have the answer in the 1960s…\n",
    "Based on a simple model of how a nerve cell works\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: Inspiration from biology\n",
    "What are computers good at?\n",
    "- calculation \n",
    "- memory.\n",
    "\n",
    "BUT traditional architecture is very poor at some tasks\n",
    "- tasks that we accomplish with ease. \n",
    "- Input: Recognition\n",
    "- Output: Control\n",
    "\n",
    "So can we use biological metaphors to guide engineering decisions, and make more useful tools?\n",
    "\n",
    "**Artificial Neural Networks** are algorithms and architectures inspired by the functioning of the biological nervous system.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic neurobiology <img src=\"figures/ML/neuron.jpg\" style = \"float:right\">\n",
    "\n",
    "**Neurons** (nerve cells) \n",
    "- about 10^11 (100 billion) in humans \n",
    "\n",
    "**Dendrites** – incoming activity\n",
    "\n",
    "**Axon** – output of neuron\n",
    "\n",
    "\n",
    "**Synapses** – connection from axon of one neuron to dendrite of another \n",
    "- 100's to 1000’s per neurone\n",
    "- Each synapse has a weight\n",
    "\n",
    "**Action potential** - electrical signal\n",
    "- in the soma the incoming electrical activity is summed over\n",
    " - space (many dendrites connected to other neurons)\n",
    " - time (not usually in ANN except 'spiking neural networks')\n",
    "-  if the sum exceeds a **threshold** then ‘all or none’ output,   \n",
    "   travels down axon and then is connected via synapses to dendrites of other neurones\n",
    "- will add to input to other neurones\n",
    "\n",
    "**The synaptic weights can be changed during learning (or forgetting)**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now answer the multiple choice questions\n",
    "\n",
    "electrical signals reach the cell body through the:   {soma| dendsrites| axons | myelin sheath]\n",
    "\n",
    "number of cells in your brain is: [much less than | about the same as | much bigger than] the number of stars in our galaxy\n",
    "\n",
    "\n",
    "number of connections in your brain is: [much less than | about the same as | much bigger than] the number of stars in our galaxy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Perceptron Research from the 50's & 60's, 1 min clip. https://www.youtube.com/watch?v=cNxadbrN_aI\n",
    "\n",
    "[![Perceptron Research from the 50's & 60's, clip](https://img.youtube.com/vi/v=cNxadbrN_aI/0.jpg)]( https://www.youtube.com/watch?v=cNxadbrN_aI)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: The Rosenblatt Perceptron - the simplest Artificial Neural Network <img src=\"figures/ML/Perceptron.png\" style=\"float:right\">\n",
    "\n",
    "Similar to the logical calculus BUT **weights can be any real number**. \n",
    "\n",
    "However only 1 'layer' : just inputs and units.   \n",
    "units cannot be connected to other units, so there is 1 unit per output.\n",
    "\n",
    "Code is just like above, but now weights can be floats not constrained [-1,0,1]\n",
    "\n",
    "### Rosenblatt proposed an algorithm to get the computer to choose the weights. \n",
    "- Once this was done, perceptrons can handle problems  \n",
    "  e.g. image recognition (letters)\n",
    "- The idea is to **supervise** the procedure (sounds familiar?) \n",
    " - give the perceptron a list of inputs and DESIRED outputs. \n",
    " - the perceptron will learn from that and generalize to previously unseen inputs.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perceptron Training Law\n",
    "\n",
    "    ∆ω = ε · i · α\n",
    "\n",
    "change in weight = error X input X learning rate.\n",
    "\n",
    "So this means that \n",
    "- Error = target-actual, can be negative \n",
    "- Weights only change when there is an error \n",
    "- Only active inputs are changed. \n",
    "- Inactive (x=0) inputs are not changed at all (which makes sense since they did not contribute to the error). \n",
    "\n",
    "See AI illuminated p297+ for a worked example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class two_input_perceptron:\n",
    "    def  __init__(self,weight1, weight2,biasweight, learningRate):\n",
    "        ##. <== note we're not constraining weights to be ints now\n",
    "        self.weight1 = weight1\n",
    "        self.weight2 = weight2\n",
    "        self.biasweight = biasweight\n",
    "        self.learningRate = learningRate ## <== this is new\n",
    "        \n",
    "    def output(self,input1:int,input2:int) -> int:\n",
    "        ## <== in general we don;t need ort contstrian the inputs\n",
    "        summedInput = input1*self.weight1 +input2*self.weight2 + self.biasweight\n",
    "        if summedInput>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def update_weights( self, in1,in2,target):\n",
    "        error = target - self.output(in1,in2)\n",
    "        if(error != 0):\n",
    "            self.biasweight += error * 1 *self.learningRate # bias is always +1\n",
    "            if (in1>0):\n",
    "                self.weight1 += error * in1 * self.learningRate\n",
    "            if (in2>0):\n",
    "                self.weight2 += error * in2 * self.learningRate           \n",
    "            return 1\n",
    "        else:\n",
    "            return 0     ## <=let the calling fuinction know if it made the right prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now answer these questions:\n",
    "\n",
    "Perceptrons are useful because they can learn rather than needing to be hand-coded [True| False]\n",
    "\n",
    "The perceptron's output is: \n",
    "- +1 if the sum of the inputs is more than 0\n",
    "- +1 if the weighted sum of the inputs is more than 0\n",
    "- equal to the weighted sum of the inputs\n",
    "- +1 if the sum of thwe weights is more than 0\n",
    "\n",
    "The weight from an input i to the perceptron is not changed when we present a data example ..\n",
    "- If the output for this example is correct\n",
    "- If the value of feature i for this case is 0\n",
    "- Both the above\n",
    "- Neither of the above"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The promise of perceptrons\n",
    "- If the perceptron can handle a problem, then the perceptron is guaranteed to find an answer\n",
    "\n",
    "        The perceptron convergence theorem\n",
    "\n",
    "- Works for OR, AND, NOT and many demonstration problems…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting with initial random weights 0.4087468894697235, 0.1823812601992444 and 0.9846649385002291\n",
      "in epoch 0 there were 3 errors\n",
      "in epoch 1 there were 3 errors\n",
      "in epoch 2 there were 3 errors\n",
      "in epoch 3 there were 3 errors\n",
      "in epoch 4 there were 2 errors\n",
      "in epoch 5 there were 2 errors\n",
      "in epoch 6 there were 3 errors\n",
      "in epoch 7 there were 2 errors\n",
      "in epoch 8 there were 1 errors\n",
      " Perceptron solved the learning problem in 9 epochs\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "# four rows of test cases,   third column is the right answer\n",
    "andData = [[0,0,0],[0,1,0],[1,0,0],[1,1,1]]\n",
    "\n",
    "# start wit hrandom weights\n",
    "w0=random()\n",
    "w1 = random()\n",
    "w2 = random()\n",
    "print(\"starting with initial random weights {}, {} and {}\".format(w1,w2,w0))\n",
    "\n",
    "\n",
    "myPerceptron = two_input_perceptron(w1,w2,w0,0.1)\n",
    "\n",
    "# just keep presenting the test cases nd updating until there are no errors\n",
    "for epoch in range(50):\n",
    "    errors = 0\n",
    "    for testcase in range(4):\n",
    "        errors += myPerceptron.update_weights(andData[testcase][0], andData[testcase][1],andData[testcase][2])\n",
    "    if(errors >0):\n",
    "        print(\"in epoch {} there were {} errors\".format(epoch,errors))\n",
    "    else:\n",
    "        print(\" Perceptron solved the learning problem in {} epochs\".format(epoch))\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Do you think perceptrons will be able to learn XOR?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The problem with perceptrons <img src=\"figures/ML/linearly_seperable.png\" style= \"float:right\" width = 50%>\n",
    "The Minsky & Papert book 'perceptrons' showed in detail the limitations of perceptrons \n",
    "- It only deals with linearly separable tasks.\n",
    "- So cannot deal with XOR… \n",
    "-  and pretty much all real world problems.\n",
    "\n",
    "Rosenblatt was aware of this but didn't know how to fix it …\n",
    "- neural network research went into a decline in the 1970s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## the answer? : add layers\n",
    "\n",
    "You can solve XOR with logical calculus units\n",
    "XOR(a,b) = AND(  OR(a,b) , NAND(a,b). )\n",
    "\n",
    "So why not just do that for perceptrons?\n",
    "\n",
    "## The catch ...\n",
    "\n",
    "Training a single layer of perceptrons is easy\n",
    "- We can measure what the outputs actually are\n",
    "- We can apply the perceptron update rule because\n",
    "- We know what the outputs should be\n",
    "\n",
    "\n",
    "Training a multi-layer perceptron is harder\n",
    "- We can measure what the *outputs* actually are\n",
    "- so we can apply the perceptron update rule to thje last layer\n",
    " - But what should the output from the hidden layers be?\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " 1:46 video clip with nice animations\n",
    " [![Video of simple neural netowrk from oolitionTech technologies](https://img.youtube.com/vi/v=gcK_5x2KsLA/0.jpg)](https://www.youtube.com/watch?v=gcK_5x2KsLA)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Summary\n",
    "-Logical Calculus units simulate logical operations but need hand-crafting\n",
    "-Nature offers us inspiration in the form of nerve cells\n",
    "-Perceptrons with training offer a way of automatically creating single-unit systems that can learn!\n",
    "-Multi-layer perceptrons offer a way of creating more complicated systems\n",
    "\n",
    "## Next Week\n",
    "Multi-Layer Perceptrons\n",
    "- Architecture\n",
    "- Mathematical foundations\n",
    "- Search/training algorithms  \n",
    "  (**don’t panic!**)\n",
    "\n",
    "\n",
    "Practicalities of using neural networks \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
