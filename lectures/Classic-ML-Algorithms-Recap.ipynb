{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap: supervised ML process and classic algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "1. Decide on a set of features you can measure\n",
    "2. Collect some examples and for each one measure:\n",
    "  - the 'label' (**y**)\n",
    "  - the values for each feature (**X**)\n",
    "3. split your data into  **training** and **test** sets\n",
    "\n",
    "4. Choose your algorithm\n",
    "\n",
    "5. Call the algorithm's **fit(train_X,train_y)** method  \n",
    "  and it will learn where to place the decision boundaries to match your training data\n",
    "6.  now call your algorithm;s **predict()** methods for your test data  \n",
    "    predictions= predict(test_X)\n",
    "    error = score(predictions, test_y)    \n",
    "  - this gives you an **estimate** of how well it will do on unseen data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Algorithm: K-Nearest Neighours\n",
    "- init() sets the choice of distance/similarity  measure\n",
    "- fit()  stores the training data items and their labels\n",
    "- predict() for a new item just:\n",
    "  - measure how far the item is from each of the stored examples\n",
    "  - finds the K closest\n",
    "  - gets their labels (taking a vote if needed)\n",
    "  - returns that as the predicted label for the new point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Algorithm: Greedy Rule Induction\n",
    "\n",
    "- **fit()**: builds rule set by repeatedly adding the 'next best rule':\n",
    "  - see which training items don't match the conditions of  any rules in your current ruleset\n",
    "  - test every possible rule to see which **matches and correctly predicts** the label for those 'uncovered' training items\n",
    "  - add new rule to ruleset and repeat\n",
    " \n",
    "- **predict()**: just goes through the learned rules in turn until one applies\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Algorithm: Decision Trees <img src=\"figures/ML/studentResults-tree.png\" style= \"float:right\" width = 500>\n",
    "Tree-based structure can capture rules and more.\n",
    "\n",
    "Basic idea: divide input space using a set of axis-parallel lines by **\"growing\"** a tree\n",
    "\n",
    "1. Start with single node that predicts majority class label.\n",
    "2. Recursively:\n",
    " 1. measure the \"data purity\"  or \"information content\"  \n",
    "  of the data that arrives at that node\n",
    " 2. examine each way of splitting data  you could put into that node\n",
    " 3. measure the information content of the left and right child nodes\n",
    " 4. if the  \"best\" split is above some threshold then add it and repeat\n",
    "\n",
    "**Interior nodes** are equivalent to conditions in a rule  \n",
    "**Leaf Nodes** are the outputs: \n",
    " - class labels (classification tree), or \n",
    "  - equation for predicting values (regression tree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
